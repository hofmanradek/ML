{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN cascade for URZ\n",
    "### With normalization of Features as in iwt_nnet.c : iwt_normalize()\n",
    "\n",
    "* Radek Hofman, Jan 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and axiliary functions and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('/','home','hofman','.dbp.txt'), 'r') as f: password = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: hofman@udb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"oracle://hofman:%s@mycelium.ctbto.org:1521/udb\" % password\n",
    "%sql $query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>362407</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(362407,)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% sql select count(*) from ml_features where sta='URZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, semilog=False):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.plot(history['acc'], label='acc')\n",
    "    ax.plot(history['val_acc'], label='val_acc')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    if semilog:\n",
    "        ax.set_yscale('log')\n",
    "    plt.legend(loc='best')\n",
    "    ax = fig.add_subplot(212)\n",
    "    ax.plot(history['loss'], label='loss')\n",
    "    ax.plot(history['val_loss'], label='val_loss')\n",
    "    plt.legend(loc='best')\n",
    "    if semilog:\n",
    "        ax.set_yscale('log')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from oracle do pandas\n",
    "import cx_Oracle\n",
    "connection = cx_Oracle.connect('hofman', password, 'udb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the misclassification rate now for all arrivals in our DB?\n",
    "\n",
    "* #(class_iphase != class_phase) / (#automatic which are not noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows affected.\n",
      "0 rows affected.\n"
     ]
    }
   ],
   "source": [
    "#select from database required numbers\n",
    "wrong_type = %sql select count(*) from ml_features where sta='URZ' and class_phase != class_iphase\n",
    "total_number = %sql select count(*) from ml_features where sta='URZ' and phase!='N' and source!='M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of misclassified initial wave types: 47.90%\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of misclassified initial wave types: %3.2f%%' % (wrong_type[0][0]/total_number[0][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes per class phase type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='regS'\"\"\"\n",
    "df_S_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25050735</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125386e+09</td>\n",
       "      <td>Sx</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.526004</td>\n",
       "      <td>0.786459</td>\n",
       "      <td>1.174982</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>1.216724</td>\n",
       "      <td>0.363039</td>\n",
       "      <td>2.166968</td>\n",
       "      <td>1.844924</td>\n",
       "      <td>0.626604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25058004</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125423e+09</td>\n",
       "      <td>Lg</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.925</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.762988</td>\n",
       "      <td>15.337161</td>\n",
       "      <td>15.337161</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.423843</td>\n",
       "      <td>1.384683</td>\n",
       "      <td>0.724611</td>\n",
       "      <td>6.547078</td>\n",
       "      <td>1.569841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25075768</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125508e+09</td>\n",
       "      <td>Lg</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.050</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.395858</td>\n",
       "      <td>1.296030</td>\n",
       "      <td>2.398118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.713705</td>\n",
       "      <td>1.641215</td>\n",
       "      <td>0.940403</td>\n",
       "      <td>1.211333</td>\n",
       "      <td>1.909713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25092371</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125587e+09</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>0.175</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.746398</td>\n",
       "      <td>5.561389</td>\n",
       "      <td>2.205452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.155050</td>\n",
       "      <td>0.646231</td>\n",
       "      <td>0.997092</td>\n",
       "      <td>1.326985</td>\n",
       "      <td>4.507104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25094351</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125598e+09</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.400</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.816413</td>\n",
       "      <td>5.135686</td>\n",
       "      <td>8.557762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.682028</td>\n",
       "      <td>0.309588</td>\n",
       "      <td>2.783780</td>\n",
       "      <td>8.199625</td>\n",
       "      <td>2.007485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE  RETIME  \\\n",
       "0  25050735  URZ  1.125386e+09     Sx         regS    Sn        regS   1.000   \n",
       "1  25058004  URZ  1.125423e+09     Lg         regS    Sn        regS   1.925   \n",
       "2  25075768  URZ  1.125508e+09     Lg         regS    Sn        regS   1.050   \n",
       "3  25092371  URZ  1.125587e+09     Sn         regS    Sn        regS   0.175   \n",
       "4  25094351  URZ  1.125598e+09     Sn         regS    Sn        regS   1.400   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN     HVRATP      HVRAT  NAB   TAB  \\\n",
       "0      A  0.166667    ...     1.526004   0.786459   1.174982 -0.1 -0.27   \n",
       "1      A  0.444444    ...     2.762988  15.337161  15.337161 -0.1 -0.31   \n",
       "2      A  0.333333    ...     1.395858   1.296030   2.398118  0.0  0.00   \n",
       "3      A  0.166667    ...     1.746398   5.561389   2.205452  0.0  0.00   \n",
       "4      A  0.444444    ...     2.816413   5.135686   8.557762  0.0  0.00   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  1.216724  0.363039  2.166968  1.844924  0.626604  \n",
       "1  0.423843  1.384683  0.724611  6.547078  1.569841  \n",
       "2  0.713705  1.641215  0.940403  1.211333  1.909713  \n",
       "3  1.155050  0.646231  0.997092  1.326985  4.507104  \n",
       "4  1.682028  0.309588  2.783780  8.199625  2.007485  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_S_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='regP'\"\"\"\n",
    "df_P_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14972252</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.069735e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>2.94244</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.542596</td>\n",
       "      <td>0.124299</td>\n",
       "      <td>0.176237</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.509125</td>\n",
       "      <td>0.983261</td>\n",
       "      <td>0.619635</td>\n",
       "      <td>0.118464</td>\n",
       "      <td>0.058735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14992929</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070059e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301846</td>\n",
       "      <td>1.112025</td>\n",
       "      <td>0.258914</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>3.730434</td>\n",
       "      <td>0.738458</td>\n",
       "      <td>0.704325</td>\n",
       "      <td>0.247701</td>\n",
       "      <td>0.060989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15029724</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070386e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>0.65000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.828008</td>\n",
       "      <td>0.132138</td>\n",
       "      <td>0.192003</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.629496</td>\n",
       "      <td>0.632947</td>\n",
       "      <td>0.277982</td>\n",
       "      <td>0.542543</td>\n",
       "      <td>0.041916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15031571</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070403e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>3.22500</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>1.375773</td>\n",
       "      <td>0.282216</td>\n",
       "      <td>0.589090</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.924411</td>\n",
       "      <td>0.941318</td>\n",
       "      <td>0.304547</td>\n",
       "      <td>0.520954</td>\n",
       "      <td>0.131296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15073977</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070752e+09</td>\n",
       "      <td>Pg</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>3.529159</td>\n",
       "      <td>0.085323</td>\n",
       "      <td>0.486815</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.211185</td>\n",
       "      <td>0.225393</td>\n",
       "      <td>0.257161</td>\n",
       "      <td>0.413598</td>\n",
       "      <td>0.287086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE   RETIME  \\\n",
       "0  14972252  URZ  1.069735e+09     Pn         regP    Pn        regP  2.94244   \n",
       "1  14992929  URZ  1.070059e+09     Pn         regP    Pn        regP  0.00000   \n",
       "2  15029724  URZ  1.070386e+09     Pn         regP    Pn        regP  0.65000   \n",
       "3  15031571  URZ  1.070403e+09     Pn         regP    Pn        regP  3.22500   \n",
       "4  15073977  URZ  1.070752e+09     Pg         regP    Pn        regP  0.00000   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN    HVRATP     HVRAT  NAB    TAB  \\\n",
       "0      A  0.166667    ...     1.542596  0.124299  0.176237  0.1  0.100   \n",
       "1      A  0.333333    ...     1.301846  1.112025  0.258914  0.2  0.245   \n",
       "2      A  0.166667    ...     1.828008  0.132138  0.192003  0.2  0.375   \n",
       "3      A  0.444444    ...     1.375773  0.282216  0.589090  0.1  0.050   \n",
       "4      A  0.166667    ...     3.529159  0.085323  0.486815  0.1  0.290   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  0.509125  0.983261  0.619635  0.118464  0.058735  \n",
       "1  3.730434  0.738458  0.704325  0.247701  0.060989  \n",
       "2  0.629496  0.632947  0.277982  0.542543  0.041916  \n",
       "3  0.924411  0.941318  0.304547  0.520954  0.131296  \n",
       "4  0.211185  0.225393  0.257161  0.413598  0.287086  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='tele'\"\"\"\n",
    "df_T_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28541585</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143123e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.550</td>\n",
       "      <td>A</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.702892</td>\n",
       "      <td>0.042081</td>\n",
       "      <td>0.042081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.467387</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.057473</td>\n",
       "      <td>0.053682</td>\n",
       "      <td>0.463873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28556291</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143197e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.500</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.957241</td>\n",
       "      <td>0.223522</td>\n",
       "      <td>0.223522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.954016</td>\n",
       "      <td>0.705002</td>\n",
       "      <td>1.308694</td>\n",
       "      <td>0.193754</td>\n",
       "      <td>0.114283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28557837</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143204e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>3.100</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190828</td>\n",
       "      <td>0.240041</td>\n",
       "      <td>0.613747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.577677</td>\n",
       "      <td>1.020239</td>\n",
       "      <td>0.372745</td>\n",
       "      <td>0.277075</td>\n",
       "      <td>0.067225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28559193</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143210e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531207</td>\n",
       "      <td>0.084376</td>\n",
       "      <td>0.084376</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.689514</td>\n",
       "      <td>0.333741</td>\n",
       "      <td>0.336678</td>\n",
       "      <td>0.119060</td>\n",
       "      <td>0.092935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28561877</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143223e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.375</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.808070</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.097918</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.358785</td>\n",
       "      <td>0.042658</td>\n",
       "      <td>0.127151</td>\n",
       "      <td>0.064076</td>\n",
       "      <td>0.062188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE  RETIME  \\\n",
       "0  28541585  URZ  1.143123e+09      P         tele     P        tele   0.550   \n",
       "1  28556291  URZ  1.143197e+09      P         tele     P        tele   0.500   \n",
       "2  28557837  URZ  1.143204e+09      P         tele     P        tele   3.100   \n",
       "3  28559193  URZ  1.143210e+09      P         tele     P        tele   0.000   \n",
       "4  28561877  URZ  1.143223e+09      P         tele     P        tele   0.375   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN    HVRATP     HVRAT  NAB    TAB  \\\n",
       "0      A  0.666667    ...     1.702892  0.042081  0.042081  0.0  0.000   \n",
       "1      A  0.333333    ...     1.957241  0.223522  0.223522  0.0  0.000   \n",
       "2      A  0.333333    ...     1.190828  0.240041  0.613747  0.0 -0.195   \n",
       "3      A  0.333333    ...     1.531207  0.084376  0.084376  0.1  0.130   \n",
       "4      A  0.444444    ...     2.808070  0.038871  0.097918  0.1  0.205   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  0.467387  0.594238  0.057473  0.053682  0.463873  \n",
       "1  0.954016  0.705002  1.308694  0.193754  0.114283  \n",
       "2  0.577677  1.020239  0.372745  0.277075  0.067225  \n",
       "3  0.689514  0.333741  0.336678  0.119060  0.092935  \n",
       "4  0.358785  0.042658  0.127151  0.064076  0.062188  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_T_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='N'\"\"\"\n",
    "df_N_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13097443</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055511e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.948945</td>\n",
       "      <td>9.600216</td>\n",
       "      <td>9.600216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.396880</td>\n",
       "      <td>1.145887</td>\n",
       "      <td>0.421142</td>\n",
       "      <td>0.406116</td>\n",
       "      <td>1.439137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13097727</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055513e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>5.750848</td>\n",
       "      <td>9.726424</td>\n",
       "      <td>3.211865</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.628241</td>\n",
       "      <td>0.617270</td>\n",
       "      <td>0.890586</td>\n",
       "      <td>2.788352</td>\n",
       "      <td>1.279634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13097728</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055513e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.550819</td>\n",
       "      <td>0.163623</td>\n",
       "      <td>12.531935</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.625565</td>\n",
       "      <td>0.419386</td>\n",
       "      <td>0.502452</td>\n",
       "      <td>1.093746</td>\n",
       "      <td>0.228218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13097729</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055513e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2.203439</td>\n",
       "      <td>0.328290</td>\n",
       "      <td>0.511023</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.532591</td>\n",
       "      <td>1.455946</td>\n",
       "      <td>0.672186</td>\n",
       "      <td>0.730198</td>\n",
       "      <td>0.130826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13097946</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055517e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.803004</td>\n",
       "      <td>0.827978</td>\n",
       "      <td>0.630203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.039954</td>\n",
       "      <td>0.420590</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>0.450562</td>\n",
       "      <td>0.274970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE  RETIME  \\\n",
       "0  13097443  URZ  1.055511e+09      N            N  None           N     0.0   \n",
       "1  13097727  URZ  1.055513e+09      N            N  None           N     0.0   \n",
       "2  13097728  URZ  1.055513e+09      N            N  None           N     0.0   \n",
       "3  13097729  URZ  1.055513e+09      N            N  None           N     0.0   \n",
       "4  13097946  URZ  1.055517e+09      N            N  None           N     0.0   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN    HVRATP      HVRAT  NAB   TAB  \\\n",
       "0      A  0.166667    ...     2.948945  9.600216   9.600216  0.0  0.00   \n",
       "1      A  0.444444    ...     5.750848  9.726424   3.211865 -0.1 -0.04   \n",
       "2      A  1.000000    ...     1.550819  0.163623  12.531935  0.1  0.50   \n",
       "3      A  0.333333    ...     2.203439  0.328290   0.511023 -0.1 -0.50   \n",
       "4      A  1.000000    ...     2.803004  0.827978   0.630203  0.0  0.00   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  0.396880  1.145887  0.421142  0.406116  1.439137  \n",
       "1  0.628241  0.617270  0.890586  2.788352  1.279634  \n",
       "2  2.625565  0.419386  0.502452  1.093746  0.228218  \n",
       "3  0.532591  1.455946  0.672186  0.730198  0.130826  \n",
       "4  1.039954  0.420590  0.983108  0.450562  0.274970  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_N_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "#df_ora.to_csv('URZ_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regS (11135, 25)\n",
      "regP (11818, 25)\n",
      "tele (38083, 25)\n",
      "noise (301371, 25)\n"
     ]
    }
   ],
   "source": [
    "#how much data we have\n",
    "print('regS', df_S_all.shape)\n",
    "print('regP', df_P_all.shape)\n",
    "print('tele', df_T_all.shape)\n",
    "print('noise', df_N_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features count: 15\n"
     ]
    }
   ],
   "source": [
    "# columns corresponding to input\n",
    "x_indices = ['PER', 'RECT', 'PLANS', 'INANG1', 'INANG3', 'HMXMN', 'HVRATP', 'HVRAT', 'NAB', 'TAB',  \n",
    "             'HTOV1', 'HTOV2', 'HTOV3', 'HTOV4', 'HTOV5']\n",
    "print('features count:', len(x_indices))\n",
    "# columns corresponding to output\n",
    "y_indices = ['CLASS_PHASE']\n",
    "\n",
    "metadata = ['ARID','STA','TIME','IPHASE','CLASS_IPHASE','PHASE','CLASS_PHASE','RETIME','SOURCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for first phase of the cascade: N vs TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9133, 25) (9133, 25) (9133, 25)\n",
      "(27399, 25)\n",
      "(27399, 25)\n"
     ]
    }
   ],
   "source": [
    "#counts of all classes\n",
    "ns = df_S_all.shape[0]\n",
    "np = df_P_all.shape[0]\n",
    "nt = df_T_all.shape[0]\n",
    "nn = df_N_all.shape[0]\n",
    "\n",
    "#those from automatic\n",
    "nsa = df_S_all[df_S_all['SOURCE'] != 'M'].shape[0]\n",
    "npa = df_P_all[df_P_all['SOURCE'] != 'M'].shape[0]\n",
    "nta = df_T_all[df_T_all['SOURCE'] != 'M'].shape[0]\n",
    "nna = df_N_all[df_N_all['SOURCE'] != 'M'].shape[0]\n",
    "\n",
    "\n",
    "#we build a balanced datased - the same portion of regS, regP and tele\n",
    "#we have this count of phases\n",
    "samp_count = min(nsa, npa, nta)\n",
    "\n",
    "#sample TPS dataset, random_state is a seed\n",
    "ssS = df_S_all[df_S_all['SOURCE'] != 'M'].sample(samp_count, random_state=11)\n",
    "ssP = df_P_all[df_P_all['SOURCE'] != 'M'].sample(samp_count, random_state=13)\n",
    "ssT = df_T_all[df_T_all['SOURCE'] != 'M'].sample(samp_count, random_state=17)\n",
    "TPS_data = pd.concat([ssS, ssP, ssT])\n",
    "\n",
    "#sample noise phases\n",
    "N_data = df_N_all[df_N_all['SOURCE'] != 'M'].sample(3*samp_count, random_state=23)\n",
    "\n",
    "#lets shuffle dataset\n",
    "TPS_data = TPS_data.sample(frac=1, random_state=51).reset_index(drop=True)\n",
    "N_data = N_data.sample(frac=1, random_state=101).reset_index(drop=True)\n",
    "\n",
    "print(ssS.shape, ssP.shape, ssT.shape)\n",
    "print(TPS_data.shape)\n",
    "print(N_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of data\n",
    "\n",
    "* normalization according to iwt_nnet.c : iwt_normalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPS\n",
      "Series([], Name: HVRAT, dtype: float64)\n",
      "Series([], Name: HVRATP, dtype: float64)\n",
      "Series([], Name: HMXMN, dtype: float64)\n",
      "Series([], Name: HTOV1, dtype: float64)\n",
      "Series([], Name: HTOV2, dtype: float64)\n",
      "Series([], Name: HTOV3, dtype: float64)\n",
      "Series([], Name: HTOV4, dtype: float64)\n",
      "Series([], Name: HTOV5, dtype: float64)\n",
      "NOISE\n",
      "Series([], Name: HVRAT, dtype: float64)\n",
      "Series([], Name: HVRATP, dtype: float64)\n",
      "Series([], Name: HMXMN, dtype: float64)\n",
      "Series([], Name: HTOV1, dtype: float64)\n",
      "Series([], Name: HTOV2, dtype: float64)\n",
      "Series([], Name: HTOV3, dtype: float64)\n",
      "Series([], Name: HTOV4, dtype: float64)\n",
      "Series([], Name: HTOV5, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# check on positivity of all features which are going to be log10-ed:)\n",
    "# check TPS data\n",
    "print('TPS')\n",
    "print(TPS_data['HVRAT'][TPS_data['HVRAT']<=0])\n",
    "print(TPS_data['HVRATP'][TPS_data['HVRATP']<=0])\n",
    "print(TPS_data['HMXMN'][TPS_data['HMXMN']<=0])\n",
    "print(TPS_data['HTOV1'][TPS_data['HTOV1']<=0])\n",
    "print(TPS_data['HTOV2'][TPS_data['HTOV2']<=0])\n",
    "print(TPS_data['HTOV3'][TPS_data['HTOV3']<=0])\n",
    "print(TPS_data['HTOV4'][TPS_data['HTOV4']<=0])\n",
    "print(TPS_data['HTOV5'][TPS_data['HTOV5']<=0])\n",
    "# check NOISE data\n",
    "print('NOISE')\n",
    "print(N_data['HVRAT'][N_data['HVRAT']<=0])\n",
    "print(N_data['HVRATP'][N_data['HVRATP']<=0])\n",
    "print(N_data['HMXMN'][N_data['HMXMN']<=0])\n",
    "print(N_data['HTOV1'][N_data['HTOV1']<=0])\n",
    "print(N_data['HTOV2'][N_data['HTOV2']<=0])\n",
    "print(N_data['HTOV3'][N_data['HTOV3']<=0])\n",
    "print(N_data['HTOV4'][N_data['HTOV4']<=0])\n",
    "print(N_data['HTOV5'][N_data['HTOV5']<=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize TPS\n",
    "TPS_data_norm = TPS_data.copy(deep=True)\n",
    "TPS_data_norm['INANG1'] /= 90.\n",
    "TPS_data_norm['INANG3'] /= 90.\n",
    "TPS_data_norm['HMXMN'] = numpy.log10(TPS_data['HMXMN'])\n",
    "TPS_data_norm['HVRATP'] = numpy.log10(TPS_data['HVRATP'])\n",
    "TPS_data_norm['HVRAT'] = numpy.log10(TPS_data['HVRAT'])\n",
    "TPS_data_norm['HTOV1'] = numpy.log10(TPS_data['HTOV1'])\n",
    "TPS_data_norm['HTOV2'] = numpy.log10(TPS_data['HTOV2'])\n",
    "TPS_data_norm['HTOV3'] = numpy.log10(TPS_data['HTOV3'])\n",
    "TPS_data_norm['HTOV4'] = numpy.log10(TPS_data['HTOV4'])\n",
    "TPS_data_norm['HTOV5'] = numpy.log10(TPS_data['HTOV5'])\n",
    "\n",
    "# normalize NOISE\n",
    "N_data_norm = N_data.copy(deep=True)\n",
    "N_data_norm['INANG1'] /= 90.\n",
    "N_data_norm['INANG3'] /= 90.\n",
    "N_data_norm['HMXMN'] = numpy.log10(N_data['HMXMN'])\n",
    "N_data_norm['HVRATP'] = numpy.log10(N_data['HVRATP'])\n",
    "N_data_norm['HVRAT'] = numpy.log10(N_data['HVRAT'])\n",
    "N_data_norm['HTOV1'] = numpy.log10(N_data['HTOV1'])\n",
    "N_data_norm['HTOV2'] = numpy.log10(N_data['HTOV2'])\n",
    "N_data_norm['HTOV3'] = numpy.log10(N_data['HTOV3'])\n",
    "N_data_norm['HTOV4'] = numpy.log10(N_data['HTOV4'])\n",
    "N_data_norm['HTOV5'] = numpy.log10(N_data['HTOV5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same for manually added arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train/test ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count= 20549 test count= 6850\n"
     ]
    }
   ],
   "source": [
    "train_test_split_ratio = 0.75\n",
    "samp_count_train = int(TPS_data_norm.shape[0] * train_test_split_ratio)\n",
    "samp_count_test = TPS_data_norm.shape[0] - samp_count_train\n",
    "print('train count=', samp_count_train, 'test count=', samp_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPS train: (20549, 25) TPS test: (6850, 25)\n",
      "N train:   (20549, 25) N test:   (6850, 25)\n"
     ]
    }
   ],
   "source": [
    "TPS_train = TPS_data_norm[:samp_count_train]\n",
    "TPS_test = TPS_data_norm[samp_count_train:]\n",
    "\n",
    "N_train = N_data_norm[:samp_count_train]\n",
    "N_test = N_data_norm[samp_count_train:]\n",
    "\n",
    "print('TPS train:',TPS_train.shape,'TPS test:',TPS_test.shape)\n",
    "print('N train:  ',N_train.shape,  'N test:  ',N_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check interclass balance of TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T in TPS train:    (6879, 25)\n",
      "regP in TPS train: (6885, 25)\n",
      "regS in TPS train: (6785, 25)\n",
      "T in TPS test:     (2254, 25)\n",
      "regP in TPS test:  (2248, 25)\n",
      "regS in TPS test:  (2348, 25)\n"
     ]
    }
   ],
   "source": [
    "print('T in TPS train:   ', TPS_train[TPS_train['CLASS_PHASE']=='tele'].shape)\n",
    "print('regP in TPS train:', TPS_train[TPS_train['CLASS_PHASE']=='regP'].shape)\n",
    "print('regS in TPS train:', TPS_train[TPS_train['CLASS_PHASE']=='regS'].shape)\n",
    "\n",
    "print('T in TPS test:    ', TPS_test[TPS_test['CLASS_PHASE']=='tele'].shape)\n",
    "print('regP in TPS test: ', TPS_test[TPS_test['CLASS_PHASE']=='regP'].shape)\n",
    "print('regS in TPS test: ', TPS_test[TPS_test['CLASS_PHASE']=='regS'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form train and test sets and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([TPS_train, N_train]).sample(frac=1, random_state=31).reset_index(drop=True)\n",
    "test =  pd.concat([TPS_test, N_test]).sample(frac=1, random_state=33).reset_index(drop=True)\n",
    "#train.apply(pd.to_numeric, errors='ignore')\n",
    "#test.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41098, 15) (41098, 2) (13700, 15) (13700, 2)\n",
      "(13700, 9)\n"
     ]
    }
   ],
   "source": [
    "train_X = train[x_indices].values.astype(float)\n",
    "train_Y = train[y_indices]\n",
    "\n",
    "test_X = test[x_indices].values.astype(float)\n",
    "test_Y = test[y_indices]\n",
    "\n",
    "train_Y_ = numpy.array(numpy.where(train_Y == 'N', 0, 1), dtype=float)\n",
    "test_Y_ = numpy.array(numpy.where(test_Y == 'N', 0, 1), dtype=float)\n",
    "\n",
    "#convert to categorical\n",
    "train_Y = keras.utils.to_categorical(train_Y_)\n",
    "test_Y = keras.utils.to_categorical(test_Y_)\n",
    "\n",
    "test_metadata = test[metadata]\n",
    "\n",
    "print(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)\n",
    "print(test_metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth for all 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvlscratch/SHI/users/hofman/ML/env/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 0, 1, 0, 0, 0, 3, 3, 1, 1, 3, 1, 0, 2, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['N', 'regP', 'regS', 'tele'])\n",
    "test_Y_GT = le.transform(test[y_indices])\n",
    "train_Y_GT = le.transform(train[y_indices])\n",
    "test_Y_GT[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = len(x_indices)\n",
    "numpy.random.seed(11)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=n_input, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1., clipvalue=0.5)\n",
    "adam = Adam(lr=0.0001) #, clipnorm, clipvalue=0.5)\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy', \n",
    "    optimizer = 'adam',  # adam, sgd\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 6)                 96        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_keys = ('acc', 'val_acc', 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {k : [] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41098 samples, validate on 13700 samples\n",
      "Epoch 1/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 2/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 3/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3584 - val_acc: 0.8401\n",
      "Epoch 4/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3452 - acc: 0.8448 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 5/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 6/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8402\n",
      "Epoch 7/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8397\n",
      "Epoch 8/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3589 - val_acc: 0.8401\n",
      "Epoch 9/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8406\n",
      "Epoch 10/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8403\n",
      "Epoch 11/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 12/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 13/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8409\n",
      "Epoch 14/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 15/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8407\n",
      "Epoch 16/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8394\n",
      "Epoch 17/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 18/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3584 - val_acc: 0.8399\n",
      "Epoch 19/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 20/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 21/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3581 - val_acc: 0.8396\n",
      "Epoch 22/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 23/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 24/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 25/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3588 - val_acc: 0.8406\n",
      "Epoch 26/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 27/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8409\n",
      "Epoch 28/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8408\n",
      "Epoch 29/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 30/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3451 - acc: 0.8453 - val_loss: 0.3580 - val_acc: 0.8404\n",
      "Epoch 31/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3586 - val_acc: 0.8402\n",
      "Epoch 32/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8458 - val_loss: 0.3580 - val_acc: 0.8395\n",
      "Epoch 33/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8400\n",
      "Epoch 34/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 35/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3587 - val_acc: 0.8399\n",
      "Epoch 36/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 37/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 38/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 39/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 40/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 41/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 42/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8398\n",
      "Epoch 43/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8398\n",
      "Epoch 44/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 45/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8408\n",
      "Epoch 46/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 47/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8396\n",
      "Epoch 48/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8400\n",
      "Epoch 49/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3586 - val_acc: 0.8401\n",
      "Epoch 50/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8456 - val_loss: 0.3584 - val_acc: 0.8399\n",
      "Epoch 51/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 52/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8399\n",
      "Epoch 53/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 54/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3581 - val_acc: 0.8400\n",
      "Epoch 55/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 56/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 57/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 58/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 59/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 61/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3580 - val_acc: 0.8400\n",
      "Epoch 62/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 63/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 64/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 65/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 66/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3585 - val_acc: 0.8399\n",
      "Epoch 67/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 68/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 69/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8399\n",
      "Epoch 70/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 71/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 72/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3452 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 73/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 74/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 75/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8399\n",
      "Epoch 76/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 77/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 78/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 79/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 80/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8396\n",
      "Epoch 81/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8400\n",
      "Epoch 82/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 83/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 84/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 85/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8399\n",
      "Epoch 86/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 87/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 88/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8405\n",
      "Epoch 89/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8402\n",
      "Epoch 90/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 91/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8449 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 92/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 93/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 94/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 95/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8449 - val_loss: 0.3582 - val_acc: 0.8392\n",
      "Epoch 96/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3452 - acc: 0.8446 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 97/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 98/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8404\n",
      "Epoch 99/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 100/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 101/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 102/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 103/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 104/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 105/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3584 - val_acc: 0.8405\n",
      "Epoch 106/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 107/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8402\n",
      "Epoch 108/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3585 - val_acc: 0.8403\n",
      "Epoch 109/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3452 - acc: 0.8447 - val_loss: 0.3587 - val_acc: 0.8400\n",
      "Epoch 110/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8393\n",
      "Epoch 111/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 112/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 113/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8400\n",
      "Epoch 114/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3581 - val_acc: 0.8393\n",
      "Epoch 115/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8398\n",
      "Epoch 116/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 117/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 118/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3585 - val_acc: 0.8402\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8449 - val_loss: 0.3582 - val_acc: 0.8397\n",
      "Epoch 120/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8399\n",
      "Epoch 121/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 122/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3584 - val_acc: 0.8408\n",
      "Epoch 123/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8393\n",
      "Epoch 124/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3584 - val_acc: 0.8401\n",
      "Epoch 125/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 126/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 127/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 128/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 129/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 130/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8398\n",
      "Epoch 131/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3452 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 132/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 133/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 134/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3583 - val_acc: 0.8396\n",
      "Epoch 135/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 136/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 137/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8399\n",
      "Epoch 138/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8395\n",
      "Epoch 139/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8409\n",
      "Epoch 140/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 141/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 142/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 143/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 144/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8399\n",
      "Epoch 145/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 146/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 147/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8395\n",
      "Epoch 148/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8407\n",
      "Epoch 149/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 150/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8459 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 151/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8397\n",
      "Epoch 152/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8402\n",
      "Epoch 153/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8397\n",
      "Epoch 154/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 155/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 156/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8450 - val_loss: 0.3582 - val_acc: 0.8397\n",
      "Epoch 157/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 158/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3579 - val_acc: 0.8399\n",
      "Epoch 159/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3452 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8397\n",
      "Epoch 160/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3451 - acc: 0.8461 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 161/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 162/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3584 - val_acc: 0.8403\n",
      "Epoch 163/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8459 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 164/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 165/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3584 - val_acc: 0.8402\n",
      "Epoch 166/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 167/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8395\n",
      "Epoch 168/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8402\n",
      "Epoch 169/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 170/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8408\n",
      "Epoch 171/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8398\n",
      "Epoch 172/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3582 - val_acc: 0.8409\n",
      "Epoch 173/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8395\n",
      "Epoch 174/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 175/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3584 - val_acc: 0.8401\n",
      "Epoch 176/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3584 - val_acc: 0.8401\n",
      "Epoch 177/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 179/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 180/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 181/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8403\n",
      "Epoch 182/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3583 - val_acc: 0.8399\n",
      "Epoch 183/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 184/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8406\n",
      "Epoch 185/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 186/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8458 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 187/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3584 - val_acc: 0.8401\n",
      "Epoch 188/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 189/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 190/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8396\n",
      "Epoch 191/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3585 - val_acc: 0.8397\n",
      "Epoch 192/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 193/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3585 - val_acc: 0.8400\n",
      "Epoch 194/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.3588 - val_acc: 0.8400\n",
      "Epoch 195/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8448 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 196/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 197/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3584 - val_acc: 0.8403\n",
      "Epoch 198/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8400\n",
      "Epoch 199/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 200/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8408\n",
      "Epoch 201/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 202/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 203/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 204/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3452 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 205/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 206/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3586 - val_acc: 0.8399\n",
      "Epoch 207/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 208/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 209/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 210/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 211/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8395\n",
      "Epoch 212/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8459 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 213/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 214/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 215/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 216/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 217/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 218/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 219/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 220/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3585 - val_acc: 0.8402\n",
      "Epoch 221/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 222/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3579 - val_acc: 0.8404\n",
      "Epoch 223/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 224/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 225/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8400\n",
      "Epoch 226/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 227/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8407\n",
      "Epoch 228/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8401\n",
      "Epoch 229/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 230/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8403\n",
      "Epoch 231/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 232/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 233/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 234/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8400\n",
      "Epoch 235/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3584 - val_acc: 0.8402\n",
      "Epoch 236/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8394\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3581 - val_acc: 0.8397\n",
      "Epoch 238/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8396\n",
      "Epoch 239/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 240/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 241/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 242/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8407\n",
      "Epoch 243/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 244/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3579 - val_acc: 0.8401\n",
      "Epoch 245/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 246/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3584 - val_acc: 0.8405\n",
      "Epoch 247/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3580 - val_acc: 0.8398\n",
      "Epoch 248/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3451 - acc: 0.8457 - val_loss: 0.3584 - val_acc: 0.8407\n",
      "Epoch 249/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 250/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3580 - val_acc: 0.8398\n",
      "Epoch 251/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 252/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8453 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 253/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8412\n",
      "Epoch 254/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 255/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 256/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 257/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8402\n",
      "Epoch 258/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 259/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8408\n",
      "Epoch 260/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 261/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8450 - val_loss: 0.3582 - val_acc: 0.8408\n",
      "Epoch 262/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8404\n",
      "Epoch 263/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8409\n",
      "Epoch 264/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8408\n",
      "Epoch 265/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8408\n",
      "Epoch 266/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 267/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 268/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8408\n",
      "Epoch 269/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3584 - val_acc: 0.8409\n",
      "Epoch 270/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 271/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8408\n",
      "Epoch 272/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 273/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 274/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 275/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8397\n",
      "Epoch 276/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8393\n",
      "Epoch 277/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 278/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 279/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 280/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8449 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 281/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 282/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 283/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 284/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8450 - val_loss: 0.3580 - val_acc: 0.8398\n",
      "Epoch 285/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 286/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8402\n",
      "Epoch 287/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 288/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8408\n",
      "Epoch 289/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3586 - val_acc: 0.8401\n",
      "Epoch 290/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3584 - val_acc: 0.8403\n",
      "Epoch 291/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8459 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 292/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8450 - val_loss: 0.3584 - val_acc: 0.8400\n",
      "Epoch 293/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8396\n",
      "Epoch 294/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3451 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8400\n",
      "Epoch 295/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3450 - acc: 0.8447 - val_loss: 0.3583 - val_acc: 0.8407\n",
      "Epoch 297/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 298/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 299/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3580 - val_acc: 0.8410\n",
      "Epoch 300/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 301/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8450 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 302/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 303/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3584 - val_acc: 0.8406\n",
      "Epoch 304/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 305/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3585 - val_acc: 0.8400\n",
      "Epoch 306/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 307/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8398\n",
      "Epoch 308/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8403\n",
      "Epoch 309/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3585 - val_acc: 0.8402\n",
      "Epoch 310/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8458 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 311/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 312/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 313/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 314/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8460 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 315/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 316/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8397\n",
      "Epoch 317/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 318/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8407\n",
      "Epoch 319/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8408\n",
      "Epoch 320/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 321/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 322/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3579 - val_acc: 0.8402\n",
      "Epoch 323/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 324/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8400\n",
      "Epoch 325/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8450 - val_loss: 0.3585 - val_acc: 0.8403\n",
      "Epoch 326/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 327/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 328/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8398\n",
      "Epoch 329/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 330/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8396\n",
      "Epoch 331/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 332/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 333/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 334/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 335/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 336/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 337/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8398\n",
      "Epoch 338/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3587 - val_acc: 0.8404\n",
      "Epoch 339/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 340/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8408\n",
      "Epoch 341/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 342/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8449 - val_loss: 0.3580 - val_acc: 0.8405\n",
      "Epoch 343/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3448 - acc: 0.8456 - val_loss: 0.3584 - val_acc: 0.8402\n",
      "Epoch 344/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 345/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3585 - val_acc: 0.8399\n",
      "Epoch 346/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8397\n",
      "Epoch 347/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8458 - val_loss: 0.3581 - val_acc: 0.8400\n",
      "Epoch 348/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 349/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3586 - val_acc: 0.8398\n",
      "Epoch 350/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8451 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 351/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8396\n",
      "Epoch 352/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 353/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3585 - val_acc: 0.8404\n",
      "Epoch 354/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 355/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 356/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 357/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8451 - val_loss: 0.3580 - val_acc: 0.8400\n",
      "Epoch 358/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 359/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 360/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 361/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3584 - val_acc: 0.8393\n",
      "Epoch 362/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8396\n",
      "Epoch 363/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 364/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3586 - val_acc: 0.8399\n",
      "Epoch 365/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8458 - val_loss: 0.3581 - val_acc: 0.8402\n",
      "Epoch 366/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3583 - val_acc: 0.8403\n",
      "Epoch 367/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3580 - val_acc: 0.8404\n",
      "Epoch 368/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 369/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3585 - val_acc: 0.8405\n",
      "Epoch 370/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 371/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3585 - val_acc: 0.8399\n",
      "Epoch 372/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8451 - val_loss: 0.3582 - val_acc: 0.8397\n",
      "Epoch 373/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 374/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8407\n",
      "Epoch 375/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8405\n",
      "Epoch 376/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3580 - val_acc: 0.8405\n",
      "Epoch 377/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 378/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 379/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 380/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3579 - val_acc: 0.8399\n",
      "Epoch 381/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 382/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8402\n",
      "Epoch 383/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 384/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 385/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 386/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 387/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 388/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 389/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8406\n",
      "Epoch 390/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 391/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8402\n",
      "Epoch 392/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 393/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8400\n",
      "Epoch 394/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3584 - val_acc: 0.8407\n",
      "Epoch 395/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 396/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3448 - acc: 0.8455 - val_loss: 0.3585 - val_acc: 0.8398\n",
      "Epoch 397/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8455 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 398/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8402\n",
      "Epoch 399/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 400/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8396\n",
      "Epoch 401/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3448 - acc: 0.8457 - val_loss: 0.3585 - val_acc: 0.8400\n",
      "Epoch 402/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 403/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8451 - val_loss: 0.3580 - val_acc: 0.8404\n",
      "Epoch 404/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 405/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 406/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8402\n",
      "Epoch 407/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8407\n",
      "Epoch 408/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8459 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 409/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 410/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3585 - val_acc: 0.8402\n",
      "Epoch 411/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8400\n",
      "Epoch 412/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8398\n",
      "Epoch 413/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 414/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8454 - val_loss: 0.3584 - val_acc: 0.8399\n",
      "Epoch 415/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 416/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8408\n",
      "Epoch 417/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 418/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8408\n",
      "Epoch 419/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 420/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 421/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 422/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3580 - val_acc: 0.8405\n",
      "Epoch 423/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8403\n",
      "Epoch 424/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 425/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3587 - val_acc: 0.8398\n",
      "Epoch 426/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8448 - val_loss: 0.3581 - val_acc: 0.8400\n",
      "Epoch 427/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8460 - val_loss: 0.3585 - val_acc: 0.8401\n",
      "Epoch 428/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8402\n",
      "Epoch 429/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3580 - val_acc: 0.8400\n",
      "Epoch 430/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 431/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 432/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3450 - acc: 0.8458 - val_loss: 0.3582 - val_acc: 0.8399\n",
      "Epoch 433/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8402\n",
      "Epoch 434/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8400\n",
      "Epoch 435/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3583 - val_acc: 0.8399\n",
      "Epoch 436/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 437/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8405\n",
      "Epoch 438/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8409\n",
      "Epoch 439/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 440/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8460 - val_loss: 0.3581 - val_acc: 0.8408\n",
      "Epoch 441/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 442/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 443/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3579 - val_acc: 0.8402\n",
      "Epoch 444/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8398\n",
      "Epoch 445/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 446/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3582 - val_acc: 0.8406\n",
      "Epoch 447/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 448/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3581 - val_acc: 0.8395\n",
      "Epoch 449/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3580 - val_acc: 0.8407\n",
      "Epoch 450/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 451/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8407\n",
      "Epoch 452/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3585 - val_acc: 0.8397\n",
      "Epoch 453/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3580 - val_acc: 0.8405\n",
      "Epoch 454/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 455/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3583 - val_acc: 0.8401\n",
      "Epoch 456/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3448 - acc: 0.8457 - val_loss: 0.3584 - val_acc: 0.8400\n",
      "Epoch 457/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3584 - val_acc: 0.8402\n",
      "Epoch 458/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 459/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 460/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 461/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 462/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8403\n",
      "Epoch 463/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8400\n",
      "Epoch 464/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 465/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 466/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3584 - val_acc: 0.8401\n",
      "Epoch 467/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8450 - val_loss: 0.3582 - val_acc: 0.8396\n",
      "Epoch 468/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8453 - val_loss: 0.3581 - val_acc: 0.8399\n",
      "Epoch 469/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 470/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8403\n",
      "Epoch 471/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8403\n",
      "Epoch 472/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8409\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8401\n",
      "Epoch 474/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3448 - acc: 0.8454 - val_loss: 0.3579 - val_acc: 0.8398\n",
      "Epoch 475/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3580 - val_acc: 0.8408\n",
      "Epoch 476/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3448 - acc: 0.8454 - val_loss: 0.3584 - val_acc: 0.8399\n",
      "Epoch 477/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3450 - acc: 0.8449 - val_loss: 0.3580 - val_acc: 0.8399\n",
      "Epoch 478/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8402\n",
      "Epoch 479/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8406\n",
      "Epoch 480/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3580 - val_acc: 0.8401\n",
      "Epoch 481/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3583 - val_acc: 0.8404\n",
      "Epoch 482/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8401\n",
      "Epoch 483/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3449 - acc: 0.8452 - val_loss: 0.3580 - val_acc: 0.8406\n",
      "Epoch 484/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3448 - acc: 0.8458 - val_loss: 0.3581 - val_acc: 0.8406\n",
      "Epoch 485/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8396\n",
      "Epoch 486/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3581 - val_acc: 0.8407\n",
      "Epoch 487/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 488/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3448 - acc: 0.8458 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 489/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8455 - val_loss: 0.3584 - val_acc: 0.8404\n",
      "Epoch 490/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.3579 - val_acc: 0.8407\n",
      "Epoch 491/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3581 - val_acc: 0.8404\n",
      "Epoch 492/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3580 - val_acc: 0.8398\n",
      "Epoch 493/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3449 - acc: 0.8458 - val_loss: 0.3580 - val_acc: 0.8404\n",
      "Epoch 494/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8452 - val_loss: 0.3582 - val_acc: 0.8404\n",
      "Epoch 495/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3448 - acc: 0.8455 - val_loss: 0.3580 - val_acc: 0.8396\n",
      "Epoch 496/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3580 - val_acc: 0.8396\n",
      "Epoch 497/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3448 - acc: 0.8457 - val_loss: 0.3582 - val_acc: 0.8411\n",
      "Epoch 498/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8456 - val_loss: 0.3583 - val_acc: 0.8409\n",
      "Epoch 499/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8454 - val_loss: 0.3584 - val_acc: 0.8396\n",
      "Epoch 500/500\n",
      "41098/41098 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8456 - val_loss: 0.3581 - val_acc: 0.8405\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_X[:], train_Y[:], \n",
    "          epochs=500, \n",
    "          batch_size=1024, \n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          #validation_split=0.1)\n",
    "          validation_data=(test_X, test_Y))\n",
    "\n",
    "history = {k : history[k] + h.history[k] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13700,), (13700,))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[train_Y==0].shape, train_Y[train_Y==1].shape\n",
    "test_Y[test_Y==0].shape, test_Y[test_Y==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d24e92b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history, semilog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41654223 0.58345777]\n",
      " [0.81460977 0.18539025]\n",
      " [0.5408284  0.45917156]\n",
      " [0.7370314  0.26296863]\n",
      " [0.36804074 0.631959  ]\n",
      " [0.9621731  0.03782687]\n",
      " [0.7224118  0.2775882 ]\n",
      " [0.95568895 0.04431105]\n",
      " [0.3493296  0.6506703 ]\n",
      " [0.17895813 0.8210419 ]]\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted values\n",
    "Y_pred_ = model.predict(test_X)\n",
    "# predictions are outputted as floats from [0,1]\n",
    "print(Y_pred_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = numpy.argmax(Y_pred_, axis=1)\n",
    "Y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5757 1093]\n",
      " [1092 5758]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# we must translate it to 0, 1 based on thresholding at 0.5\n",
    "# where < 0.5 set to 0, to 1 otherwise\n",
    "#Y_pred = numpy.where(Y_pred < 0.5, 0, 1)\n",
    "\n",
    "# calculate confusion matrix\n",
    "conf_mat = confusion_matrix(test_Y_, Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17266  3283]\n",
      " [ 3061 17488]]\n"
     ]
    }
   ],
   "source": [
    "## confusion matrix on Train?\n",
    "Y_pred = numpy.argmax(model.predict(train_X), axis=1)\n",
    "conf_mat = confusion_matrix(train_Y_, Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13700/13700 [==============================] - 0s 9us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3580600393775606, 0.8405109489051095]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval on test data\n",
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34474496577417096, 0.8456372572845586]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model and weights\n",
    "model.save('URZ_model_15-6-2_norm_NTPS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network TP vs S \n",
    "\n",
    "* we need a new dataset for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for TP vs S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20549, 25) (6850, 25)\n"
     ]
    }
   ],
   "source": [
    "print(TPS_train.shape, TPS_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20549, 15) (20549, 2) (6850, 15) (6850, 2)\n"
     ]
    }
   ],
   "source": [
    "train_X_TPS = TPS_train[x_indices].values.astype(float)\n",
    "train_Y_TPS = TPS_train[y_indices]\n",
    "\n",
    "test_X_TPS = TPS_test[x_indices].values.astype(float)\n",
    "test_Y_TPS = TPS_test[y_indices]\n",
    "\n",
    "#regS = 0, T/regP = 1\n",
    "train_Y_TPS_ = numpy.array(numpy.where(train_Y_TPS['CLASS_PHASE'] == 'regS', 0, 1), dtype=float)\n",
    "test_Y_TPS_ = numpy.array(numpy.where(test_Y_TPS['CLASS_PHASE'] == 'regS', 0, 1), dtype=float)\n",
    "\n",
    "#convert to categorical\n",
    "train_Y_TPS = keras.utils.to_categorical(train_Y_TPS_)\n",
    "test_Y_TPS = keras.utils.to_categorical(test_Y_TPS_)\n",
    "\n",
    "print(train_X_TPS.shape, train_Y_TPS.shape, test_X_TPS.shape, test_Y_TPS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually added datasets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6006, 15) (6006, 2) (6006, 1)\n"
     ]
    }
   ],
   "source": [
    "#those manually added\n",
    "nsm = df_S_all[df_S_all['SOURCE'] == 'M'].shape[0]\n",
    "npm = df_P_all[df_P_all['SOURCE'] == 'M'].shape[0]\n",
    "ntm = df_T_all[df_T_all['SOURCE'] == 'M'].shape[0]\n",
    "nnm = df_N_all[df_N_all['SOURCE'] == 'M'].shape[0]\n",
    "\n",
    "#we build a balanced datased - the same portion of regS, regP and tele\n",
    "#we have this count of phases\n",
    "man_samp_count = min(nsm, npm, ntm)\n",
    "\n",
    "#sample TPS dataset, random_state is a seed\n",
    "mssS = df_S_all[df_S_all['SOURCE'] == 'M'].sample(man_samp_count, random_state=11)\n",
    "mssP = df_P_all[df_P_all['SOURCE'] == 'M'].sample(man_samp_count)\n",
    "mssT = df_T_all[df_T_all['SOURCE'] == 'M'].sample(man_samp_count)\n",
    "MTPS_data = pd.concat([mssS, mssP, mssT])\n",
    "\n",
    "\n",
    "#normalize\n",
    "\n",
    "MTPS_data_norm = MTPS_data.copy(deep=True)\n",
    "MTPS_data_norm['INANG1'] /= 90.\n",
    "MTPS_data_norm['INANG3'] /= 90.\n",
    "MTPS_data_norm['HMXMN'] = numpy.log10(MTPS_data['HMXMN'])\n",
    "MTPS_data_norm['HVRATP'] = numpy.log10(MTPS_data['HVRATP'])\n",
    "MTPS_data_norm['HVRAT'] = numpy.log10(MTPS_data['HVRAT'])\n",
    "MTPS_data_norm['HTOV1'] = numpy.log10(MTPS_data['HTOV1'])\n",
    "MTPS_data_norm['HTOV2'] = numpy.log10(MTPS_data['HTOV2'])\n",
    "MTPS_data_norm['HTOV3'] = numpy.log10(MTPS_data['HTOV3'])\n",
    "MTPS_data_norm['HTOV4'] = numpy.log10(MTPS_data['HTOV4'])\n",
    "MTPS_data_norm['HTOV5'] = numpy.log10(MTPS_data['HTOV5'])\n",
    "\n",
    "#manually added noise makes nos sense - we do not sanmple N\n",
    "\n",
    "#lets shuffle dataset\n",
    "MTPS_data_norm = MTPS_data_norm.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "manual_X = MTPS_data_norm[x_indices].values.astype(float)\n",
    "#regS = 0, T/regP = 1\n",
    "manual_Y_TPS_ = numpy.array(numpy.where(MTPS_data_norm[y_indices] == 'regS', 0, 1), dtype=float)\n",
    "manual_Y_TPS = keras.utils.to_categorical(manual_Y_TPS_)\n",
    "\n",
    "print(manual_X.shape, manual_Y_TPS.shape, manual_Y_TPS_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual dataset ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvlscratch/SHI/users/hofman/ML/env/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 2, 3, 3, 2, 2, 1, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_Y_GT = le.transform(MTPS_data_norm[y_indices])\n",
    "manual_Y_GT[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_TPS = {k : [] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = len(x_indices)\n",
    "numpy.random.seed(11)\n",
    "\n",
    "# create model\n",
    "model_TPS = Sequential()\n",
    "model_TPS.add(Dense(6, input_dim=n_input, activation='sigmoid'))\n",
    "model_TPS.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_TPS.compile(\n",
    "    loss = 'binary_crossentropy', \n",
    "    optimizer = 'adam',  # adam, sgd\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 6)                 96        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_TPS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model_TPS.fit(train_X_TPS[:], train_Y_TPS[:], \n",
    "          epochs=5000, \n",
    "          batch_size=1024, \n",
    "          verbose=0,\n",
    "          shuffle=True,\n",
    "          #validation_split=0.1)\n",
    "          validation_data=([test_X_TPS, test_Y_TPS]))\n",
    "\n",
    "history_TPS = {k : history_TPS[k] + h.history[k] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmcVOWd7/HPr6p6YWnWRrYGgQQ3bHFpvWjiHg2aRLJocI8m0TvGuMRcJxjNxBgzk+1mJr6G0XAdjTou4JYwxsiNSiTeKAEMiiAgokKDyiI79FJVv/vHOV0WLXRXd1d1VVd9369Xvfqc5zzn1O+pB+pXZ3uOuTsiIiIAkXwHICIihUNJQUREUpQUREQkRUlBRERSlBRERCRFSUFERFKUFEREJEVJQUREUpQUREQkJZbvADqqurrax4wZk+8wRER6lEWLFm1y9yHt1etxSWHMmDEsXLgw32GIiPQoZvZuJvV0+EhERFKUFEREJKXHHT4qee5gBok4JOMQicK2tVDWG3pXQ/Pu4LV2PlSNgP/6MjRuh6G1UFEF770Kzbv23uaBn4Leg+GN2Z2PK1IGyeauta09E74MS5/Y97KDJsPKZ3Lzvr0Hw4EnwBv/vXf5J06Ht57LfDtjToR3/tLx9z/6Utj9ISx/av91hhwCG5d/vLx3Neze1PH3zKbeg2H35vzGkKnqg2D7ehhWC2teync0H3fxE/DJ03P6FtbThs6uq6vzLp9TmD8D/nhjx9b5/noo79N2nUQc1r8Crz8BFX1hxFHw6sNw0FnwzLTgy1k67a3kcD4ReS/fYaTEPULMkjl/nw98AENta87fJ1eaPUqZJfIdRlFYesrdTDjlvE6ta2aL3L2uvXoltafQ/Nw/U/aXn3Vq3cafjafiB+v3LmzaBf88ov2VW//C7CF+2XwehmPAn5MTWetDOCSylqRH2EUFy300g9iBY2yjD4fau+yikj400MuaqPdqNnl/jOCHRyPl9KaBPVTgGBGSxIkCFr6jp01nwjEcTx0FbVk/eL+B7GAPFfSikS1U0YcGdlNBlCQRnDLiNBOjiRhgREmQIJK2TSeCk9znUdbWsXY09szat+/3cA6oqmTDjsY23nt/8XQ87uH9K3lvW8M+l/WtiLGzMb7fdasqY+wIl0dIkgzfawjb+JAqEkQBsHBJX/awk14kMSI4LT9ZYyRJEEn1RT92ESHJVqqIESdBBCeChX2bIEoFTcSJEgvXbKSckbaRZo/hGEf3/oBnd48nidGHhvB9I3t9JmUEsTcTZQA7KSdOnCjNxGigPGzLVtYzmBgJqthNggi9+w5gvK/mvXg/tjYaldYEwCbvR41tZK0fAAT/J1r6oTeNNBOjmVgqhkFsp4w4HzAQgDsGHs2ENnur60oqKXQ2IQC8WHYC6TttO1YvoOr+z7S5zkPxU9lDJX9LHswF0bkMsJ0cGXmLXfSi3qv5ffwE3vGhLEgezCfHjOald4I9if1/EWVfVWWMHQ0f/aeuqgj+E48e1JujRw+gT0WMB+ev4R8nH0wsYgzo9Wk+cUAfqirLWLp+G73LY4wa2Jve5VE272qkOeGMq+7DoD7lNMaTlMcilEUjJJNOJBL8R3N3EknHzIhGsv1FKp3lHvRJT4mh5ShHS/39rdu6XluSSccss7qdUQifcXtKJinE4/FUYx///OscXjOA0YN6UxY1mhJJyqMRYtF9fxEnfziAPtWjU/Ob/j6b6t9fAgSHNJ459SlOPmgIw/tXMqhPearTLwzrfwOAH6XW7wMcDPxjNhuYQz/5Uu0+yw8aWrXX/JjqvQ+vpX+ekbQvfzMjFi3s/xilqBC+rDoSQ+u6+1u3I9uM5PhHSiF8xu0pmaSwdcNaqoH5h97MV+pG7bVsf8lgX9bO/U9GvXADAI8NvZZzr/oxV2czUBGRPCqZpLDz/VVUAzZoTKe30bzu1VRCeOvSRZw77pPZCU5EpECUzH0Ku7ZtAaDXgAM6vY01f3kIgLlnPc8nlBBEpAiVTFJoagqunuhVWdnpbVS+8xyLOJRP1x2VrbBERApKySQFSwSXhEVi5e3U3LfejRsY2fAm64ecRFkHzkGIiPQkpfPtlgjutrVOJoVRm18EYMDEz2UtJBGRQlMyScGSwZ4C0bJOrT8wuYV1Xs3RdSdkMSoRkcJSMkmBZHiDVqTjewoRC25+WdlvEn0qO5dURER6gpJJCpFwsDbr5J4CQPmoY7IVjohIQSqZpNByotliFZ3eRmyILkMVkeJWMklh7fAz+VrT97BY5y9J7TVgWBYjEhEpPCWTFHb2GskLyYlYNNrpbVT1H5DFiERECk/JJIWWx0Z0ZTyqfgMGZycYEZECVTJJIRlmhUgXskL/ftpTEJHiVjJJoeVhHV1JCrFY5w89iYj0BCWTFD7aU8hzICIiBayEkkLwtyc85EJEJF9KJim49hRERNpVMkkhmcz8Oa0iIqWqZJLCRyea8xqGiEhBy2lSMLPJZrbCzFaZ2bR9LB9tZnPN7O9m9pqZnZ2rWHROQUSkfRklBTN7wsw+Z2YZJxEziwLTgbOAw4ALzOywVtVuAWa5+1HA+cB/ZLr9jtI5BRGR9mX6Jf8fwIXAm2b2UzM7OIN1jgNWuftqd28CHgGmtKrjQL9wuj+wPsN4OiwbN6+JiBS7jJKCuz/r7hcBRwPvAM+a2V/N7HIz299Y1COBtWnz9WFZuluBi82sHngauKYDsXdINBKhV1m0S8NciIgUu44cDhoMXAZ8E/g78GuCJPGnLrz/BcBv3b0GOBt4YF+HqMzsSjNbaGYLN27c2Kk3+sanx/LGjyfTuzzWhXBFRIpbpucUngT+AvQGvuDu57j7THe/Bui7n9XWAaPS5mvCsnTfAGYBuPtLQCVQ3XpD7j7D3evcvW7IkCGZhCwiIp2Q6c/mO9x97r4WuHvdftZZAIw3s7EEyeB8gvMS6dYApwO/NbNDCZJC53YFRESkyzI9fHSYmaWGCDWzgWb2rbZWcPc48G1gDvAGwVVGS83sNjM7J6z2XeAKM3sVeBi4zFsuExIRkW6X6Z7CFe4+vWXG3beY2RW0cwmpuz9NcAI5veyf0qaXAZ/KPFwREcmlTPcUopZ211d4D0J5bkISEZF8yXRP4Rlgppn9Jpz/n2GZiIgUkUyTwvcIEsFV4fyfgLtzEpGIiORNRknB3ZPAneFLRESKVEZJwczGA/9CMIZRZUu5u4/LUVwiIpIHmZ5ovpdgLyEOnArcD/xXroISEZH8yDQp9HL35wBz93fd/Vbgc7kLS0RE8iHTE82N4ZhEb5rZtwnuUN7f8BYiItJDZbqncB3BuEfXAscAFwNfy1VQIiKSH+3uKYQ3qk119/8F7AQuz3lUIiKSF+3uKbh7Avh0N8QiIiJ5luk5hb+b2WzgUWBXS6G7P5GTqEREJC8yTQqVwGbgtLQyB5QURESKSKZ3NOs8gohICcj0juZ7CfYM9uLuX896RCIikjeZHj56Km26EvgSsD774YiISD5levjo8fR5M3sYeDEnEYmISN5kevNaa+OBA7IZiIiI5F+m5xR2sPc5hfcJnrEgIiJFJNPDR1W5DkRERPIvo8NHZvYlM+ufNj/AzL6Yu7BERCQfMj2n8EN339Yy4+5bgR/mJiQREcmXTC9J3VfyyHRdEZGsaW5upr6+noaGhnyHUpAqKyupqamhrKysU+tn+sW+0Mx+BUwP568GFnXqHUVEuqC+vp6qqirGjBmDmeU7nILi7mzevJn6+nrGjh3bqW1kevjoGqAJmAk8AjQQJAYRkW7V0NDA4MGDlRD2wcwYPHhwl/aiMr36aBcwrdPvIiKSRUoI+9fVzybTq4/+ZGYD0uYHmtmcLr2ziIgUnEwPH1WHVxwB4O5b0B3NIiJFJ9OkkDSz0S0zZjaGfYyaKiIiPVumSeFm4EUze8DM/gt4Abgpd2GJiBS2L37xixxzzDFMmDCBGTNmAPDMM89w9NFHM3HiRE4//XQAdu7cyeWXX05tbS1HHHEEjz/+eFubzbtMTzQ/Y2Z1wJXA34HfAXtyGZiISHt+9N9LWbZ+e1a3ediIfvzwCxParXfPPfcwaNAg9uzZw7HHHsuUKVO44oormDdvHmPHjuXDDz8E4Mc//jH9+/dnyZIlAGzZsiWr8WZbpgPifRO4DqgBFgOTgJfY+/GcIiIl44477uDJJ58EYO3atcyYMYOTTjopdX/AoEGDAHj22Wd55JFHUusNHDiw+4PtgExvXrsOOBZ42d1PNbNDgH/OXVgiIu3L5Bd9Lvz5z3/m2Wef5aWXXqJ3796ccsopHHnkkSxfvjwv8WRTpucUGty9AcDMKtx9OXBw7sISESlc27ZtY+DAgfTu3Zvly5fz8ssv09DQwLx583j77bcBUoePzjjjDKZPn55at9APH2WaFOrD+xR+B/zJzH4PvJu7sERECtfkyZOJx+MceuihTJs2jUmTJjFkyBBmzJjBl7/8ZSZOnMjUqVMBuOWWW9iyZQuHH344EydOZO7cuXmOvm2Znmj+Ujh5q5nNBfoDz+QsKhGRAlZRUcEf//jHfS4766yz9prv27cv9913X3eElRUdHunU3V/IRSAiIpJ/nX1Gs4iIFCElBRERSclpUjCzyWa2wsxWmdk+R1k1s6+a2TIzW2pmD+UyHhERaVvOnp5mZlGCh/KcAdQDC8xstrsvS6sznmC4jE+5+xYz0yB7IiJ5lMs9heOAVe6+2t2bCB7OM6VVnSuA6eGoq7j7hhzGIyIi7chlUhgJrE2brw/L0h0EHGRm/8/MXjazyTmMR0RE2pHvE80xYDxwCnAB8H/SH+bTwsyuNLOFZrZw48aN3RyiiEjn9e3bN98hdEguk8I6YFTafE1Ylq4emO3uze7+NrCSIEnsxd1nuHudu9cNGTIkZwGLiJS6nJ1oBhYA481sLEEyOB+4sFWd3xHsIdxrZtUEh5NW5zCmTvt94oSPnRARkTz74zR4f0l2tzmsFs766X4XT5s2jVGjRnH11VcDcOuttxKLxZg7dy5btmyhubmZ22+/nSlT2v/G2LlzJ1OmTNnnevfffz+//OUvMTOOOOIIHnjgAT744AP+4R/+gdWrg6/JO++8kxNOOCELjf5IzpKCu8fN7NvAHCAK3OPuS83sNmChu88Ol51pZsuABHCju2/OVUyddXDDb2kmpqQgIkydOpXrr78+lRRmzZrFnDlzuPbaa+nXrx+bNm1i0qRJnHPOOZhZm9uqrKzkySef/Nh6y5Yt4/bbb+evf/0r1dXVqcH1rr32Wk4++WSefPJJEokEO3fuzHr7crmngLs/DTzdquyf0qYduCF8FaxGyvMdgojsSxu/6HPlqKOOYsOGDaxfv56NGzcycOBAhg0bxne+8x3mzZtHJBJh3bp1fPDBBwwbNqzNbbk73//+9z+23vPPP895551HdXU18NGzGZ5//nnuv/9+AKLRKP379896+3KaFEREitF5553HY489xvvvv8/UqVN58MEH2bhxI4sWLaKsrIwxY8bQ0NDQ7nY6u14u5fvqIxGRHmfq1Kk88sgjPPbYY5x33nls27aNAw44gLKyMubOncu772b2ZIH9rXfaaafx6KOPsnlzcDS95fDR6aefzp133glAIpFg27ZtWW+bkoKISAdNmDCBHTt2MHLkSIYPH85FF13EwoULqa2t5f777+eQQw7JaDv7W2/ChAncfPPNnHzyyUycOJEbbgiOsP/6179m7ty51NbWcswxx7Bs2bK2Nt8pFhzW7znq6up84cKF3fqeY6b9AYB3fvq5bn1fEfm4N954g0MPPTTfYRS0fX1GZrbI3evaW1d7CiIikqITzSIiObZkyRIuueSSvcoqKiqYP39+niLaPyUFEelx3L3dewAKSW1tLYsXL+6W9+rqKQEdPhKRHqWyspLNmzd3+cuvGLk7mzdvprKystPb0J5Chob2q8h3CCIC1NTUUF9fjwbH3LfKykpqamo6vb6SQgYeuXIS46r75DsMEQHKysoYO3ZsvsMoWkoKGZg0bnC+QxAR6RY6pyAiIilKCiIiktLj7mg2s41AZgOLfFw1sCmL4RQqtbO4qJ3FJV/tPNDd231KWY9LCl1hZgszuc27p1M7i4vaWVwKvZ06fCQiIilKCiIiklJqSWFGvgPoJmpncVE7i0tBt7OkzimIiEjbSm1PQURE2qCkICIiKSWTFMxsspmtMLNVZjYt3/F0hJmNMrO5ZrbMzJaa2XVh+SAz+5OZvRn+HRiWm5ndEbb1NTM7Om1bXwvrv2lmX8tXm9piZlEz+7uZPRXOjzWz+WF7ZppZeVheEc6vCpePSdvGTWH5CjP7bH5asn9mNsDMHjOz5Wb2hpkdX4z9aWbfCf/Nvm5mD5tZZbH0p5ndY2YbzOz1tLKs9aGZHWNmS8J17rDuGivc3Yv+BUSBt4BxQDnwKnBYvuPqQPzDgaPD6SpgJXAY8HNgWlg+DfhZOH028EfAgEnA/LB8ELA6/DswnB6Y7/bto703AA8BT4Xzs4Dzw+m7gKvC6W8Bd4XT5wMzw+nDwj6uAMaGfR/Nd7tatfE+4JvhdDkwoNj6ExgJvA30SuvHy4qlP4GTgKOB19PKstaHwN/Cuhaue1a3tCvfH2w3dd7xwJy0+ZuAm/IdVxfa83vgDGAFMDwsGw6sCKd/A1yQVn9FuPwC4Ddp5XvVK4QXUAM8B5wGPBX+h9gExFr3JTAHOD6cjoX1rHX/ptcrhBfQP/yytFblRdWfYVJYG37hxcL+/Gwx9ScwplVSyEofhsuWp5XvVS+Xr1I5fNTyj7NFfVjW44S71EcB84Gh7v5euOh9YGg4vb/29oTP4d+AfwSS4fxgYKu7x8P59JhT7QmXbwvrF3o7xwIbgXvDw2R3m1kfiqw/3X0d8EtgDfAeQf8sovj6M122+nBkON26POdKJSkUBTPrCzwOXO/u29OXefBzokdfX2xmnwc2uPuifMeSYzGCww53uvtRwC6CQw0pRdKfA4EpBElwBNAHmJzXoLpRT+3DUkkK64BRafM1YVmPYWZlBAnhQXd/Iiz+wMyGh8uHAxvC8v21t9A/h08B55jZO8AjBIeQfg0MMLOWZ3+kx5xqT7i8P7CZwm9nPVDv7i1PbX+MIEkUW39+Bnjb3Te6ezPwBEEfF1t/pstWH64Lp1uX51ypJIUFwPjwqodygpNYs/McU8bCqw7+E3jD3X+Vtmg20HK1wtcIzjW0lF8aXvEwCdgW7tLOAc40s4Hhr7gzw7KC4O43uXuNu48h6KPn3f0iYC5wblitdTtb2n9uWN/D8vPDq1nGAuMJTtoVBHd/H1hrZgeHRacDyyiy/iQ4bDTJzHqH/4Zb2llU/dlKVvowXLbdzCaFn92ladvKrXyfqOmuF8HZ/5UEVy7cnO94Ohj7pwl2Q18DFoevswmOtz4HvAk8CwwK6xswPWzrEqAubVtfB1aFr8vz3bY22nwKH119NI7gS2AV8ChQEZZXhvOrwuXj0ta/OWz/Crrpqo0Otu9IYGHYp78juPKk6PoT+BGwHHgdeIDgCqKi6E/gYYJzJc0Ee3/fyGYfAnXh5/YW8O+0ujAhVy8NcyEiIimlcvhIREQyoKQgIiIpSgoiIpISa79KYamurvYxY8bkOwwRkR5l0aJFmzyDZzT3uKQwZswYFi5cmO8wRER6FDN7N5N6OnwkIiIpSgoZWPvhbnY3xduvKCLSwykpZODEn8/l4rvnt19RRKSH63HnFPLllTVb8x2CSElrbm6mvr6ehoaGfIdS0CorK6mpqaGsrKxT6yspZOCdygu5Jz4Z+Fy+QxEpWfX19VRVVTFmzBi66yFkPY27s3nzZurr6xk7dmyntqHDRxn6euyZfIcgUtIaGhoYPHiwEkIbzIzBgwd3aW9KSUFEegwlhPZ19TNSUhARyVDfvn3zHULOKSmIiEiKkoKISAe5OzfeeCOHH344tbW1zJw5E4D33nuPk046iSOPPJLDDz+cv/zlLyQSCS677LJU3X/913/Nc/Rt09VHItLj/Oi/l7Js/fb2K3bAYSP68cMvTMio7hNPPMHixYt59dVX2bRpE8ceeywnnXQSDz30EJ/97Ge5+eabSSQS7N69m8WLF7Nu3Tpef/11ALZuLezL27WnICLSQS+++CIXXHAB0WiUoUOHcvLJJ7NgwQKOPfZY7r33Xm699VaWLFlCVVUV48aNY/Xq1VxzzTU888wz9OvXL9/ht0l7CiLS42T6i767nXTSScybN48//OEPXHbZZdxwww1ceumlvPrqq8yZM4e77rqLWbNmcc899+Q71P3SnoKISAedeOKJzJw5k0QiwcaNG5k3bx7HHXcc7777LkOHDuWKK67gm9/8Jq+88gqbNm0imUzyla98hdtvv51XXnkl3+G3SXsKIiId9KUvfYmXXnqJiRMnYmb8/Oc/Z9iwYdx333384he/oKysjL59+3L//fezbt06Lr/8cpLJJAD/8i//kufo22bunu8YOqSurs67/XkKt/YP/27r3vcVkZQ33niDQw89NN9h9Aj7+qzMbJG717W3bk4PH5nZZDNbYWarzGzafup81cyWmdlSM3sol/GIiEjbcnb4yMyiwHTgDKAeWGBms919WVqd8cBNwKfcfYuZHZCreEREpH253FM4Dljl7qvdvQl4BJjSqs4VwHR33wLg7htyGI+IiLQjl0lhJLA2bb4+LEt3EHCQmf0/M3vZzCbnMB4REWlHvq8+igHjgVOAGmCemdW6+163/JnZlcCVAKNHj+7uGEVESkYu9xTWAaPS5mvCsnT1wGx3b3b3t4GVBEliL+4+w93r3L1uyJAhOQtYRKTU5TIpLADGm9lYMysHzgdmt6rzO4K9BMysmuBw0uocxiQiIm3IWVJw9zjwbWAO8AYwy92XmtltZnZOWG0OsNnMlgFzgRvdfXOuYhIR6S5tPXvhnXfe4fDDD+/GaDKX03MK7v408HSrsn9Km3bghvAlIiJ5lu8TzSIiHffHafD+kuxuc1gtnPXT/S6eNm0ao0aN4uqrrwbg1ltvJRaLMXfuXLZs2UJzczO33347U6a0vvK+bQ0NDVx11VUsXLiQWCzGr371K0499VSWLl3K5ZdfTlNTE8lkkscff5wRI0bw1a9+lfr6ehKJBD/4wQ+YOnVql5rdmpKCiEgGpk6dyvXXX59KCrNmzWLOnDlce+219OvXj02bNjFp0iTOOeecDj0nefr06ZgZS5YsYfny5Zx55pmsXLmSu+66i+uuu46LLrqIpqYmEokETz/9NCNGjOAPf/gDANu2ZX/oHSUFEel52vhFnytHHXUUGzZsYP369WzcuJGBAwcybNgwvvOd7zBv3jwikQjr1q3jgw8+YNiwYRlv98UXX+Saa64B4JBDDuHAAw9k5cqVHH/88fzkJz+hvr6eL3/5y4wfP57a2lq++93v8r3vfY/Pf/7znHjiiVlvp4bOFhHJ0Hnnncdjjz3GzJkzmTp1Kg8++CAbN25k0aJFLF68mKFDh9LQ0JCV97rwwguZPXs2vXr14uyzz+b555/noIMO4pVXXqG2tpZbbrmF2267LSvvlU57CiIiGZo6dSpXXHEFmzZt4oUXXmDWrFkccMABlJWVMXfuXN59990Ob/PEE0/kwQcf5LTTTmPlypWsWbOGgw8+mNWrVzNu3DiuvfZa1qxZw2uvvcYhhxzCoEGDuPjiixkwYAB333131tuopCAikqEJEyawY8cORo4cyfDhw7nooov4whe+QG1tLXV1dRxyyCEd3ua3vvUtrrrqKmpra4nFYvz2t7+loqKCWbNm8cADD1BWVsawYcP4/ve/z4IFC7jxxhuJRCKUlZVx5513Zr2Nep5CJvQ8BZG80/MUMlewz1MQEZGeRYePRERyZMmSJVxyySV7lVVUVDB//vw8RdQ+JQURkRypra1l8eLF+Q6jQ0rm8NHcFRu48dFXaYwn8h2KiHRSTzsHmg9d/YxKJimseH8Hjy6qJ5HUPyqRnqiyspLNmzcrMbTB3dm8eTOVlZWd3kbJHD5quelc/55Eeqaamhrq6+vZuHFjvkMpaJWVldTU1HR6/dJJCmFWUE4Q6ZnKysoYO3ZsvsMoeiVz+EhERNqXUVIws+vMrJ8F/tPMXjGzM3MdXDZZeACpK8cjkwmdpBaR4pbpnsLX3X07cCYwELgE6P5hCrsgG4eP4vF4VmIRESlUmSaFlvO0ZwMPuPvStLIepSsnmpNJJQURKW6ZJoVFZvZ/CZLCHDOrApK5Cyv7LAu7CvF4c3aCEREpUJleffQN4EhgtbvvNrNBwOW5Cyv7srFbk9A5BREpcpnuKRwPrHD3rWZ2MXAL0COHDPUu7Cq4zimISJHLNCncCew2s4nAd4G3gPtzFlUOpI4edeXwUUKHj0SkuGWaFOIeXMs5Bfh3d58OVOUurOxL3dHchW24Dh+JSJHL9JzCDjO7ieBS1BPNLAKU5S6s7Gs50dyV+xQS2lMQkSKX6Z7CVKCR4H6F94Ea4Bc5iyoHsnGfgm5eE5Fil1FSCBPBg0B/M/s80ODuPeucQha2kdQlqSJS5DId5uKrwN+A84CvAvPN7NxcBpYrXTnRnEjo6iMRKW6ZnlO4GTjW3TcAmNkQ4FngsVwFlnUt5xS6ckmq7mgWkSKX6TmFSEtCCG3OZF0zm2xmK8xslZlNa6PeV8zMzawuw3g6LHX4qCvDXOicgogUuUz3FJ4xsznAw+H8VODptlYwsygwHTgDqAcWmNlsd1/Wql4VcB2Q0ydZZ+dEs84piEhxy/RE843ADOCI8DXD3b/XzmrHAavcfbW7NwGPENzn0NqPgZ8BDRlH3QkfDZ3d+W1oT0FEil3GT15z98eBxzuw7ZHA2rT5euB/pFcws6OBUe7+BzO7sQPb7jDLwuVHntSegogUtzaTgpntYN9HXAxwd+/X2TcOb4D7FXBZBnWvBK4EGD16dGffEujaiWbtKYhIsWszKbh7V4ayWAeMSpuvCctaVAGHA38O7zYeBsw2s3PcfWGrOGYQHL6irq6uU9/qqWEuunD4yHVJqogUuVw+o3kBMN7MxppZOXA+MLtlobtvc/dqdx/j7mOAl4GPJYRsOfidB1hacTk07+r0NpLxpixGJCJSeHKWFNzQquM0AAAPwElEQVQ9DnwbmAO8Acxy96VmdpuZnZOr992fSDJBH2vEk10Y+6ix8wlFRKQnyPhEc2e4+9O0unTV3f9pP3VPyWkskSD/ebLzD4xLNOzMVjgiIgUpl4ePCkzLWYXOJ4Vko5KCiBS3kkkKTtefspPU4SMRKXIlkxSIRIO/XTh85E1KCiJS3EomKXz05LWOJ4W4hx9T8+7sBSQiUoBKJilgLSeaO3/4KNKFy1lFRHqCkkkKHiYFvPOHj2JN27MUjYhIYSqZpGCpYVI7nhQsHBqjovHDbIYkIlJwSiYpeNhU78KeQu/4lmyFIyJSkEomKaTOKXQhKVQllBREpLiVUFII/3bhPoUByW1duqRVRKTQlVBSCO9T8M4Nf73dexM1Z8/W97MYlIhIYSmhpNDy5LXO7Smsjw4HYNO7y9qpKSLSc5VQUgiaap08p/BhZfBwnx3r3shaSCIihaZ0kgJd21No6ltDo5cR3/BmNoMSESkopZMUUlcfdS4pVJSVscaGUbb1rWxGJSJSUEouKXT2RDMGmypGM2DX6uzFJCJSYEooKYSHj7pwSenOQbUMT6wnuUt3NotIcSq5pNCZ+xRabnEoO/BYAD5Y/tcsBSUiUlhKKCm03KfQuXMKhjHskEkk3dj65stZDExEpHCUTFKIhA/ZSSaaOr2N8aNH8hY1ROvnZyssEZGCUjJJIVbRG4Dmxj2d3kY0YrzT7xhG71wM8cZshSYiUjBKJilEK/sCEG/Y2aXtJMacTCVNbFn5YjbCEhEpKCWTFMoqgz2FeEPXHqlZc9QZNHoZH778cDbCEhEpKCWTFMp7BXsKyaauPVJzwtga5pSdyqg1v4OdG7IRmohIwSi9pNDYtaRgZmw/6h+IeZwNv7tZQ2mLSFEpmaRQNXAIAIPem9flbX3pMyfzSNkUDlg1i213T4H1i7v0nAYRkUIRy3cA3aV3n34AfHLzXBpnXUHFyddD405YtzD4Uu83AqLlMPIY2LoGeg8KXsOO+Ni2+lTEmHTlv/OL/zOUb6+7F2aczM5eI+HAT9FnbB3WfxQMGAX9R0FFP4iUTO4VkR7OOjtAXL7U1dX5woULO7Xu4r/8N0c+d3Gn1l0w4mKOvXL6XmU7Gpp5bN6r9Jn/v+nftIFjI8sZZHtf3ZSwKA0V1XisEi/rSyQSwSuqsPI+RJLN0G8EkbIKomUVRKNlGA7RMohVBn+j5cGGohUQjUEkfFkUItGP5iOxcL5VWab1ItGwbvq8ISLFwcwWuXtde/VyuqdgZpOBXwNR4G53/2mr5TcA3wTiwEbg6+7+bq7iOfLEL7Cmz2PU/3UmJ2x6NOP1nk4cx/ph53Jsq/KqyjIuP7MOP+Mhlr+/g2fe/ZCNH9TTuOld2FbPwJ2rGNn8Dg3xcmIk6EMD/WwXUfZQSTP9bSdRXqWMePgKBuuLkaDMOjlwXxYlLYpbBLcYHiYMtygeiQUDDEZiwXQqwQR/LUwslpZ8LBKDaPDXosGylmkiMWjaDZ6EPkOCZAhBUor1Cso9ERyii8TCZGXB30jso/qwj8N4afPrF0P1eCjvA4nmcBstC+2j98z1/O4PoaJvUBaJQuOOoN0blwd7l817YHt9MP3aTDjywuDHQcM2KO8brBOr2LutyXjwWTRuh7LewbJEU9BPm1eF99U49K8JtrV9HSTiMOJI2LM12MaO9dBrUPD57NkClf2D8rJesOUdGPzJoG4yHiwzC7Y9rDbY627aCb0GBvFHYkEcZkH/JZqD9cr7BPPJBOz5ECqqgnpNu4L1INh2085gmSehYXuw9149HuINQWzxBhhwYNDGsl5BO5OJ4MfU7s3BNraugaqhQVnDdug1IPjh44kgvkRT8NqzFSr7BT+8zD76XC0SzMcbw4E0LfjsLBK0pXFH8LkMqw3LLWjDB68H/55HHh2ss3tTENMBhwafeTQW/psGYuUQbwr6Jloe/FvetSk4ShFvDN4rWha8n0Xhk6fD0AnkUs72FMwsCqwEzgDqgQXABe6+LK3OqcB8d99tZlcBp7j71La225U9hXTuzjubd/PO5l1s3NHI9j3N7GyM0xhP0hRP0hhP0Bx3mpNJ3OGKE8dx2Ih+HX6fpniSTTsb2dEQZ2djM7ubEuxuStDQnKCxOUlj4qP3C/4G88l4E4lEHG9uJJ5M4vEmkokEnozjiWZIJvBEPJhPxsP5oBwPlkU8jicTmAfLLZw2T2CeIEqSGOl/E0RxoiSI2d7Lg1dir/oRSxJLlSfD9dO2mbaNCOl107Zpwbq9aKTKgi+FZo+StAgx4kTDL/U4EcCIkf9kWcqSGBF61tGFYrL+pJ8z4rT/2al1C2FP4ThglbuvDgN6BJgCpJKCu89Nq/8y0LljO51gZoyt7sPY6j45fZ/yWIQRA3rl9D06w91JOsSTSRJJpznhxBPBdMKdeMKJJ51EMklzwkkknaQHZfGwbjysmwjrNiU9tb1g3eBvMpxu2XbLdDKMIRmWt9RLJp1kGKN5kjiWqucOSXfcHfcksWQz7gkcSCQdBxwLlxPWD36UGUnMEyQ8QtzKSXqQ8L3lS67lqXxhWcs28GRQw8NXaqJlveCvAQn3lg2ktmsexuVJIuEvUceDqzw8ibsTIXiPpBtJjF7JPeyJ9CLiSeJESBKhmRhRjwNQbnFi3kScGM0eJdh3SoTpNkIv30MzMRwjYVHMk8Q8TrPFMHfiRIgES4l5nLjFiCaDv6kfyjhxokQ9QYw4cWLhekmSbhhOlHjwPuF8I+WU00SUJHEP4k5iQRs8QgVNJImQCPecIp7APuoB4h7E5RhGkiTBdg0n7tEwriQe/psoI04j5RD+mAn6M0KUZhqoSD1psYJmEljw+Xjw46KJaGq75TTTTIwICZJEUn3aElnSjWZilBPHcJIYCSKUESdBNPXjp4FykhhNlKXirqAZw8OfRhEMp5w4Hn4GLYm25W80jGEPFakfWkmMKEluLj+cC7r2X79duUwKI4G1afP1wP9oo/43gD/ua4GZXQlcCTB69OhsxVfSzIyoQTQcE0pEOqblKEtLEvV9LUvNt9TZe529t7f/Oi2T5dHcX7RSEFcfmdnFQB1w8r6Wu/sMYAYEh4+6MTQRkX2y8HzRvq/H6LkXaeQyKawDRqXN14RlezGzzwA3Aye7u0aZExHJo1zuiywAxpvZWDMrB84HZqdXMLOjgN8A57i7xowQEcmznN6nYGZnA/9GcEnqPe7+EzO7DVjo7rPN7FmgFngvXGWNu5/TzjY3Ap29bLUa2NTJdXsStbO4qJ3FJV/tPNDdh7RXqcfdvNYVZrYwk0uyejq1s7ioncWl0Nup8RdERCRFSUFERFJKLSnMyHcA3UTtLC5qZ3Ep6HaW1DkFERFpW6ntKYiISBtKJimY2WQzW2Fmq8xsWr7j6QgzG2Vmc81smZktNbPrwvJBZvYnM3sz/DswLDczuyNs62tmdnTatr4W1n/TzL6Wrza1xcyiZvZ3M3sqnB9rZvPD9swM73vBzCrC+VXh8jFp27gpLF9hZp/NT0v2z8wGmNljZrbczN4ws+OLsT/N7Dvhv9nXzexhM6sslv40s3vMbIOZvZ5WlrU+NLNjzGxJuM4dZt00lr2nBhcr3hfBfRJvAeOAcuBV4LB8x9WB+IcDR4fTVQSjzx4G/ByYFpZPA34WTp9NMI6UAZMIRqIFGASsDv8ODKcH5rt9+2jvDcBDwFPh/Czg/HD6LuCqcPpbwF3h9PnAzHD6sLCPK4CxYd9H892uVm28D/hmOF0ODCi2/iQY/+xtoFdaP15WLP0JnAQcDbyeVpa1PgT+Fta1cN2zuqVd+f5gu6nzjgfmpM3fBNyU77i60J7fEwxJvgIYHpYNB1aE078hGKa8pf6KcPkFwG/SyveqVwgvguFQngNOA54K/0NsAmKt+xKYAxwfTsfCeta6f9PrFcIL6B9+WVqr8qLqTz4aFHNQ2D9PAZ8tpv4ExrRKClnpw3DZ8rTyverl8lUqh4/2NWLryDzF0iXhLvVRwHxgqLu33A3+PjA0nN5fe3vC5/BvwD8C4TjWDAa2uodjRu8dc6o94fJtYf1Cb+dYgodK3RseJrvbzPpQZP3p7uuAXwJrCEYt2AYsovj6M122+nBkON26POdKJSkUBTPrCzwOXO/u29OXefBzokdfSmZmnwc2uPuifMeSYzGCww53uvtRwC6CQw0pRdKfAwmeoTIWGAH0ASbnNahu1FP7sFSSQkYjthYyMysjSAgPuvsTYfEHZjY8XD4caBlUcH/tLfTP4VPAOWb2DvAIwSGkXwMDzKxlRN/0mFPtCZf3BzZT+O2sB+rdfX44/xhBkii2/vwM8La7b3T3ZuAJgj4utv5Ml60+XBdOty7PuVJJCu2O2FrIwqsO/hN4w91/lbZoNtBytcLXCM41tJRfGl7xMAnYFu7SzgHONLOB4a+4M8OyguDuN7l7jbuPIeij5939ImAucG5YrXU7W9p/bljfw/Lzw6tZxgLjCU7aFQR3fx9Ya2YHh0WnEzyRsKj6k+Cw0SQz6x3+G25pZ1H1ZytZ6cNw2XYzmxR+dpembSu38n2iprteBGf/VxJcuXBzvuPpYOyfJtgNfQ1YHL7OJjje+hzwJvAsMCisb8D0sK1LgLq0bX0dWBW+Ls9329po8yl8dPXROIIvgVXAo0BFWF4Zzq8Kl49LW//msP0r6KarNjrYviOBhWGf/o7gypOi60/gR8By4HXgAYIriIqiP4GHCc6VNBPs/X0jm31I8OCx18N1/p1WFybk6qU7mkVEJKVUDh+JiEgGlBRERCRFSUFERFKUFEREJEVJQUREUpQURLqRmZ1i4eivIoVISUFERFKUFET2wcwuNrO/mdliM/uNBc942Glm/xo+H+A5MxsS1j3SzF4Ox8l/Mm0M/U+a2bNm9qqZvWJmnwg339c+epbCg902Tr5IBpQURFoxs0OBqcCn3P1IIAFcRDCg20J3nwC8APwwXOV+4HvufgTB3aot5Q8C0919InACwd2vEIxyez3BcwLGEYwHJFIQYu1XESk5pwPHAAvCH/G9CAY2SwIzwzr/BTxhZv2BAe7+Qlh+H/ComVUBI939SQB3bwAIt/c3d68P5xcTjMn/Yu6bJdI+JQWRjzPgPne/aa9Csx+0qtfZMWIa06YT6P+hFBAdPhL5uOeAc83sAEg9d/dAgv8vLaN7Xgi86O7bgC1mdmJYfgnwgrvvAOrN7IvhNirMrHe3tkKkE/QLRaQVd19mZrcA/9fMIgSjYF5N8DCc48JlGwjOO0AwRPJd4Zf+auDysPwS4Ddmdlu4jfO6sRkinaJRUkUyZGY73b1vvuMQySUdPhIRkRTtKYiISIr2FEREJEVJQUREUpQUREQkRUlBRERSlBRERCRFSUFERFL+P3w2myi3qGIbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d27f63ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history_TPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('URZ_model_15-6-2_norm_TPS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6850/6850 [==============================] - 0s 8us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21168624090452265, 0.915036496350365]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TPS.evaluate(test_X_TPS, test_Y_TPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2218  130]\n",
      " [ 452 4050]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = numpy.reshape(numpy.argmax(model_TPS.predict(test_X_TPS), axis=1), (test_X_TPS.shape[0],1))\n",
    "\n",
    "# calculate confusion matrix\n",
    "C = confusion_matrix(test_Y_TPS_, Y_pred)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.50%\n"
     ]
    }
   ],
   "source": [
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just for curiosity -  Manual associations confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1817  185]\n",
      " [ 913 3091]]\n",
      "Accuracy: 81.72%\n"
     ]
    }
   ],
   "source": [
    "Y_pred_man = numpy.argmax(model_TPS.predict(manual_X), axis=1)\n",
    "\n",
    "# calculate confusion matrix\n",
    "C = confusion_matrix(manual_Y_TPS_, Y_pred_man)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network T vs regP \n",
    "\n",
    "* we need a new dataset for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for T vs regP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude regS\n",
    "TP_train = TPS_train[TPS_train['CLASS_PHASE'] != 'regS']\n",
    "TP_test  = TPS_test [TPS_test ['CLASS_PHASE'] != 'regS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13764, 15) (13764, 2) (4502, 15) (4502, 2)\n"
     ]
    }
   ],
   "source": [
    "train_X_TP = TP_train[x_indices].values.astype(float)\n",
    "train_Y_TP = TP_train[y_indices]\n",
    "\n",
    "test_X_TP = TP_test[x_indices].values.astype(float)\n",
    "test_Y_TP = TP_test[y_indices]\n",
    "\n",
    "#regS = 0, T/regP = 1\n",
    "train_Y_TP_ = numpy.array(numpy.where(train_Y_TP['CLASS_PHASE'] == 'regP', 0, 1), dtype=float)\n",
    "test_Y_TP_ = numpy.array(numpy.where(test_Y_TP['CLASS_PHASE'] == 'regP', 0, 1), dtype=float)\n",
    "\n",
    "#convert to categorical\n",
    "train_Y_TP = keras.utils.to_categorical(train_Y_TP_)\n",
    "test_Y_TP = keras.utils.to_categorical(test_Y_TP_)\n",
    "\n",
    "print(train_X_TP.shape, train_Y_TP.shape, test_X_TP.shape, test_Y_TP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_TP = {k : [] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = len(x_indices)\n",
    "numpy.random.seed(11)\n",
    "\n",
    "# create model\n",
    "model_TP = Sequential()\n",
    "model_TP.add(Dense(6, input_dim=n_input, activation='sigmoid'))\n",
    "model_TP.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_TP.compile(\n",
    "    loss = 'binary_crossentropy', \n",
    "    optimizer = 'adam',  # adam, sgd\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13764 samples, validate on 4502 samples\n",
      "Epoch 1/1000\n",
      "13764/13764 [==============================] - 1s 44us/step - loss: 0.4306 - acc: 0.8111 - val_loss: 0.4614 - val_acc: 0.7930\n",
      "Epoch 2/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4299 - acc: 0.8105 - val_loss: 0.4617 - val_acc: 0.7910\n",
      "Epoch 3/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4305 - acc: 0.8128 - val_loss: 0.4605 - val_acc: 0.7939\n",
      "Epoch 4/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4299 - acc: 0.8131 - val_loss: 0.4614 - val_acc: 0.7943\n",
      "Epoch 5/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4303 - acc: 0.8122 - val_loss: 0.4622 - val_acc: 0.7921\n",
      "Epoch 6/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4303 - acc: 0.8116 - val_loss: 0.4603 - val_acc: 0.7925\n",
      "Epoch 7/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4302 - acc: 0.8123 - val_loss: 0.4614 - val_acc: 0.7939\n",
      "Epoch 8/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4302 - acc: 0.8114 - val_loss: 0.4625 - val_acc: 0.7916\n",
      "Epoch 9/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4303 - acc: 0.8116 - val_loss: 0.4616 - val_acc: 0.7921\n",
      "Epoch 10/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4302 - acc: 0.8131 - val_loss: 0.4606 - val_acc: 0.7934\n",
      "Epoch 11/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4305 - acc: 0.8118 - val_loss: 0.4607 - val_acc: 0.7936\n",
      "Epoch 12/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4304 - acc: 0.8129 - val_loss: 0.4603 - val_acc: 0.7936\n",
      "Epoch 13/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8120 - val_loss: 0.4604 - val_acc: 0.7932\n",
      "Epoch 14/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4301 - acc: 0.8118 - val_loss: 0.4606 - val_acc: 0.7934\n",
      "Epoch 15/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4303 - acc: 0.8120 - val_loss: 0.4606 - val_acc: 0.7941\n",
      "Epoch 16/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4301 - acc: 0.8125 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 17/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4301 - acc: 0.8119 - val_loss: 0.4603 - val_acc: 0.7945\n",
      "Epoch 18/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4302 - acc: 0.8119 - val_loss: 0.4612 - val_acc: 0.7934\n",
      "Epoch 19/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4302 - acc: 0.8128 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 20/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4301 - acc: 0.8118 - val_loss: 0.4609 - val_acc: 0.7945\n",
      "Epoch 21/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4301 - acc: 0.8129 - val_loss: 0.4605 - val_acc: 0.7943\n",
      "Epoch 22/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4301 - acc: 0.8123 - val_loss: 0.4612 - val_acc: 0.7943\n",
      "Epoch 23/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4303 - acc: 0.8125 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 24/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4303 - acc: 0.8117 - val_loss: 0.4603 - val_acc: 0.7916\n",
      "Epoch 25/1000\n",
      "13764/13764 [==============================] - 1s 43us/step - loss: 0.4303 - acc: 0.8118 - val_loss: 0.4609 - val_acc: 0.7936\n",
      "Epoch 26/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4302 - acc: 0.8123 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 27/1000\n",
      "13764/13764 [==============================] - 1s 46us/step - loss: 0.4303 - acc: 0.8124 - val_loss: 0.4603 - val_acc: 0.7932\n",
      "Epoch 28/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4301 - acc: 0.8119 - val_loss: 0.4611 - val_acc: 0.7948\n",
      "Epoch 29/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4302 - acc: 0.8126 - val_loss: 0.4608 - val_acc: 0.7943\n",
      "Epoch 30/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4299 - acc: 0.8121 - val_loss: 0.4605 - val_acc: 0.7928\n",
      "Epoch 31/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4303 - acc: 0.8126 - val_loss: 0.4604 - val_acc: 0.7934\n",
      "Epoch 32/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4300 - acc: 0.8120 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 33/1000\n",
      "13764/13764 [==============================] - 1s 49us/step - loss: 0.4303 - acc: 0.8122 - val_loss: 0.4603 - val_acc: 0.7925\n",
      "Epoch 34/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4302 - acc: 0.8126 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 35/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4302 - acc: 0.8111 - val_loss: 0.4603 - val_acc: 0.7921\n",
      "Epoch 36/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4301 - acc: 0.8120 - val_loss: 0.4604 - val_acc: 0.7941\n",
      "Epoch 37/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4301 - acc: 0.8126 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 38/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4298 - acc: 0.8119 - val_loss: 0.4635 - val_acc: 0.7905\n",
      "Epoch 39/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4302 - acc: 0.8119 - val_loss: 0.4617 - val_acc: 0.7923\n",
      "Epoch 40/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4301 - acc: 0.8109 - val_loss: 0.4610 - val_acc: 0.7943\n",
      "Epoch 41/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4300 - acc: 0.8135 - val_loss: 0.4604 - val_acc: 0.7925\n",
      "Epoch 42/1000\n",
      "13764/13764 [==============================] - 1s 48us/step - loss: 0.4300 - acc: 0.8132 - val_loss: 0.4606 - val_acc: 0.7941\n",
      "Epoch 43/1000\n",
      "13764/13764 [==============================] - 1s 60us/step - loss: 0.4302 - acc: 0.8120 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 44/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4299 - acc: 0.8126 - val_loss: 0.4610 - val_acc: 0.7939\n",
      "Epoch 45/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8131 - val_loss: 0.4616 - val_acc: 0.7930\n",
      "Epoch 46/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8126 - val_loss: 0.4611 - val_acc: 0.7943\n",
      "Epoch 47/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4302 - acc: 0.8121 - val_loss: 0.4615 - val_acc: 0.7932\n",
      "Epoch 48/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4301 - acc: 0.8117 - val_loss: 0.4603 - val_acc: 0.7932\n",
      "Epoch 49/1000\n",
      "13764/13764 [==============================] - 1s 48us/step - loss: 0.4301 - acc: 0.8113 - val_loss: 0.4614 - val_acc: 0.7932\n",
      "Epoch 50/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4300 - acc: 0.8123 - val_loss: 0.4601 - val_acc: 0.7919\n",
      "Epoch 51/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4303 - acc: 0.8124 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 52/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4301 - acc: 0.8127 - val_loss: 0.4605 - val_acc: 0.7943\n",
      "Epoch 53/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4300 - acc: 0.8112 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 54/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4301 - acc: 0.8119 - val_loss: 0.4607 - val_acc: 0.7948\n",
      "Epoch 55/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4302 - acc: 0.8118 - val_loss: 0.4602 - val_acc: 0.7934\n",
      "Epoch 56/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4302 - acc: 0.8114 - val_loss: 0.4602 - val_acc: 0.7936\n",
      "Epoch 57/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4300 - acc: 0.8124 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 58/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4300 - acc: 0.8131 - val_loss: 0.4603 - val_acc: 0.7923\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4300 - acc: 0.8111 - val_loss: 0.4628 - val_acc: 0.7921\n",
      "Epoch 60/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4301 - acc: 0.8117 - val_loss: 0.4603 - val_acc: 0.7932\n",
      "Epoch 61/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4302 - acc: 0.8119 - val_loss: 0.4601 - val_acc: 0.7921\n",
      "Epoch 62/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8118 - val_loss: 0.4606 - val_acc: 0.7943\n",
      "Epoch 63/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4300 - acc: 0.8120 - val_loss: 0.4625 - val_acc: 0.7923\n",
      "Epoch 64/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4303 - acc: 0.8122 - val_loss: 0.4601 - val_acc: 0.7925\n",
      "Epoch 65/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4299 - acc: 0.8112 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 66/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4300 - acc: 0.8112 - val_loss: 0.4606 - val_acc: 0.7941\n",
      "Epoch 67/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8122 - val_loss: 0.4603 - val_acc: 0.7930\n",
      "Epoch 68/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4299 - acc: 0.8124 - val_loss: 0.4615 - val_acc: 0.7936\n",
      "Epoch 69/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4301 - acc: 0.8124 - val_loss: 0.4605 - val_acc: 0.7943\n",
      "Epoch 70/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4300 - acc: 0.8116 - val_loss: 0.4602 - val_acc: 0.7934\n",
      "Epoch 71/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4299 - acc: 0.8125 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 72/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4607 - val_acc: 0.7930\n",
      "Epoch 73/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4301 - acc: 0.8112 - val_loss: 0.4609 - val_acc: 0.7941\n",
      "Epoch 74/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4300 - acc: 0.8119 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 75/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4301 - acc: 0.8133 - val_loss: 0.4601 - val_acc: 0.7928\n",
      "Epoch 76/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4300 - acc: 0.8110 - val_loss: 0.4608 - val_acc: 0.7936\n",
      "Epoch 77/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8124 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 78/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4300 - acc: 0.8129 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 79/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4300 - acc: 0.8127 - val_loss: 0.4605 - val_acc: 0.7945\n",
      "Epoch 80/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4303 - acc: 0.8114 - val_loss: 0.4605 - val_acc: 0.7943\n",
      "Epoch 81/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4298 - acc: 0.8139 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 82/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8121 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 83/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8112 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 84/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4302 - acc: 0.8121 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 85/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4299 - acc: 0.8113 - val_loss: 0.4605 - val_acc: 0.7923\n",
      "Epoch 86/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4299 - acc: 0.8125 - val_loss: 0.4617 - val_acc: 0.7925\n",
      "Epoch 87/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4301 - acc: 0.8119 - val_loss: 0.4608 - val_acc: 0.7943\n",
      "Epoch 88/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4299 - acc: 0.8133 - val_loss: 0.4603 - val_acc: 0.7932\n",
      "Epoch 89/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4300 - acc: 0.8118 - val_loss: 0.4612 - val_acc: 0.7939\n",
      "Epoch 90/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4299 - acc: 0.8125 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 91/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4301 - acc: 0.8108 - val_loss: 0.4612 - val_acc: 0.7936\n",
      "Epoch 92/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4298 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7941\n",
      "Epoch 93/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4300 - acc: 0.8112 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 94/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4300 - acc: 0.8129 - val_loss: 0.4603 - val_acc: 0.7934\n",
      "Epoch 95/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4300 - acc: 0.8128 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 96/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4609 - val_acc: 0.7945\n",
      "Epoch 97/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4299 - acc: 0.8125 - val_loss: 0.4612 - val_acc: 0.7941\n",
      "Epoch 98/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4300 - acc: 0.8118 - val_loss: 0.4603 - val_acc: 0.7943\n",
      "Epoch 99/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8127 - val_loss: 0.4612 - val_acc: 0.7939\n",
      "Epoch 100/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8126 - val_loss: 0.4601 - val_acc: 0.7923\n",
      "Epoch 101/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4299 - acc: 0.8122 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 102/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4300 - acc: 0.8118 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 103/1000\n",
      "13764/13764 [==============================] - 1s 50us/step - loss: 0.4299 - acc: 0.8126 - val_loss: 0.4604 - val_acc: 0.7925\n",
      "Epoch 104/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4302 - acc: 0.8111 - val_loss: 0.4600 - val_acc: 0.7923\n",
      "Epoch 105/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4299 - acc: 0.8121 - val_loss: 0.4602 - val_acc: 0.7934\n",
      "Epoch 106/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4300 - acc: 0.8127 - val_loss: 0.4606 - val_acc: 0.7936\n",
      "Epoch 107/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4608 - val_acc: 0.7943\n",
      "Epoch 108/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4298 - acc: 0.8125 - val_loss: 0.4606 - val_acc: 0.7950\n",
      "Epoch 109/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4299 - acc: 0.8128 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 110/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4303 - acc: 0.8120 - val_loss: 0.4600 - val_acc: 0.7921\n",
      "Epoch 111/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4299 - acc: 0.8124 - val_loss: 0.4602 - val_acc: 0.7928\n",
      "Epoch 112/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4301 - acc: 0.8118 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 113/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4300 - acc: 0.8116 - val_loss: 0.4601 - val_acc: 0.7932\n",
      "Epoch 114/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4298 - acc: 0.8118 - val_loss: 0.4601 - val_acc: 0.7921\n",
      "Epoch 115/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4298 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 116/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4301 - acc: 0.8124 - val_loss: 0.4602 - val_acc: 0.7919\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4300 - acc: 0.8118 - val_loss: 0.4610 - val_acc: 0.7939\n",
      "Epoch 118/1000\n",
      "13764/13764 [==============================] - 1s 44us/step - loss: 0.4298 - acc: 0.8115 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 119/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4298 - acc: 0.8136 - val_loss: 0.4601 - val_acc: 0.7928\n",
      "Epoch 120/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4297 - acc: 0.8120 - val_loss: 0.4600 - val_acc: 0.7932\n",
      "Epoch 121/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4298 - acc: 0.8113 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 122/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4300 - acc: 0.8120 - val_loss: 0.4608 - val_acc: 0.7936\n",
      "Epoch 123/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7941\n",
      "Epoch 124/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4299 - acc: 0.8130 - val_loss: 0.4600 - val_acc: 0.7936\n",
      "Epoch 125/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4299 - acc: 0.8130 - val_loss: 0.4603 - val_acc: 0.7923\n",
      "Epoch 126/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4300 - acc: 0.8128 - val_loss: 0.4606 - val_acc: 0.7936\n",
      "Epoch 127/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4301 - acc: 0.8115 - val_loss: 0.4599 - val_acc: 0.7941\n",
      "Epoch 128/1000\n",
      "13764/13764 [==============================] - 1s 44us/step - loss: 0.4300 - acc: 0.8122 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 129/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7936\n",
      "Epoch 130/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4298 - acc: 0.8125 - val_loss: 0.4600 - val_acc: 0.7925\n",
      "Epoch 131/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4297 - acc: 0.8127 - val_loss: 0.4601 - val_acc: 0.7939\n",
      "Epoch 132/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4297 - acc: 0.8128 - val_loss: 0.4619 - val_acc: 0.7936\n",
      "Epoch 133/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4607 - val_acc: 0.7943\n",
      "Epoch 134/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4299 - acc: 0.8134 - val_loss: 0.4600 - val_acc: 0.7936\n",
      "Epoch 135/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4298 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7936\n",
      "Epoch 136/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 137/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4607 - val_acc: 0.7930\n",
      "Epoch 138/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4301 - acc: 0.8119 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 139/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4299 - acc: 0.8126 - val_loss: 0.4609 - val_acc: 0.7941\n",
      "Epoch 140/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4298 - acc: 0.8127 - val_loss: 0.4602 - val_acc: 0.7934\n",
      "Epoch 141/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4297 - acc: 0.8122 - val_loss: 0.4602 - val_acc: 0.7939\n",
      "Epoch 142/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4298 - acc: 0.8138 - val_loss: 0.4599 - val_acc: 0.7914\n",
      "Epoch 143/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4298 - acc: 0.8114 - val_loss: 0.4601 - val_acc: 0.7925\n",
      "Epoch 144/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4612 - val_acc: 0.7925\n",
      "Epoch 145/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4298 - acc: 0.8122 - val_loss: 0.4611 - val_acc: 0.7943\n",
      "Epoch 146/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4298 - acc: 0.8126 - val_loss: 0.4604 - val_acc: 0.7943\n",
      "Epoch 147/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4297 - acc: 0.8122 - val_loss: 0.4610 - val_acc: 0.7910\n",
      "Epoch 148/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4296 - acc: 0.8117 - val_loss: 0.4600 - val_acc: 0.7925\n",
      "Epoch 149/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4298 - acc: 0.8127 - val_loss: 0.4600 - val_acc: 0.7925\n",
      "Epoch 150/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4300 - acc: 0.8112 - val_loss: 0.4600 - val_acc: 0.7916\n",
      "Epoch 151/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4298 - acc: 0.8136 - val_loss: 0.4620 - val_acc: 0.7934\n",
      "Epoch 152/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4299 - acc: 0.8115 - val_loss: 0.4601 - val_acc: 0.7921\n",
      "Epoch 153/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7939\n",
      "Epoch 154/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4297 - acc: 0.8120 - val_loss: 0.4613 - val_acc: 0.7930\n",
      "Epoch 155/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4624 - val_acc: 0.7916\n",
      "Epoch 156/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4298 - acc: 0.8136 - val_loss: 0.4628 - val_acc: 0.7923\n",
      "Epoch 157/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4298 - acc: 0.8126 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 158/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4299 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 159/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4298 - acc: 0.8129 - val_loss: 0.4601 - val_acc: 0.7941\n",
      "Epoch 160/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4295 - acc: 0.8134 - val_loss: 0.4602 - val_acc: 0.7923\n",
      "Epoch 161/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4300 - acc: 0.8120 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 162/1000\n",
      "13764/13764 [==============================] - 1s 49us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 163/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4299 - acc: 0.8119 - val_loss: 0.4600 - val_acc: 0.7936\n",
      "Epoch 164/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4300 - acc: 0.8127 - val_loss: 0.4600 - val_acc: 0.7943\n",
      "Epoch 165/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4297 - acc: 0.8120 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 166/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4298 - acc: 0.8124 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 167/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4297 - acc: 0.8118 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 168/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4296 - acc: 0.8116 - val_loss: 0.4616 - val_acc: 0.7934\n",
      "Epoch 169/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4298 - acc: 0.8115 - val_loss: 0.4599 - val_acc: 0.7941\n",
      "Epoch 170/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4299 - acc: 0.8115 - val_loss: 0.4599 - val_acc: 0.7939\n",
      "Epoch 171/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8119 - val_loss: 0.4611 - val_acc: 0.7905\n",
      "Epoch 172/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4299 - acc: 0.8129 - val_loss: 0.4602 - val_acc: 0.7936\n",
      "Epoch 173/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4296 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7921\n",
      "Epoch 174/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4296 - acc: 0.8117 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4297 - acc: 0.8131 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 176/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4297 - acc: 0.8131 - val_loss: 0.4599 - val_acc: 0.7934\n",
      "Epoch 177/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4298 - acc: 0.8139 - val_loss: 0.4610 - val_acc: 0.7939\n",
      "Epoch 178/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4300 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7941\n",
      "Epoch 179/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4301 - acc: 0.8127 - val_loss: 0.4599 - val_acc: 0.7939\n",
      "Epoch 180/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4298 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 181/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4298 - acc: 0.8119 - val_loss: 0.4611 - val_acc: 0.7941\n",
      "Epoch 182/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8132 - val_loss: 0.4602 - val_acc: 0.7925\n",
      "Epoch 183/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4299 - acc: 0.8116 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 184/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 185/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8118 - val_loss: 0.4608 - val_acc: 0.7943\n",
      "Epoch 186/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8128 - val_loss: 0.4600 - val_acc: 0.7930\n",
      "Epoch 187/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4297 - acc: 0.8131 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 188/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4295 - acc: 0.8138 - val_loss: 0.4602 - val_acc: 0.7928\n",
      "Epoch 189/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4296 - acc: 0.8127 - val_loss: 0.4603 - val_acc: 0.7934\n",
      "Epoch 190/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4294 - acc: 0.8125 - val_loss: 0.4615 - val_acc: 0.7905\n",
      "Epoch 191/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4297 - acc: 0.8119 - val_loss: 0.4604 - val_acc: 0.7934\n",
      "Epoch 192/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8127 - val_loss: 0.4598 - val_acc: 0.7923\n",
      "Epoch 193/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4297 - acc: 0.8131 - val_loss: 0.4604 - val_acc: 0.7923\n",
      "Epoch 194/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4298 - acc: 0.8131 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 195/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4298 - acc: 0.8117 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 196/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4299 - acc: 0.8117 - val_loss: 0.4598 - val_acc: 0.7925\n",
      "Epoch 197/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4298 - acc: 0.8109 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 198/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4603 - val_acc: 0.7928\n",
      "Epoch 199/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4296 - acc: 0.8129 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 200/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4298 - acc: 0.8119 - val_loss: 0.4605 - val_acc: 0.7948\n",
      "Epoch 201/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4298 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7934\n",
      "Epoch 202/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4295 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7939\n",
      "Epoch 203/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4600 - val_acc: 0.7939\n",
      "Epoch 204/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8121 - val_loss: 0.4608 - val_acc: 0.7930\n",
      "Epoch 205/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7934\n",
      "Epoch 206/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4627 - val_acc: 0.7930\n",
      "Epoch 207/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4298 - acc: 0.8113 - val_loss: 0.4600 - val_acc: 0.7932\n",
      "Epoch 208/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4297 - acc: 0.8120 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 209/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4298 - acc: 0.8127 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 210/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4295 - acc: 0.8118 - val_loss: 0.4601 - val_acc: 0.7941\n",
      "Epoch 211/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.8121 - val_loss: 0.4600 - val_acc: 0.7930\n",
      "Epoch 212/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4298 - acc: 0.8119 - val_loss: 0.4607 - val_acc: 0.7941\n",
      "Epoch 213/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 214/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4296 - acc: 0.8115 - val_loss: 0.4597 - val_acc: 0.7928\n",
      "Epoch 215/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4299 - acc: 0.8118 - val_loss: 0.4604 - val_acc: 0.7934\n",
      "Epoch 216/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8128 - val_loss: 0.4612 - val_acc: 0.7939\n",
      "Epoch 217/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4298 - acc: 0.8128 - val_loss: 0.4604 - val_acc: 0.7939\n",
      "Epoch 218/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7923\n",
      "Epoch 219/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4297 - acc: 0.8122 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 220/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4298 - acc: 0.8115 - val_loss: 0.4600 - val_acc: 0.7936\n",
      "Epoch 221/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4295 - acc: 0.8122 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 222/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 223/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8126 - val_loss: 0.4610 - val_acc: 0.7930\n",
      "Epoch 224/1000\n",
      "13764/13764 [==============================] - 1s 53us/step - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 225/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8123 - val_loss: 0.4602 - val_acc: 0.7943\n",
      "Epoch 226/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4296 - acc: 0.8119 - val_loss: 0.4598 - val_acc: 0.7941\n",
      "Epoch 227/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4297 - acc: 0.8130 - val_loss: 0.4603 - val_acc: 0.7916\n",
      "Epoch 228/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4297 - acc: 0.8125 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 229/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4297 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 230/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4299 - acc: 0.8129 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 231/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4296 - acc: 0.8116 - val_loss: 0.4598 - val_acc: 0.7943\n",
      "Epoch 232/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4295 - acc: 0.8123 - val_loss: 0.4597 - val_acc: 0.7930\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8110 - val_loss: 0.4597 - val_acc: 0.7928\n",
      "Epoch 234/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4297 - acc: 0.8118 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 235/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 236/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8117 - val_loss: 0.4603 - val_acc: 0.7934\n",
      "Epoch 237/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8120 - val_loss: 0.4601 - val_acc: 0.7921\n",
      "Epoch 238/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4297 - acc: 0.8132 - val_loss: 0.4597 - val_acc: 0.7936\n",
      "Epoch 239/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4298 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 240/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4295 - acc: 0.8138 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 241/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4296 - acc: 0.8130 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 242/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 243/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8133 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 244/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4296 - acc: 0.8120 - val_loss: 0.4604 - val_acc: 0.7936\n",
      "Epoch 245/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4598 - val_acc: 0.7934\n",
      "Epoch 246/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8124 - val_loss: 0.4598 - val_acc: 0.7912\n",
      "Epoch 247/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 248/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.8120 - val_loss: 0.4603 - val_acc: 0.7928\n",
      "Epoch 249/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4297 - acc: 0.8122 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 250/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4296 - acc: 0.8134 - val_loss: 0.4602 - val_acc: 0.7928\n",
      "Epoch 251/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4297 - acc: 0.8118 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 252/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4601 - val_acc: 0.7925\n",
      "Epoch 253/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4294 - acc: 0.8134 - val_loss: 0.4601 - val_acc: 0.7939\n",
      "Epoch 254/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 255/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4295 - acc: 0.8123 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 256/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4296 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7930\n",
      "Epoch 257/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4295 - acc: 0.8136 - val_loss: 0.4605 - val_acc: 0.7932\n",
      "Epoch 258/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4294 - acc: 0.8133 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 259/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 260/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 261/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8119 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 262/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8121 - val_loss: 0.4597 - val_acc: 0.7921\n",
      "Epoch 263/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 264/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4599 - val_acc: 0.7934\n",
      "Epoch 265/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4295 - acc: 0.8142 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 266/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4297 - acc: 0.8124 - val_loss: 0.4601 - val_acc: 0.7941\n",
      "Epoch 267/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4294 - acc: 0.8118 - val_loss: 0.4604 - val_acc: 0.7936\n",
      "Epoch 268/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4295 - acc: 0.8130 - val_loss: 0.4599 - val_acc: 0.7923\n",
      "Epoch 269/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4295 - acc: 0.8131 - val_loss: 0.4606 - val_acc: 0.7934\n",
      "Epoch 270/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4295 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 271/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4297 - acc: 0.8131 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 272/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 273/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4293 - acc: 0.8140 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 274/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4298 - acc: 0.8127 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 275/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4299 - acc: 0.8127 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 276/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4297 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 277/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 278/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4296 - acc: 0.8127 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 279/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 280/1000\n",
      "13764/13764 [==============================] - 1s 45us/step - loss: 0.4296 - acc: 0.8118 - val_loss: 0.4613 - val_acc: 0.7941\n",
      "Epoch 281/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4298 - acc: 0.8128 - val_loss: 0.4604 - val_acc: 0.7934\n",
      "Epoch 282/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 283/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7923\n",
      "Epoch 284/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8114 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 285/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 286/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4297 - acc: 0.8122 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 287/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.8115 - val_loss: 0.4597 - val_acc: 0.7943\n",
      "Epoch 288/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4297 - acc: 0.8129 - val_loss: 0.4597 - val_acc: 0.7930\n",
      "Epoch 289/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4296 - acc: 0.8120 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 290/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8124 - val_loss: 0.4597 - val_acc: 0.7941\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8125 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 292/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8136 - val_loss: 0.4599 - val_acc: 0.7934\n",
      "Epoch 293/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 294/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8128 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 295/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4603 - val_acc: 0.7934\n",
      "Epoch 296/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8122 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 297/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4296 - acc: 0.8118 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 298/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4294 - acc: 0.8131 - val_loss: 0.4605 - val_acc: 0.7932\n",
      "Epoch 299/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4295 - acc: 0.8120 - val_loss: 0.4597 - val_acc: 0.7928\n",
      "Epoch 300/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.8127 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 301/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4295 - acc: 0.8131 - val_loss: 0.4601 - val_acc: 0.7939\n",
      "Epoch 302/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8114 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 303/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8129 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 304/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4296 - acc: 0.8123 - val_loss: 0.4605 - val_acc: 0.7934\n",
      "Epoch 305/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4295 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7932\n",
      "Epoch 306/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4297 - acc: 0.8131 - val_loss: 0.4600 - val_acc: 0.7921\n",
      "Epoch 307/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8140 - val_loss: 0.4597 - val_acc: 0.7924\n",
      "Epoch 308/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7943\n",
      "Epoch 309/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4296 - acc: 0.8132 - val_loss: 0.4597 - val_acc: 0.7943\n",
      "Epoch 310/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4294 - acc: 0.8136 - val_loss: 0.4605 - val_acc: 0.7934\n",
      "Epoch 311/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4295 - acc: 0.8119 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 312/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 313/1000\n",
      "13764/13764 [==============================] - 1s 44us/step - loss: 0.4295 - acc: 0.8129 - val_loss: 0.4605 - val_acc: 0.7928\n",
      "Epoch 314/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8119 - val_loss: 0.4607 - val_acc: 0.7941\n",
      "Epoch 315/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 316/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4296 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 317/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4297 - acc: 0.8127 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 318/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 319/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 320/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4297 - acc: 0.8124 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 321/1000\n",
      "13764/13764 [==============================] - 1s 44us/step - loss: 0.4294 - acc: 0.8130 - val_loss: 0.4600 - val_acc: 0.7930\n",
      "Epoch 322/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8122 - val_loss: 0.4604 - val_acc: 0.7943\n",
      "Epoch 323/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4296 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 324/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4294 - acc: 0.8129 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 325/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8140 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 326/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8142 - val_loss: 0.4598 - val_acc: 0.7934\n",
      "Epoch 327/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4293 - acc: 0.8121 - val_loss: 0.4599 - val_acc: 0.7934\n",
      "Epoch 328/1000\n",
      "13764/13764 [==============================] - 1s 45us/step - loss: 0.4294 - acc: 0.8132 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 329/1000\n",
      "13764/13764 [==============================] - 1s 43us/step - loss: 0.4294 - acc: 0.8129 - val_loss: 0.4605 - val_acc: 0.7939\n",
      "Epoch 330/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4295 - acc: 0.8121 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 331/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4598 - val_acc: 0.7939\n",
      "Epoch 332/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4294 - acc: 0.8120 - val_loss: 0.4614 - val_acc: 0.7934\n",
      "Epoch 333/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 334/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4609 - val_acc: 0.7939\n",
      "Epoch 335/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4295 - acc: 0.8129 - val_loss: 0.4617 - val_acc: 0.7925\n",
      "Epoch 336/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8127 - val_loss: 0.4605 - val_acc: 0.7930\n",
      "Epoch 337/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8127 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 338/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8123 - val_loss: 0.4605 - val_acc: 0.7939\n",
      "Epoch 339/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 340/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4294 - acc: 0.8134 - val_loss: 0.4604 - val_acc: 0.7934\n",
      "Epoch 341/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8140 - val_loss: 0.4600 - val_acc: 0.7914\n",
      "Epoch 342/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4295 - acc: 0.8136 - val_loss: 0.4601 - val_acc: 0.7932\n",
      "Epoch 343/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8137 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 344/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4297 - acc: 0.8125 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 345/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8132 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 346/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8135 - val_loss: 0.4596 - val_acc: 0.7925\n",
      "Epoch 347/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4295 - acc: 0.8130 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 348/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4295 - acc: 0.8131 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7936\n",
      "Epoch 350/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4608 - val_acc: 0.7934\n",
      "Epoch 351/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4296 - acc: 0.8132 - val_loss: 0.4596 - val_acc: 0.7936\n",
      "Epoch 352/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4295 - acc: 0.8132 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 353/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4295 - acc: 0.8125 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 354/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4293 - acc: 0.8137 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 355/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7939\n",
      "Epoch 356/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4295 - acc: 0.8125 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 357/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4294 - acc: 0.8139 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 358/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8134 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 359/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7941\n",
      "Epoch 360/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4601 - val_acc: 0.7932\n",
      "Epoch 361/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.8136 - val_loss: 0.4597 - val_acc: 0.7923\n",
      "Epoch 362/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4292 - acc: 0.8130 - val_loss: 0.4607 - val_acc: 0.7905\n",
      "Epoch 363/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4616 - val_acc: 0.7925\n",
      "Epoch 364/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4294 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 365/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8131 - val_loss: 0.4599 - val_acc: 0.7945\n",
      "Epoch 366/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 367/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8129 - val_loss: 0.4598 - val_acc: 0.7941\n",
      "Epoch 368/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8127 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 369/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8135 - val_loss: 0.4598 - val_acc: 0.7921\n",
      "Epoch 370/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8132 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 371/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8125 - val_loss: 0.4606 - val_acc: 0.7916\n",
      "Epoch 372/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4295 - acc: 0.8130 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 373/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4296 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 374/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8130 - val_loss: 0.4598 - val_acc: 0.7919\n",
      "Epoch 375/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4294 - acc: 0.8124 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 376/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8120 - val_loss: 0.4600 - val_acc: 0.7923\n",
      "Epoch 377/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4294 - acc: 0.8130 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 378/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.8136 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 379/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8120 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 380/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8134 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 381/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4604 - val_acc: 0.7932\n",
      "Epoch 382/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8121 - val_loss: 0.4602 - val_acc: 0.7925\n",
      "Epoch 383/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4293 - acc: 0.8137 - val_loss: 0.4617 - val_acc: 0.7928\n",
      "Epoch 384/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8133 - val_loss: 0.4595 - val_acc: 0.7943\n",
      "Epoch 385/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8125 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 386/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.8123 - val_loss: 0.4597 - val_acc: 0.7925\n",
      "Epoch 387/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4293 - acc: 0.8135 - val_loss: 0.4594 - val_acc: 0.7952\n",
      "Epoch 388/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4293 - acc: 0.8127 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 389/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7939\n",
      "Epoch 390/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 391/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8122 - val_loss: 0.4596 - val_acc: 0.7945\n",
      "Epoch 392/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8127 - val_loss: 0.4601 - val_acc: 0.7925\n",
      "Epoch 393/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4290 - acc: 0.8129 - val_loss: 0.4597 - val_acc: 0.7919\n",
      "Epoch 394/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.8132 - val_loss: 0.4597 - val_acc: 0.7943\n",
      "Epoch 395/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4293 - acc: 0.8125 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 396/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8139 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 397/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7925\n",
      "Epoch 398/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8129 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 399/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4293 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7925\n",
      "Epoch 400/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4601 - val_acc: 0.7943\n",
      "Epoch 401/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8142 - val_loss: 0.4602 - val_acc: 0.7941\n",
      "Epoch 402/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7921\n",
      "Epoch 403/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8135 - val_loss: 0.4617 - val_acc: 0.7930\n",
      "Epoch 404/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4293 - acc: 0.8137 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 405/1000\n",
      "13764/13764 [==============================] - 1s 43us/step - loss: 0.4293 - acc: 0.8127 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 406/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8127 - val_loss: 0.4601 - val_acc: 0.7923\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8142 - val_loss: 0.4605 - val_acc: 0.7932\n",
      "Epoch 408/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 409/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7943\n",
      "Epoch 410/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8131 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 411/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8127 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 412/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8136 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 413/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7923\n",
      "Epoch 414/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4293 - acc: 0.8141 - val_loss: 0.4616 - val_acc: 0.7928\n",
      "Epoch 415/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4294 - acc: 0.8132 - val_loss: 0.4596 - val_acc: 0.7945\n",
      "Epoch 416/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8135 - val_loss: 0.4597 - val_acc: 0.7923\n",
      "Epoch 417/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4297 - acc: 0.8118 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 418/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4294 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7914\n",
      "Epoch 419/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8139 - val_loss: 0.4611 - val_acc: 0.7925\n",
      "Epoch 420/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8121 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 421/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8137 - val_loss: 0.4602 - val_acc: 0.7930\n",
      "Epoch 422/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4293 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 423/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4292 - acc: 0.8124 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 424/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4294 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7936\n",
      "Epoch 425/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8116 - val_loss: 0.4597 - val_acc: 0.7945\n",
      "Epoch 426/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4294 - acc: 0.8117 - val_loss: 0.4599 - val_acc: 0.7941\n",
      "Epoch 427/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4293 - acc: 0.8120 - val_loss: 0.4596 - val_acc: 0.7921\n",
      "Epoch 428/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 429/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4295 - acc: 0.8138 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 430/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8124 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 431/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4291 - acc: 0.8120 - val_loss: 0.4608 - val_acc: 0.7939\n",
      "Epoch 432/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8110 - val_loss: 0.4601 - val_acc: 0.7939\n",
      "Epoch 433/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4293 - acc: 0.8143 - val_loss: 0.4604 - val_acc: 0.7925\n",
      "Epoch 434/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 435/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7948\n",
      "Epoch 436/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 437/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7945\n",
      "Epoch 438/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4293 - acc: 0.8114 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 439/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8121 - val_loss: 0.4598 - val_acc: 0.7948\n",
      "Epoch 440/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 441/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4292 - acc: 0.8133 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 442/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7916\n",
      "Epoch 443/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4295 - acc: 0.8128 - val_loss: 0.4605 - val_acc: 0.7934\n",
      "Epoch 444/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8125 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 445/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4293 - acc: 0.8121 - val_loss: 0.4594 - val_acc: 0.7930\n",
      "Epoch 446/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4594 - val_acc: 0.7923\n",
      "Epoch 447/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4293 - acc: 0.8114 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 448/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4296 - acc: 0.8129 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 449/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4294 - acc: 0.8123 - val_loss: 0.4595 - val_acc: 0.7950\n",
      "Epoch 450/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4293 - acc: 0.8138 - val_loss: 0.4602 - val_acc: 0.7919\n",
      "Epoch 451/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 452/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 453/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4292 - acc: 0.8127 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 454/1000\n",
      "13764/13764 [==============================] - 1s 49us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 455/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8133 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 456/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4292 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7932\n",
      "Epoch 457/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8137 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 458/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 459/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.8135 - val_loss: 0.4598 - val_acc: 0.7943\n",
      "Epoch 460/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8132 - val_loss: 0.4610 - val_acc: 0.7928\n",
      "Epoch 461/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4294 - acc: 0.8132 - val_loss: 0.4601 - val_acc: 0.7928\n",
      "Epoch 462/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 463/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7930\n",
      "Epoch 464/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4295 - acc: 0.8135 - val_loss: 0.4594 - val_acc: 0.7925\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4597 - val_acc: 0.7921\n",
      "Epoch 466/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4294 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 467/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7925\n",
      "Epoch 468/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4595 - val_acc: 0.7919\n",
      "Epoch 469/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7919\n",
      "Epoch 470/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4293 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 471/1000\n",
      "13764/13764 [==============================] - 1s 47us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7934\n",
      "Epoch 472/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4606 - val_acc: 0.7936\n",
      "Epoch 473/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7930\n",
      "Epoch 474/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8125 - val_loss: 0.4604 - val_acc: 0.7936\n",
      "Epoch 475/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4606 - val_acc: 0.7936\n",
      "Epoch 476/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4293 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7928\n",
      "Epoch 477/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.8140 - val_loss: 0.4594 - val_acc: 0.7921\n",
      "Epoch 478/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 479/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7936\n",
      "Epoch 480/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 481/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4294 - acc: 0.8128 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 482/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8124 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 483/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4605 - val_acc: 0.7934\n",
      "Epoch 484/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 485/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8134 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 486/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7941\n",
      "Epoch 487/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 488/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8139 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 489/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4593 - val_acc: 0.7934\n",
      "Epoch 490/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4296 - acc: 0.8138 - val_loss: 0.4604 - val_acc: 0.7932\n",
      "Epoch 491/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8125 - val_loss: 0.4599 - val_acc: 0.7945\n",
      "Epoch 492/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8133 - val_loss: 0.4593 - val_acc: 0.7923\n",
      "Epoch 493/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4293 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 494/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 495/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4293 - acc: 0.8134 - val_loss: 0.4600 - val_acc: 0.7930\n",
      "Epoch 496/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7923\n",
      "Epoch 497/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8120 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 498/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4605 - val_acc: 0.7932\n",
      "Epoch 499/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8127 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 500/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4292 - acc: 0.8135 - val_loss: 0.4611 - val_acc: 0.7930\n",
      "Epoch 501/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4293 - acc: 0.8137 - val_loss: 0.4594 - val_acc: 0.7930\n",
      "Epoch 502/1000\n",
      "13764/13764 [==============================] - 1s 47us/step - loss: 0.4292 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 503/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4294 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 504/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8120 - val_loss: 0.4600 - val_acc: 0.7919\n",
      "Epoch 505/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 506/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4292 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 507/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4292 - acc: 0.8139 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 508/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4294 - acc: 0.8138 - val_loss: 0.4596 - val_acc: 0.7916\n",
      "Epoch 509/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 510/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8123 - val_loss: 0.4602 - val_acc: 0.7919\n",
      "Epoch 511/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4293 - acc: 0.8124 - val_loss: 0.4602 - val_acc: 0.7928\n",
      "Epoch 512/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4295 - acc: 0.8120 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 513/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8137 - val_loss: 0.4595 - val_acc: 0.7921\n",
      "Epoch 514/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 515/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7936\n",
      "Epoch 516/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 517/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4293 - acc: 0.8129 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 518/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8127 - val_loss: 0.4606 - val_acc: 0.7932\n",
      "Epoch 519/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4291 - acc: 0.8119 - val_loss: 0.4605 - val_acc: 0.7932\n",
      "Epoch 520/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7945\n",
      "Epoch 521/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8139 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 522/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4290 - acc: 0.8137 - val_loss: 0.4612 - val_acc: 0.7928\n",
      "Epoch 523/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4293 - acc: 0.8125 - val_loss: 0.4593 - val_acc: 0.7936\n",
      "Epoch 524/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8120 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 525/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 526/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8129 - val_loss: 0.4594 - val_acc: 0.7925\n",
      "Epoch 527/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4293 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 528/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 529/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8127 - val_loss: 0.4595 - val_acc: 0.7943\n",
      "Epoch 530/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4290 - acc: 0.8142 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 531/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8124 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 532/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8135 - val_loss: 0.4596 - val_acc: 0.7921\n",
      "Epoch 533/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7916\n",
      "Epoch 534/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8125 - val_loss: 0.4596 - val_acc: 0.7912\n",
      "Epoch 535/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4292 - acc: 0.8135 - val_loss: 0.4597 - val_acc: 0.7936\n",
      "Epoch 536/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7908\n",
      "Epoch 537/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4598 - val_acc: 0.7905\n",
      "Epoch 538/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4294 - acc: 0.8132 - val_loss: 0.4601 - val_acc: 0.7921\n",
      "Epoch 539/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8129 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 540/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 541/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 542/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8149 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 543/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4605 - val_acc: 0.7932\n",
      "Epoch 544/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8133 - val_loss: 0.4596 - val_acc: 0.7925\n",
      "Epoch 545/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7928\n",
      "Epoch 546/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4600 - val_acc: 0.7948\n",
      "Epoch 547/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4293 - acc: 0.8131 - val_loss: 0.4604 - val_acc: 0.7932\n",
      "Epoch 548/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 549/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8146 - val_loss: 0.4597 - val_acc: 0.7916\n",
      "Epoch 550/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 551/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7916\n",
      "Epoch 552/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8129 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 553/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 554/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7941\n",
      "Epoch 555/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4292 - acc: 0.8125 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 556/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 557/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 558/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 559/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8127 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 560/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8130 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 561/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8132 - val_loss: 0.4605 - val_acc: 0.7930\n",
      "Epoch 562/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8125 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 563/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8136 - val_loss: 0.4607 - val_acc: 0.7928\n",
      "Epoch 564/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8144 - val_loss: 0.4606 - val_acc: 0.7930\n",
      "Epoch 565/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8117 - val_loss: 0.4597 - val_acc: 0.7925\n",
      "Epoch 566/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4290 - acc: 0.8132 - val_loss: 0.4634 - val_acc: 0.7934\n",
      "Epoch 567/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4297 - acc: 0.8136 - val_loss: 0.4606 - val_acc: 0.7936\n",
      "Epoch 568/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8122 - val_loss: 0.4619 - val_acc: 0.7934\n",
      "Epoch 569/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4294 - acc: 0.8119 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 570/1000\n",
      "13764/13764 [==============================] - 1s 47us/step - loss: 0.4291 - acc: 0.8124 - val_loss: 0.4599 - val_acc: 0.7945\n",
      "Epoch 571/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4294 - acc: 0.8129 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 572/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8142 - val_loss: 0.4596 - val_acc: 0.7912\n",
      "Epoch 573/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 574/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8123 - val_loss: 0.4612 - val_acc: 0.7934\n",
      "Epoch 575/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4292 - acc: 0.8135 - val_loss: 0.4597 - val_acc: 0.7921\n",
      "Epoch 576/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4291 - acc: 0.8135 - val_loss: 0.4608 - val_acc: 0.7936\n",
      "Epoch 577/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4288 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7941\n",
      "Epoch 578/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4291 - acc: 0.8133 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 579/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 580/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4618 - val_acc: 0.7943\n",
      "Epoch 581/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4604 - val_acc: 0.7932\n",
      "Epoch 582/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4293 - acc: 0.8130 - val_loss: 0.4596 - val_acc: 0.7945\n",
      "Epoch 583/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8142 - val_loss: 0.4602 - val_acc: 0.7908\n",
      "Epoch 584/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4293 - acc: 0.8121 - val_loss: 0.4595 - val_acc: 0.7943\n",
      "Epoch 585/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4292 - acc: 0.8137 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 586/1000\n",
      "13764/13764 [==============================] - 1s 48us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4600 - val_acc: 0.7928\n",
      "Epoch 587/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.8133 - val_loss: 0.4597 - val_acc: 0.7930\n",
      "Epoch 588/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8135 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 589/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4290 - acc: 0.8138 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 590/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7930\n",
      "Epoch 591/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8138 - val_loss: 0.4605 - val_acc: 0.7923\n",
      "Epoch 592/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8118 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 593/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4606 - val_acc: 0.7923\n",
      "Epoch 594/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4293 - acc: 0.8132 - val_loss: 0.4607 - val_acc: 0.7930\n",
      "Epoch 595/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8120 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 596/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8132 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 597/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8137 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 598/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 599/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7936\n",
      "Epoch 600/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4592 - val_acc: 0.7936\n",
      "Epoch 601/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 602/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4291 - acc: 0.8129 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 603/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4294 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7923\n",
      "Epoch 604/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4291 - acc: 0.8129 - val_loss: 0.4596 - val_acc: 0.7925\n",
      "Epoch 605/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7925\n",
      "Epoch 606/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 607/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 608/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8142 - val_loss: 0.4596 - val_acc: 0.7925\n",
      "Epoch 609/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4596 - val_acc: 0.7925\n",
      "Epoch 610/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8120 - val_loss: 0.4599 - val_acc: 0.7916\n",
      "Epoch 611/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7945\n",
      "Epoch 612/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 613/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8141 - val_loss: 0.4595 - val_acc: 0.7912\n",
      "Epoch 614/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8120 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 615/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7945\n",
      "Epoch 616/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4599 - val_acc: 0.7948\n",
      "Epoch 617/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8130 - val_loss: 0.4614 - val_acc: 0.7950\n",
      "Epoch 618/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8133 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 619/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7919\n",
      "Epoch 620/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8127 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 621/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8133 - val_loss: 0.4594 - val_acc: 0.7930\n",
      "Epoch 622/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8127 - val_loss: 0.4613 - val_acc: 0.7923\n",
      "Epoch 623/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4293 - acc: 0.8117 - val_loss: 0.4599 - val_acc: 0.7923\n",
      "Epoch 624/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4598 - val_acc: 0.7923\n",
      "Epoch 625/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 626/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8124 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 627/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4596 - val_acc: 0.7943\n",
      "Epoch 628/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8137 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 629/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 630/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7921\n",
      "Epoch 631/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8132 - val_loss: 0.4598 - val_acc: 0.7912\n",
      "Epoch 632/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 633/1000\n",
      "13764/13764 [==============================] - 1s 44us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4602 - val_acc: 0.7939\n",
      "Epoch 634/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8140 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 635/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8136 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 636/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4600 - val_acc: 0.7925\n",
      "Epoch 637/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4599 - val_acc: 0.7923\n",
      "Epoch 638/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4598 - val_acc: 0.7941\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4602 - val_acc: 0.7934\n",
      "Epoch 640/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8129 - val_loss: 0.4600 - val_acc: 0.7945\n",
      "Epoch 641/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8132 - val_loss: 0.4601 - val_acc: 0.7914\n",
      "Epoch 642/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7925\n",
      "Epoch 643/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4600 - val_acc: 0.7941\n",
      "Epoch 644/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4293 - acc: 0.8136 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 645/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8139 - val_loss: 0.4596 - val_acc: 0.7945\n",
      "Epoch 646/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 647/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8124 - val_loss: 0.4596 - val_acc: 0.7936\n",
      "Epoch 648/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 649/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4295 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7943\n",
      "Epoch 650/1000\n",
      "13764/13764 [==============================] - 1s 44us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 651/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8133 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 652/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8133 - val_loss: 0.4595 - val_acc: 0.7941\n",
      "Epoch 653/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4593 - val_acc: 0.7932\n",
      "Epoch 654/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 655/1000\n",
      "13764/13764 [==============================] - ETA: 0s - loss: 0.4291 - acc: 0.813 - 1s 37us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4597 - val_acc: 0.7950\n",
      "Epoch 656/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4288 - acc: 0.8142 - val_loss: 0.4608 - val_acc: 0.7932\n",
      "Epoch 657/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8138 - val_loss: 0.4601 - val_acc: 0.7928\n",
      "Epoch 658/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 659/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8137 - val_loss: 0.4602 - val_acc: 0.7925\n",
      "Epoch 660/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4288 - acc: 0.8133 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 661/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4293 - acc: 0.8133 - val_loss: 0.4599 - val_acc: 0.7941\n",
      "Epoch 662/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4598 - val_acc: 0.7941\n",
      "Epoch 663/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8137 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 664/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4290 - acc: 0.8140 - val_loss: 0.4603 - val_acc: 0.7932\n",
      "Epoch 665/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4290 - acc: 0.8124 - val_loss: 0.4602 - val_acc: 0.7930\n",
      "Epoch 666/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4291 - acc: 0.8122 - val_loss: 0.4598 - val_acc: 0.7934\n",
      "Epoch 667/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7941\n",
      "Epoch 668/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4599 - val_acc: 0.7921\n",
      "Epoch 669/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8138 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 670/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8143 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 671/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 672/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8127 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 673/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4607 - val_acc: 0.7932\n",
      "Epoch 674/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4292 - acc: 0.8132 - val_loss: 0.4598 - val_acc: 0.7939\n",
      "Epoch 675/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8125 - val_loss: 0.4599 - val_acc: 0.7938\n",
      "Epoch 676/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4611 - val_acc: 0.7932\n",
      "Epoch 677/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8138 - val_loss: 0.4599 - val_acc: 0.7939\n",
      "Epoch 678/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8144 - val_loss: 0.4605 - val_acc: 0.7921\n",
      "Epoch 679/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 680/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8138 - val_loss: 0.4597 - val_acc: 0.7921\n",
      "Epoch 681/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4290 - acc: 0.8133 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 682/1000\n",
      "13764/13764 [==============================] - 1s 36us/step - loss: 0.4291 - acc: 0.8130 - val_loss: 0.4607 - val_acc: 0.7939\n",
      "Epoch 683/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8135 - val_loss: 0.4612 - val_acc: 0.7936\n",
      "Epoch 684/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8132 - val_loss: 0.4597 - val_acc: 0.7943\n",
      "Epoch 685/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8133 - val_loss: 0.4605 - val_acc: 0.7930\n",
      "Epoch 686/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8142 - val_loss: 0.4601 - val_acc: 0.7928\n",
      "Epoch 687/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4293 - acc: 0.8135 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 688/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7932\n",
      "Epoch 689/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 690/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8136 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 691/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8135 - val_loss: 0.4593 - val_acc: 0.7932\n",
      "Epoch 692/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 693/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 694/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 695/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4293 - acc: 0.8122 - val_loss: 0.4594 - val_acc: 0.7921\n",
      "Epoch 696/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8123 - val_loss: 0.4595 - val_acc: 0.7930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 697/1000\n",
      "13764/13764 [==============================] - 1s 43us/step - loss: 0.4288 - acc: 0.8129 - val_loss: 0.4605 - val_acc: 0.7934\n",
      "Epoch 698/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4292 - acc: 0.8136 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 699/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8116 - val_loss: 0.4609 - val_acc: 0.7932\n",
      "Epoch 700/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4602 - val_acc: 0.7914\n",
      "Epoch 701/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4293 - acc: 0.8132 - val_loss: 0.4611 - val_acc: 0.7934\n",
      "Epoch 702/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4593 - val_acc: 0.7923\n",
      "Epoch 703/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 704/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8137 - val_loss: 0.4609 - val_acc: 0.7932\n",
      "Epoch 705/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 706/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 707/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4292 - acc: 0.8121 - val_loss: 0.4597 - val_acc: 0.7930\n",
      "Epoch 708/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 709/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8129 - val_loss: 0.4611 - val_acc: 0.7932\n",
      "Epoch 710/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 711/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4289 - acc: 0.8142 - val_loss: 0.4601 - val_acc: 0.7925\n",
      "Epoch 712/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4604 - val_acc: 0.7928\n",
      "Epoch 713/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8135 - val_loss: 0.4595 - val_acc: 0.7923\n",
      "Epoch 714/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8134 - val_loss: 0.4600 - val_acc: 0.7932\n",
      "Epoch 715/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7923\n",
      "Epoch 716/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8141 - val_loss: 0.4594 - val_acc: 0.7945\n",
      "Epoch 717/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7914\n",
      "Epoch 718/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 719/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4603 - val_acc: 0.7934\n",
      "Epoch 720/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4289 - acc: 0.8131 - val_loss: 0.4600 - val_acc: 0.7932\n",
      "Epoch 721/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4290 - acc: 0.8130 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 722/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7930\n",
      "Epoch 723/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4605 - val_acc: 0.7923\n",
      "Epoch 724/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8139 - val_loss: 0.4604 - val_acc: 0.7930\n",
      "Epoch 725/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7923\n",
      "Epoch 726/1000\n",
      "13764/13764 [==============================] - 1s 46us/step - loss: 0.4292 - acc: 0.8125 - val_loss: 0.4600 - val_acc: 0.7925\n",
      "Epoch 727/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8130 - val_loss: 0.4600 - val_acc: 0.7932\n",
      "Epoch 728/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 729/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8129 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 730/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4290 - acc: 0.8119 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 731/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7921\n",
      "Epoch 732/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4605 - val_acc: 0.7932\n",
      "Epoch 733/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8112 - val_loss: 0.4594 - val_acc: 0.7925\n",
      "Epoch 734/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7921\n",
      "Epoch 735/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8135 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 736/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 737/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8121 - val_loss: 0.4602 - val_acc: 0.7928\n",
      "Epoch 738/1000\n",
      "13764/13764 [==============================] - 1s 45us/step - loss: 0.4291 - acc: 0.8127 - val_loss: 0.4596 - val_acc: 0.7948\n",
      "Epoch 739/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8144 - val_loss: 0.4607 - val_acc: 0.7930\n",
      "Epoch 740/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7950\n",
      "Epoch 741/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4293 - acc: 0.8135 - val_loss: 0.4594 - val_acc: 0.7943\n",
      "Epoch 742/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4293 - acc: 0.8134 - val_loss: 0.4605 - val_acc: 0.7930\n",
      "Epoch 743/1000\n",
      "13764/13764 [==============================] - 1s 47us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7923\n",
      "Epoch 744/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4288 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 745/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 746/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7941\n",
      "Epoch 747/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4289 - acc: 0.8136 - val_loss: 0.4604 - val_acc: 0.7932\n",
      "Epoch 748/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8133 - val_loss: 0.4594 - val_acc: 0.7945\n",
      "Epoch 749/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7932\n",
      "Epoch 750/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8120 - val_loss: 0.4598 - val_acc: 0.7923\n",
      "Epoch 751/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8135 - val_loss: 0.4597 - val_acc: 0.7950\n",
      "Epoch 752/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4293 - acc: 0.8123 - val_loss: 0.4594 - val_acc: 0.7941\n",
      "Epoch 753/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4289 - acc: 0.8138 - val_loss: 0.4604 - val_acc: 0.7925\n",
      "Epoch 754/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8129 - val_loss: 0.4601 - val_acc: 0.7939\n",
      "Epoch 756/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4291 - acc: 0.8125 - val_loss: 0.4593 - val_acc: 0.7925\n",
      "Epoch 757/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8144 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 758/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 759/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4286 - acc: 0.8147 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 760/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 761/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4288 - acc: 0.8133 - val_loss: 0.4616 - val_acc: 0.7916\n",
      "Epoch 762/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7932\n",
      "Epoch 763/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4289 - acc: 0.8136 - val_loss: 0.4605 - val_acc: 0.7923\n",
      "Epoch 764/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 765/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4596 - val_acc: 0.7921\n",
      "Epoch 766/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4291 - acc: 0.8127 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 767/1000\n",
      "13764/13764 [==============================] - 1s 43us/step - loss: 0.4289 - acc: 0.8120 - val_loss: 0.4601 - val_acc: 0.7932\n",
      "Epoch 768/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4293 - acc: 0.8135 - val_loss: 0.4599 - val_acc: 0.7943\n",
      "Epoch 769/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4292 - acc: 0.8131 - val_loss: 0.4609 - val_acc: 0.7930\n",
      "Epoch 770/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 771/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4289 - acc: 0.8138 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 772/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4287 - acc: 0.8139 - val_loss: 0.4604 - val_acc: 0.7928\n",
      "Epoch 773/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8120 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 774/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4599 - val_acc: 0.7932\n",
      "Epoch 775/1000\n",
      "13764/13764 [==============================] - 1s 61us/step - loss: 0.4288 - acc: 0.8137 - val_loss: 0.4606 - val_acc: 0.7928\n",
      "Epoch 776/1000\n",
      "13764/13764 [==============================] - 1s 46us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4608 - val_acc: 0.7948\n",
      "Epoch 777/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7941\n",
      "Epoch 778/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7948\n",
      "Epoch 779/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8121 - val_loss: 0.4600 - val_acc: 0.7919\n",
      "Epoch 780/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8130 - val_loss: 0.4605 - val_acc: 0.7939\n",
      "Epoch 781/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4290 - acc: 0.8135 - val_loss: 0.4593 - val_acc: 0.7925\n",
      "Epoch 782/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4287 - acc: 0.8144 - val_loss: 0.4608 - val_acc: 0.7943\n",
      "Epoch 783/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7919\n",
      "Epoch 784/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4602 - val_acc: 0.7936\n",
      "Epoch 785/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4291 - acc: 0.8138 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 786/1000\n",
      "13764/13764 [==============================] - 1s 49us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4609 - val_acc: 0.7932\n",
      "Epoch 787/1000\n",
      "13764/13764 [==============================] - 1s 50us/step - loss: 0.4289 - acc: 0.8135 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 788/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8133 - val_loss: 0.4596 - val_acc: 0.7923\n",
      "Epoch 789/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4598 - val_acc: 0.7941\n",
      "Epoch 790/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 791/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.8133 - val_loss: 0.4594 - val_acc: 0.7923\n",
      "Epoch 792/1000\n",
      "13764/13764 [==============================] - 1s 61us/step - loss: 0.4290 - acc: 0.8129 - val_loss: 0.4599 - val_acc: 0.7945\n",
      "Epoch 793/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4288 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7950\n",
      "Epoch 794/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8141 - val_loss: 0.4597 - val_acc: 0.7919\n",
      "Epoch 795/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8133 - val_loss: 0.4598 - val_acc: 0.7923\n",
      "Epoch 796/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8138 - val_loss: 0.4603 - val_acc: 0.7930\n",
      "Epoch 797/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4287 - acc: 0.8138 - val_loss: 0.4594 - val_acc: 0.7950\n",
      "Epoch 798/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 799/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4602 - val_acc: 0.7936\n",
      "Epoch 800/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4605 - val_acc: 0.7928\n",
      "Epoch 801/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4290 - acc: 0.8137 - val_loss: 0.4599 - val_acc: 0.7936\n",
      "Epoch 802/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 803/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4287 - acc: 0.8136 - val_loss: 0.4597 - val_acc: 0.7948\n",
      "Epoch 804/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4292 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 805/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4602 - val_acc: 0.7936\n",
      "Epoch 806/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8146 - val_loss: 0.4594 - val_acc: 0.7932\n",
      "Epoch 807/1000\n",
      "13764/13764 [==============================] - 1s 49us/step - loss: 0.4291 - acc: 0.8125 - val_loss: 0.4593 - val_acc: 0.7936\n",
      "Epoch 808/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7936\n",
      "Epoch 809/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4288 - acc: 0.8142 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 810/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4602 - val_acc: 0.7928\n",
      "Epoch 811/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8119 - val_loss: 0.4594 - val_acc: 0.7925\n",
      "Epoch 812/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4594 - val_acc: 0.7945\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4288 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7925\n",
      "Epoch 814/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4594 - val_acc: 0.7930\n",
      "Epoch 815/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8127 - val_loss: 0.4598 - val_acc: 0.7939\n",
      "Epoch 816/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7936\n",
      "Epoch 817/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8122 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 818/1000\n",
      "13764/13764 [==============================] - 1s 50us/step - loss: 0.4291 - acc: 0.8141 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 819/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4616 - val_acc: 0.7941\n",
      "Epoch 820/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8125 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 821/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7921\n",
      "Epoch 822/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8144 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 823/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4290 - acc: 0.8122 - val_loss: 0.4595 - val_acc: 0.7921\n",
      "Epoch 824/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4289 - acc: 0.8126 - val_loss: 0.4595 - val_acc: 0.7943\n",
      "Epoch 825/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4288 - acc: 0.8123 - val_loss: 0.4623 - val_acc: 0.7932\n",
      "Epoch 826/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8143 - val_loss: 0.4596 - val_acc: 0.7945\n",
      "Epoch 827/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8132 - val_loss: 0.4593 - val_acc: 0.7923\n",
      "Epoch 828/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8129 - val_loss: 0.4593 - val_acc: 0.7925\n",
      "Epoch 829/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8143 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 830/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4292 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7923\n",
      "Epoch 831/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 832/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.8137 - val_loss: 0.4594 - val_acc: 0.7928\n",
      "Epoch 833/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4289 - acc: 0.8134 - val_loss: 0.4598 - val_acc: 0.7919\n",
      "Epoch 834/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4291 - acc: 0.8125 - val_loss: 0.4594 - val_acc: 0.7952\n",
      "Epoch 835/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8129 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 836/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4593 - val_acc: 0.7930\n",
      "Epoch 837/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8142 - val_loss: 0.4597 - val_acc: 0.7925\n",
      "Epoch 838/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 839/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4289 - acc: 0.8129 - val_loss: 0.4601 - val_acc: 0.7921\n",
      "Epoch 840/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8130 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 841/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 842/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4288 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7945\n",
      "Epoch 843/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4608 - val_acc: 0.7934\n",
      "Epoch 844/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7945\n",
      "Epoch 845/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8129 - val_loss: 0.4598 - val_acc: 0.7943\n",
      "Epoch 846/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4605 - val_acc: 0.7930\n",
      "Epoch 847/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 848/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8124 - val_loss: 0.4595 - val_acc: 0.7919\n",
      "Epoch 849/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4286 - acc: 0.8139 - val_loss: 0.4603 - val_acc: 0.7923\n",
      "Epoch 850/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 851/1000\n",
      "13764/13764 [==============================] - 1s 36us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4594 - val_acc: 0.7941\n",
      "Epoch 852/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4288 - acc: 0.8135 - val_loss: 0.4594 - val_acc: 0.7921\n",
      "Epoch 853/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4289 - acc: 0.8134 - val_loss: 0.4595 - val_acc: 0.7923\n",
      "Epoch 854/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 855/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4286 - acc: 0.8127 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 856/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4596 - val_acc: 0.7923\n",
      "Epoch 857/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4291 - acc: 0.8132 - val_loss: 0.4596 - val_acc: 0.7919\n",
      "Epoch 858/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 859/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4603 - val_acc: 0.7923\n",
      "Epoch 860/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4291 - acc: 0.8127 - val_loss: 0.4597 - val_acc: 0.7914\n",
      "Epoch 861/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4289 - acc: 0.8127 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 862/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 863/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8130 - val_loss: 0.4596 - val_acc: 0.7936\n",
      "Epoch 864/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 865/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7943\n",
      "Epoch 866/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4594 - val_acc: 0.7932\n",
      "Epoch 867/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4290 - acc: 0.8145 - val_loss: 0.4600 - val_acc: 0.7936\n",
      "Epoch 868/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4286 - acc: 0.8128 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 869/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4596 - val_acc: 0.7923\n",
      "Epoch 870/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4288 - acc: 0.8117 - val_loss: 0.4596 - val_acc: 0.7932\n",
      "Epoch 871/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4289 - acc: 0.8149 - val_loss: 0.4628 - val_acc: 0.7928\n",
      "Epoch 872/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4294 - acc: 0.8141 - val_loss: 0.4599 - val_acc: 0.7943\n",
      "Epoch 873/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 874/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4288 - acc: 0.8132 - val_loss: 0.4599 - val_acc: 0.7919\n",
      "Epoch 875/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8142 - val_loss: 0.4595 - val_acc: 0.7941\n",
      "Epoch 876/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8133 - val_loss: 0.4593 - val_acc: 0.7932\n",
      "Epoch 877/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8130 - val_loss: 0.4596 - val_acc: 0.7921\n",
      "Epoch 878/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4593 - val_acc: 0.7939\n",
      "Epoch 879/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7941\n",
      "Epoch 880/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4593 - val_acc: 0.7923\n",
      "Epoch 881/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8125 - val_loss: 0.4601 - val_acc: 0.7939\n",
      "Epoch 882/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7943\n",
      "Epoch 883/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8132 - val_loss: 0.4594 - val_acc: 0.7934\n",
      "Epoch 884/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4289 - acc: 0.8129 - val_loss: 0.4594 - val_acc: 0.7943\n",
      "Epoch 885/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4288 - acc: 0.8137 - val_loss: 0.4599 - val_acc: 0.7952\n",
      "Epoch 886/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4594 - val_acc: 0.7925\n",
      "Epoch 887/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 888/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7923\n",
      "Epoch 889/1000\n",
      "13764/13764 [==============================] - 1s 36us/step - loss: 0.4288 - acc: 0.8135 - val_loss: 0.4632 - val_acc: 0.7934\n",
      "Epoch 890/1000\n",
      "13764/13764 [==============================] - 1s 47us/step - loss: 0.4291 - acc: 0.8143 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 891/1000\n",
      "13764/13764 [==============================] - 1s 43us/step - loss: 0.4288 - acc: 0.8136 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 892/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8136 - val_loss: 0.4611 - val_acc: 0.7941\n",
      "Epoch 893/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4593 - val_acc: 0.7936\n",
      "Epoch 894/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4603 - val_acc: 0.7943\n",
      "Epoch 895/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4288 - acc: 0.8130 - val_loss: 0.4594 - val_acc: 0.7950\n",
      "Epoch 896/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4592 - val_acc: 0.7923\n",
      "Epoch 897/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 898/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4290 - acc: 0.8137 - val_loss: 0.4598 - val_acc: 0.7939\n",
      "Epoch 899/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4289 - acc: 0.8141 - val_loss: 0.4594 - val_acc: 0.7919\n",
      "Epoch 900/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4288 - acc: 0.8140 - val_loss: 0.4594 - val_acc: 0.7912\n",
      "Epoch 901/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8131 - val_loss: 0.4592 - val_acc: 0.7936\n",
      "Epoch 902/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4287 - acc: 0.8144 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 903/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8129 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 904/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4288 - acc: 0.8133 - val_loss: 0.4611 - val_acc: 0.7943\n",
      "Epoch 905/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4288 - acc: 0.8145 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 906/1000\n",
      "13764/13764 [==============================] - 1s 45us/step - loss: 0.4289 - acc: 0.8131 - val_loss: 0.4604 - val_acc: 0.7945\n",
      "Epoch 907/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4288 - acc: 0.8134 - val_loss: 0.4605 - val_acc: 0.7939\n",
      "Epoch 908/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7945\n",
      "Epoch 909/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8137 - val_loss: 0.4593 - val_acc: 0.7934\n",
      "Epoch 910/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4290 - acc: 0.8138 - val_loss: 0.4592 - val_acc: 0.7919\n",
      "Epoch 911/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8130 - val_loss: 0.4596 - val_acc: 0.7961\n",
      "Epoch 912/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4593 - val_acc: 0.7930\n",
      "Epoch 913/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8127 - val_loss: 0.4598 - val_acc: 0.7939\n",
      "Epoch 914/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4288 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 915/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4604 - val_acc: 0.7939\n",
      "Epoch 916/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4595 - val_acc: 0.7914\n",
      "Epoch 917/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4287 - acc: 0.8136 - val_loss: 0.4597 - val_acc: 0.7923\n",
      "Epoch 918/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4290 - acc: 0.8142 - val_loss: 0.4599 - val_acc: 0.7934\n",
      "Epoch 919/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8124 - val_loss: 0.4597 - val_acc: 0.7910\n",
      "Epoch 920/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8127 - val_loss: 0.4594 - val_acc: 0.7921\n",
      "Epoch 921/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 922/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 923/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4289 - acc: 0.8133 - val_loss: 0.4601 - val_acc: 0.7932\n",
      "Epoch 924/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 925/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4287 - acc: 0.8135 - val_loss: 0.4602 - val_acc: 0.7930\n",
      "Epoch 926/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4288 - acc: 0.8124 - val_loss: 0.4598 - val_acc: 0.7936\n",
      "Epoch 927/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8127 - val_loss: 0.4596 - val_acc: 0.7925\n",
      "Epoch 928/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8138 - val_loss: 0.4596 - val_acc: 0.7954\n",
      "Epoch 929/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4287 - acc: 0.8133 - val_loss: 0.4612 - val_acc: 0.7939\n",
      "Epoch 930/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8125 - val_loss: 0.4595 - val_acc: 0.7945\n",
      "Epoch 931/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4289 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 932/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4593 - val_acc: 0.7930\n",
      "Epoch 933/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4602 - val_acc: 0.7936\n",
      "Epoch 934/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8120 - val_loss: 0.4593 - val_acc: 0.7930\n",
      "Epoch 935/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8128 - val_loss: 0.4593 - val_acc: 0.7925\n",
      "Epoch 936/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4290 - acc: 0.8132 - val_loss: 0.4594 - val_acc: 0.7928\n",
      "Epoch 937/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4289 - acc: 0.8125 - val_loss: 0.4594 - val_acc: 0.7928\n",
      "Epoch 938/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4287 - acc: 0.8131 - val_loss: 0.4596 - val_acc: 0.7921\n",
      "Epoch 939/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8125 - val_loss: 0.4593 - val_acc: 0.7943\n",
      "Epoch 940/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4595 - val_acc: 0.7941\n",
      "Epoch 941/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8137 - val_loss: 0.4601 - val_acc: 0.7930\n",
      "Epoch 942/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4598 - val_acc: 0.7943\n",
      "Epoch 943/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7956\n",
      "Epoch 944/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4288 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 945/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4286 - acc: 0.8119 - val_loss: 0.4597 - val_acc: 0.7930\n",
      "Epoch 946/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4289 - acc: 0.8140 - val_loss: 0.4593 - val_acc: 0.7934\n",
      "Epoch 947/1000\n",
      "13764/13764 [==============================] - 1s 48us/step - loss: 0.4287 - acc: 0.8140 - val_loss: 0.4594 - val_acc: 0.7921\n",
      "Epoch 948/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4595 - val_acc: 0.7936\n",
      "Epoch 949/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4289 - acc: 0.8118 - val_loss: 0.4601 - val_acc: 0.7948\n",
      "Epoch 950/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7923\n",
      "Epoch 951/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4289 - acc: 0.8138 - val_loss: 0.4612 - val_acc: 0.7930\n",
      "Epoch 952/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4289 - acc: 0.8124 - val_loss: 0.4594 - val_acc: 0.7932\n",
      "Epoch 953/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8125 - val_loss: 0.4594 - val_acc: 0.7943\n",
      "Epoch 954/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4596 - val_acc: 0.7923\n",
      "Epoch 955/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8133 - val_loss: 0.4594 - val_acc: 0.7921\n",
      "Epoch 956/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8122 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 957/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4287 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7925\n",
      "Epoch 958/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4289 - acc: 0.8136 - val_loss: 0.4597 - val_acc: 0.7948\n",
      "Epoch 959/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.4596 - val_acc: 0.7934\n",
      "Epoch 960/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4292 - acc: 0.8136 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 961/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4287 - acc: 0.8141 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 962/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8140 - val_loss: 0.4597 - val_acc: 0.7936\n",
      "Epoch 963/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4289 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 964/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4290 - acc: 0.8130 - val_loss: 0.4601 - val_acc: 0.7919\n",
      "Epoch 965/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4288 - acc: 0.8127 - val_loss: 0.4593 - val_acc: 0.7925\n",
      "Epoch 966/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4287 - acc: 0.8126 - val_loss: 0.4604 - val_acc: 0.7936\n",
      "Epoch 967/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4286 - acc: 0.8135 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 968/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7934\n",
      "Epoch 969/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8139 - val_loss: 0.4602 - val_acc: 0.7939\n",
      "Epoch 970/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8135 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 971/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4291 - acc: 0.8138 - val_loss: 0.4602 - val_acc: 0.7923\n",
      "Epoch 972/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4288 - acc: 0.8135 - val_loss: 0.4602 - val_acc: 0.7943\n",
      "Epoch 973/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4288 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7943\n",
      "Epoch 974/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4610 - val_acc: 0.7936\n",
      "Epoch 975/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8136 - val_loss: 0.4593 - val_acc: 0.7928\n",
      "Epoch 976/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4288 - acc: 0.8120 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 977/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8134 - val_loss: 0.4594 - val_acc: 0.7928\n",
      "Epoch 978/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4291 - acc: 0.8119 - val_loss: 0.4599 - val_acc: 0.7943\n",
      "Epoch 979/1000\n",
      "13764/13764 [==============================] - 1s 53us/step - loss: 0.4290 - acc: 0.8125 - val_loss: 0.4596 - val_acc: 0.7930\n",
      "Epoch 980/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8141 - val_loss: 0.4593 - val_acc: 0.7930\n",
      "Epoch 981/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8129 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 982/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8135 - val_loss: 0.4594 - val_acc: 0.7932\n",
      "Epoch 983/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4595 - val_acc: 0.7921\n",
      "Epoch 984/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8142 - val_loss: 0.4597 - val_acc: 0.7932\n",
      "Epoch 985/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4287 - acc: 0.8130 - val_loss: 0.4607 - val_acc: 0.7930\n",
      "Epoch 986/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4286 - acc: 0.8146 - val_loss: 0.4593 - val_acc: 0.7923\n",
      "Epoch 987/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4592 - val_acc: 0.7941\n",
      "Epoch 988/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4287 - acc: 0.8128 - val_loss: 0.4605 - val_acc: 0.7945\n",
      "Epoch 989/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8131 - val_loss: 0.4595 - val_acc: 0.7943\n",
      "Epoch 990/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8125 - val_loss: 0.4603 - val_acc: 0.7945\n",
      "Epoch 991/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4595 - val_acc: 0.7921\n",
      "Epoch 992/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4288 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7939\n",
      "Epoch 993/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8134 - val_loss: 0.4597 - val_acc: 0.7941\n",
      "Epoch 994/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4287 - acc: 0.8139 - val_loss: 0.4592 - val_acc: 0.7930\n",
      "Epoch 995/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8136 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 996/1000\n",
      "13764/13764 [==============================] - 1s 49us/step - loss: 0.4288 - acc: 0.8137 - val_loss: 0.4593 - val_acc: 0.7945\n",
      "Epoch 997/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4288 - acc: 0.8139 - val_loss: 0.4595 - val_acc: 0.7928\n",
      "Epoch 998/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8141 - val_loss: 0.4595 - val_acc: 0.7930\n",
      "Epoch 999/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7923\n",
      "Epoch 1000/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8136 - val_loss: 0.4594 - val_acc: 0.7928\n"
     ]
    }
   ],
   "source": [
    "h = model_TP.fit(train_X_TP[:], train_Y_TP[:], \n",
    "          epochs=1000, \n",
    "          #batch_size=32, \n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          #validation_split=0.1)\n",
    "          validation_data=([test_X_TP, test_Y_TP]))\n",
    "\n",
    "history_TP = {k : history_TP[k] + h.history[k] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d27abcf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('URZ_model_15-6-2_norm_TP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4502/4502 [==============================] - 0s 23us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4593694880770239, 0.7927587738253179]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TP.evaluate(test_X_TP, test_Y_TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1809  439]\n",
      " [ 494 1760]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = numpy.reshape(numpy.argmax(model_TP.predict(test_X_TP), axis=1), (test_X_TP.shape[0],1))\n",
    "\n",
    "# calculate confusion matrix\n",
    "conf_mat = confusion_matrix(test_Y_TP_, Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cascade of all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NTPS = load_model('URZ_model_15-6-2_norm_NTPS.h5')\n",
    "model_STP = load_model('URZ_model_15-6-2_norm_TPS.h5')\n",
    "model_PT = load_model('URZ_model_15-6-2_norm_TP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iwt(X, stage=0):\n",
    "    \"\"\"\n",
    "    predicts initial wave type for given featrue vectors\n",
    "    Class encoding generated by sklearn Label Encoder\n",
    "    0 - noise\n",
    "    2 - regS \n",
    "    1 - regP\n",
    "    3 - T\n",
    "    \"\"\"\n",
    "    Y = numpy.arange(X.shape[0])\n",
    "    \n",
    "    N_indices = None\n",
    "    S_indices = None \n",
    "    \n",
    "    if stage >= 1:\n",
    "        N_indices = [False] * X.shape[0]\n",
    "        X_PTS = X\n",
    "        Y_NPTS = numpy.ones(X.shape[0])\n",
    "    if stage >= 2:\n",
    "        S_indices =  [False] * X.shape[0]  # numpy.array([])\n",
    "        X_PT = X\n",
    "        Y_PTS = numpy.ones(X.shape[0])\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    N vs regS, regP, T\n",
    "    \"\"\"\n",
    "    if N_indices is None:\n",
    "        #predict N vs T,regP,regS\n",
    "        Y_NPTS = numpy.argmax(model_NTPS.predict(X), axis=1)\n",
    "        #set which are noise\n",
    "        N_indices = Y[Y_NPTS == 0]  #  = 0\n",
    "        #get candidates for TPS\n",
    "        X_PTS = X[Y_NPTS > 0]\n",
    "    else:\n",
    "        print('Skipping N, classifying TPS only')\n",
    "\n",
    "    \"\"\"\n",
    "    regS vs regP, T\n",
    "    \"\"\"\n",
    "    if S_indices is None:    \n",
    "        #predict regS vs T,regP\n",
    "        Y_PTS = numpy.argmax(model_TPS.predict(X_PTS), axis=1)\n",
    "        #set which are regS\n",
    "        S_indices = Y[Y_NPTS > 0][Y_PTS == 0]  # = 2\n",
    "        #get candidates for regP,T\n",
    "        X_PT = X_PTS[Y_PTS > 0]\n",
    "    else:\n",
    "        print('Skipping N, regS, classifying TP only')\n",
    "\n",
    "    \"\"\"\n",
    "    regP vs T\n",
    "    \"\"\"\n",
    "    #predict regP vs T\n",
    "    Y_PT = numpy.argmax(model_TP.predict(X_PT), axis=1)\n",
    "    #set which are regP\n",
    "    P_indices = Y[Y_NPTS > 0][Y_PTS > 0][Y_PT == 0]  # = 1    \n",
    "    #set which are T\n",
    "    T_indices = Y[Y_NPTS > 0][Y_PTS > 0][Y_PT > 0]   # = 3\n",
    "    #get those which are T\n",
    "    X_T = X_PT[Y_PT > 0]\n",
    "    \n",
    "    Y[N_indices] = 0 # N\n",
    "    Y[S_indices] = 2 # regS\n",
    "    Y[P_indices] = 1 # regP \n",
    "    Y[T_indices] = 3 # tele\n",
    "    \n",
    "    return Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = predict_iwt(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall confusion matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13700, 15), (13700,), (13700,))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, Y.shape, test_Y_GT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5757  253  570  269]\n",
      " [ 304 1626   18  436]\n",
      " [ 624    9 1685  382]\n",
      " [ 165  360   75 1167]]\n"
     ]
    }
   ],
   "source": [
    "C = confusion_matrix(Y, test_Y_GT)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.71%\n"
     ]
    }
   ],
   "source": [
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's produce result data frame from metadata and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13700\n"
     ]
    }
   ],
   "source": [
    "class_dict = {0:'N', 1:'regP', 2:'regS', 3:'tele'}\n",
    "Y_pred_cat = [class_dict[yi] for yi in Y]\n",
    "print(len(Y_pred_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata = test[metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add new column to the pandas dataframe\n",
    "#test_metadata['CLASS_IPHASE_NEW'] = Y_pred_cat  # deprecated - gives warning\n",
    "test_metadata.assign(CLASS_IPHASE_NEW = pd.Series(Y_pred_cat, index=test_metadata.index)).to_csv('test_set_prediction.txt', na_rep='null',columns=metadata+[\"CLASS_IPHASE_NEW\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data just regS, regP, T - not to be confused with overall statistics on all arrivals in our DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_TPS = test[test['CLASS_PHASE'] != 'N']\n",
    "test_TPS_X = test_TPS[x_indices]\n",
    "test_TPS_pred_Y = predict_iwt(test_TPS_X)\n",
    "test_TPS_truth_Y = le.transform(test_TPS['CLASS_PHASE'])\n",
    "test_TPS_iphase_Y = le.transform(test_TPS['CLASS_IPHASE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_TPS_iphase_Y.min(), test_TPS_iphase_Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPHASE:\n",
      "[[   0  337  829  380]\n",
      " [   0 1655  170  540]\n",
      " [   0   45 1312  351]\n",
      " [   0  211   37  983]]\n",
      "Accuracy: 57.66%\n"
     ]
    }
   ],
   "source": [
    "print('IPHASE:')\n",
    "C = confusion_matrix(test_TPS_iphase_Y, test_TPS_truth_Y)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW PREDICTION\n",
      "[[   0  232  609  265]\n",
      " [   0 1648   18  445]\n",
      " [   0    8 1645  377]\n",
      " [   0  360   76 1167]]\n",
      "Accuracy: 65.11%\n"
     ]
    }
   ],
   "source": [
    "print('NEW PREDICTION')\n",
    "C = confusion_matrix(test_TPS_pred_Y, test_TPS_truth_Y)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in the test data, the current iwt characterized 1501 as NOISE (first row of confusion matrix)\n",
    "* our new iwt would characterize 1089 as NOISE\n",
    "* after re-training, accuracy on the test set rose from 58.68% to 65.82% for regS, regP and T phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ..on train data just to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17308   698  1622   834]\n",
      " [  963  5167    22  1221]\n",
      " [ 1753    24  4922  1174]\n",
      " [  525   996   219  3650]]\n",
      "Accuracy: 75.54%\n"
     ]
    }
   ],
   "source": [
    "Y = predict_iwt(train_X)\n",
    "C = confusion_matrix(Y, train_Y_GT)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall confusion matrix for all manual associations (no Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping N, classifying TPS only\n",
      "[[1317   77  387]\n",
      " [ 211 1811  717]\n",
      " [ 474  114  898]]\n"
     ]
    }
   ],
   "source": [
    "C = confusion_matrix(predict_iwt(manual_X, stage=1), manual_Y_GT)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.03%\n"
     ]
    }
   ],
   "source": [
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try all noise samples we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get all noise phases not used for training\n",
    "N_data_diff = pd.concat([df_N_all, N_data]).loc[\n",
    "    df_N_all.index.symmetric_difference(N_data.index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273972, 25) (301371, 25) (27399, 25)\n",
      "273972 should equal 273972\n"
     ]
    }
   ],
   "source": [
    "print(N_data_diff.shape, df_N_all.shape, N_data.shape)\n",
    "print(N_data_diff.shape[0], 'should equal', df_N_all.shape[0]-N_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_data_norm = N_data_diff[x_indices].copy(deep=True)\n",
    "N_data_norm['INANG1'] /= 90.\n",
    "N_data_norm['INANG3'] /= 90.\n",
    "N_data_norm['HMXMN'] = numpy.log10(df_N_all['HMXMN'])\n",
    "N_data_norm['HVRATP'] = numpy.log10(df_N_all['HVRATP'])\n",
    "N_data_norm['HVRAT'] = numpy.log10(df_N_all['HVRAT'])\n",
    "N_data_norm['HTOV1'] = numpy.log10(df_N_all['HTOV1'])\n",
    "N_data_norm['HTOV2'] = numpy.log10(df_N_all['HTOV2'])\n",
    "N_data_norm['HTOV3'] = numpy.log10(df_N_all['HTOV3'])\n",
    "N_data_norm['HTOV4'] = numpy.log10(df_N_all['HTOV4'])\n",
    "N_data_norm['HTOV5'] = numpy.log10(df_N_all['HTOV5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273972, 15)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_data_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273972, 15) (273972,)\n"
     ]
    }
   ],
   "source": [
    "N_X = N_data_norm[x_indices].values.astype(float)\n",
    "N_Y = numpy.zeros(N_X.shape[0])\n",
    "\n",
    "print(N_X.shape, N_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[229214      0      0      0]\n",
      " [ 13555      0      0      0]\n",
      " [ 23749      0      0      0]\n",
      " [  7454      0      0      0]]\n",
      "Accuracy: 83.66%\n"
     ]
    }
   ],
   "source": [
    "Y = predict_iwt(N_X)\n",
    "C = confusion_matrix(Y, N_Y)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From all avaibale noise phases which were not used for training we are able to correctly identify 83.8%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
