{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN cascade for URZ correct nodes\n",
    "### With normalization of Features as in iwt_nnet.c : iwt_normalize()\n",
    "\n",
    "* Radek Hofman, Jan 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and axiliary functions and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('/','home','hofman','.dbp.txt'), 'r') as f: password = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: hofman@udb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"oracle://hofman:%s@mycelium.ctbto.org:1521/udb\" % password\n",
    "%sql $query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>362407</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(362407,)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% sql select count(*) from ml_features where sta='URZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, semilog=False):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.plot(history['acc'], label='acc')\n",
    "    ax.plot(history['val_acc'], label='val_acc')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    if semilog:\n",
    "        ax.set_yscale('log')\n",
    "    plt.legend(loc='best')\n",
    "    ax = fig.add_subplot(212)\n",
    "    ax.plot(history['loss'], label='loss')\n",
    "    ax.plot(history['val_loss'], label='val_loss')\n",
    "    plt.legend(loc='best')\n",
    "    if semilog:\n",
    "        ax.set_yscale('log')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from oracle do pandas\n",
    "import cx_Oracle\n",
    "connection = cx_Oracle.connect('hofman', password, 'udb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the misclassification rate now for all arrivals in our DB?\n",
    "\n",
    "* #(class_iphase != class_phase) / (#automatic which are not noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows affected.\n",
      "0 rows affected.\n"
     ]
    }
   ],
   "source": [
    "#select from database required numbers\n",
    "wrong_type = %sql select count(*) from ml_features where sta='URZ' and class_phase != class_iphase\n",
    "total_number = %sql select count(*) from ml_features where sta='URZ' and phase!='N' and source!='M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of misclassified initial wave types: 47.90%\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of misclassified initial wave types: %3.2f%%' % (wrong_type[0][0]/total_number[0][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes per class phase type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='regS'\"\"\"\n",
    "df_S_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25050735</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125386e+09</td>\n",
       "      <td>Sx</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.526004</td>\n",
       "      <td>0.786459</td>\n",
       "      <td>1.174982</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>1.216724</td>\n",
       "      <td>0.363039</td>\n",
       "      <td>2.166968</td>\n",
       "      <td>1.844924</td>\n",
       "      <td>0.626604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25058004</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125423e+09</td>\n",
       "      <td>Lg</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.925</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.762988</td>\n",
       "      <td>15.337161</td>\n",
       "      <td>15.337161</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.423843</td>\n",
       "      <td>1.384683</td>\n",
       "      <td>0.724611</td>\n",
       "      <td>6.547078</td>\n",
       "      <td>1.569841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25075768</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125508e+09</td>\n",
       "      <td>Lg</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.050</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.395858</td>\n",
       "      <td>1.296030</td>\n",
       "      <td>2.398118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.713705</td>\n",
       "      <td>1.641215</td>\n",
       "      <td>0.940403</td>\n",
       "      <td>1.211333</td>\n",
       "      <td>1.909713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25092371</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125587e+09</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>0.175</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.746398</td>\n",
       "      <td>5.561389</td>\n",
       "      <td>2.205452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.155050</td>\n",
       "      <td>0.646231</td>\n",
       "      <td>0.997092</td>\n",
       "      <td>1.326985</td>\n",
       "      <td>4.507104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25094351</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.125598e+09</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>Sn</td>\n",
       "      <td>regS</td>\n",
       "      <td>1.400</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.816413</td>\n",
       "      <td>5.135686</td>\n",
       "      <td>8.557762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.682028</td>\n",
       "      <td>0.309588</td>\n",
       "      <td>2.783780</td>\n",
       "      <td>8.199625</td>\n",
       "      <td>2.007485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE  RETIME  \\\n",
       "0  25050735  URZ  1.125386e+09     Sx         regS    Sn        regS   1.000   \n",
       "1  25058004  URZ  1.125423e+09     Lg         regS    Sn        regS   1.925   \n",
       "2  25075768  URZ  1.125508e+09     Lg         regS    Sn        regS   1.050   \n",
       "3  25092371  URZ  1.125587e+09     Sn         regS    Sn        regS   0.175   \n",
       "4  25094351  URZ  1.125598e+09     Sn         regS    Sn        regS   1.400   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN     HVRATP      HVRAT  NAB   TAB  \\\n",
       "0      A  0.166667    ...     1.526004   0.786459   1.174982 -0.1 -0.27   \n",
       "1      A  0.444444    ...     2.762988  15.337161  15.337161 -0.1 -0.31   \n",
       "2      A  0.333333    ...     1.395858   1.296030   2.398118  0.0  0.00   \n",
       "3      A  0.166667    ...     1.746398   5.561389   2.205452  0.0  0.00   \n",
       "4      A  0.444444    ...     2.816413   5.135686   8.557762  0.0  0.00   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  1.216724  0.363039  2.166968  1.844924  0.626604  \n",
       "1  0.423843  1.384683  0.724611  6.547078  1.569841  \n",
       "2  0.713705  1.641215  0.940403  1.211333  1.909713  \n",
       "3  1.155050  0.646231  0.997092  1.326985  4.507104  \n",
       "4  1.682028  0.309588  2.783780  8.199625  2.007485  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_S_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='regP'\"\"\"\n",
    "df_P_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14972252</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.069735e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>2.94244</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.542596</td>\n",
       "      <td>0.124299</td>\n",
       "      <td>0.176237</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.509125</td>\n",
       "      <td>0.983261</td>\n",
       "      <td>0.619635</td>\n",
       "      <td>0.118464</td>\n",
       "      <td>0.058735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14992929</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070059e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301846</td>\n",
       "      <td>1.112025</td>\n",
       "      <td>0.258914</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>3.730434</td>\n",
       "      <td>0.738458</td>\n",
       "      <td>0.704325</td>\n",
       "      <td>0.247701</td>\n",
       "      <td>0.060989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15029724</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070386e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>0.65000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.828008</td>\n",
       "      <td>0.132138</td>\n",
       "      <td>0.192003</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.629496</td>\n",
       "      <td>0.632947</td>\n",
       "      <td>0.277982</td>\n",
       "      <td>0.542543</td>\n",
       "      <td>0.041916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15031571</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070403e+09</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>3.22500</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>1.375773</td>\n",
       "      <td>0.282216</td>\n",
       "      <td>0.589090</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.924411</td>\n",
       "      <td>0.941318</td>\n",
       "      <td>0.304547</td>\n",
       "      <td>0.520954</td>\n",
       "      <td>0.131296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15073977</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.070752e+09</td>\n",
       "      <td>Pg</td>\n",
       "      <td>regP</td>\n",
       "      <td>Pn</td>\n",
       "      <td>regP</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>3.529159</td>\n",
       "      <td>0.085323</td>\n",
       "      <td>0.486815</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.211185</td>\n",
       "      <td>0.225393</td>\n",
       "      <td>0.257161</td>\n",
       "      <td>0.413598</td>\n",
       "      <td>0.287086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE   RETIME  \\\n",
       "0  14972252  URZ  1.069735e+09     Pn         regP    Pn        regP  2.94244   \n",
       "1  14992929  URZ  1.070059e+09     Pn         regP    Pn        regP  0.00000   \n",
       "2  15029724  URZ  1.070386e+09     Pn         regP    Pn        regP  0.65000   \n",
       "3  15031571  URZ  1.070403e+09     Pn         regP    Pn        regP  3.22500   \n",
       "4  15073977  URZ  1.070752e+09     Pg         regP    Pn        regP  0.00000   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN    HVRATP     HVRAT  NAB    TAB  \\\n",
       "0      A  0.166667    ...     1.542596  0.124299  0.176237  0.1  0.100   \n",
       "1      A  0.333333    ...     1.301846  1.112025  0.258914  0.2  0.245   \n",
       "2      A  0.166667    ...     1.828008  0.132138  0.192003  0.2  0.375   \n",
       "3      A  0.444444    ...     1.375773  0.282216  0.589090  0.1  0.050   \n",
       "4      A  0.166667    ...     3.529159  0.085323  0.486815  0.1  0.290   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  0.509125  0.983261  0.619635  0.118464  0.058735  \n",
       "1  3.730434  0.738458  0.704325  0.247701  0.060989  \n",
       "2  0.629496  0.632947  0.277982  0.542543  0.041916  \n",
       "3  0.924411  0.941318  0.304547  0.520954  0.131296  \n",
       "4  0.211185  0.225393  0.257161  0.413598  0.287086  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='tele'\"\"\"\n",
    "df_T_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28541585</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143123e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.550</td>\n",
       "      <td>A</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.702892</td>\n",
       "      <td>0.042081</td>\n",
       "      <td>0.042081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.467387</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.057473</td>\n",
       "      <td>0.053682</td>\n",
       "      <td>0.463873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28556291</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143197e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.500</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.957241</td>\n",
       "      <td>0.223522</td>\n",
       "      <td>0.223522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.954016</td>\n",
       "      <td>0.705002</td>\n",
       "      <td>1.308694</td>\n",
       "      <td>0.193754</td>\n",
       "      <td>0.114283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28557837</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143204e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>3.100</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190828</td>\n",
       "      <td>0.240041</td>\n",
       "      <td>0.613747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.577677</td>\n",
       "      <td>1.020239</td>\n",
       "      <td>0.372745</td>\n",
       "      <td>0.277075</td>\n",
       "      <td>0.067225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28559193</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143210e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.000</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531207</td>\n",
       "      <td>0.084376</td>\n",
       "      <td>0.084376</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.689514</td>\n",
       "      <td>0.333741</td>\n",
       "      <td>0.336678</td>\n",
       "      <td>0.119060</td>\n",
       "      <td>0.092935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28561877</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.143223e+09</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>P</td>\n",
       "      <td>tele</td>\n",
       "      <td>0.375</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.808070</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.097918</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.358785</td>\n",
       "      <td>0.042658</td>\n",
       "      <td>0.127151</td>\n",
       "      <td>0.064076</td>\n",
       "      <td>0.062188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE  RETIME  \\\n",
       "0  28541585  URZ  1.143123e+09      P         tele     P        tele   0.550   \n",
       "1  28556291  URZ  1.143197e+09      P         tele     P        tele   0.500   \n",
       "2  28557837  URZ  1.143204e+09      P         tele     P        tele   3.100   \n",
       "3  28559193  URZ  1.143210e+09      P         tele     P        tele   0.000   \n",
       "4  28561877  URZ  1.143223e+09      P         tele     P        tele   0.375   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN    HVRATP     HVRAT  NAB    TAB  \\\n",
       "0      A  0.666667    ...     1.702892  0.042081  0.042081  0.0  0.000   \n",
       "1      A  0.333333    ...     1.957241  0.223522  0.223522  0.0  0.000   \n",
       "2      A  0.333333    ...     1.190828  0.240041  0.613747  0.0 -0.195   \n",
       "3      A  0.333333    ...     1.531207  0.084376  0.084376  0.1  0.130   \n",
       "4      A  0.444444    ...     2.808070  0.038871  0.097918  0.1  0.205   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  0.467387  0.594238  0.057473  0.053682  0.463873  \n",
       "1  0.954016  0.705002  1.308694  0.193754  0.114283  \n",
       "2  0.577677  1.020239  0.372745  0.277075  0.067225  \n",
       "3  0.689514  0.333741  0.336678  0.119060  0.092935  \n",
       "4  0.358785  0.042658  0.127151  0.064076  0.062188  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_T_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from ml_features where sta='URZ' and class_phase='N'\"\"\"\n",
    "df_N_all = pd.read_sql(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARID</th>\n",
       "      <th>STA</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IPHASE</th>\n",
       "      <th>CLASS_IPHASE</th>\n",
       "      <th>PHASE</th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>RETIME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13097443</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055511e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.948945</td>\n",
       "      <td>9.600216</td>\n",
       "      <td>9.600216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.396880</td>\n",
       "      <td>1.145887</td>\n",
       "      <td>0.421142</td>\n",
       "      <td>0.406116</td>\n",
       "      <td>1.439137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13097727</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055513e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>5.750848</td>\n",
       "      <td>9.726424</td>\n",
       "      <td>3.211865</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.628241</td>\n",
       "      <td>0.617270</td>\n",
       "      <td>0.890586</td>\n",
       "      <td>2.788352</td>\n",
       "      <td>1.279634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13097728</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055513e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.550819</td>\n",
       "      <td>0.163623</td>\n",
       "      <td>12.531935</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.625565</td>\n",
       "      <td>0.419386</td>\n",
       "      <td>0.502452</td>\n",
       "      <td>1.093746</td>\n",
       "      <td>0.228218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13097729</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055513e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2.203439</td>\n",
       "      <td>0.328290</td>\n",
       "      <td>0.511023</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.532591</td>\n",
       "      <td>1.455946</td>\n",
       "      <td>0.672186</td>\n",
       "      <td>0.730198</td>\n",
       "      <td>0.130826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13097946</td>\n",
       "      <td>URZ</td>\n",
       "      <td>1.055517e+09</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.803004</td>\n",
       "      <td>0.827978</td>\n",
       "      <td>0.630203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.039954</td>\n",
       "      <td>0.420590</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>0.450562</td>\n",
       "      <td>0.274970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARID  STA          TIME IPHASE CLASS_IPHASE PHASE CLASS_PHASE  RETIME  \\\n",
       "0  13097443  URZ  1.055511e+09      N            N  None           N     0.0   \n",
       "1  13097727  URZ  1.055513e+09      N            N  None           N     0.0   \n",
       "2  13097728  URZ  1.055513e+09      N            N  None           N     0.0   \n",
       "3  13097729  URZ  1.055513e+09      N            N  None           N     0.0   \n",
       "4  13097946  URZ  1.055517e+09      N            N  None           N     0.0   \n",
       "\n",
       "  SOURCE       PER    ...        HMXMN    HVRATP      HVRAT  NAB   TAB  \\\n",
       "0      A  0.166667    ...     2.948945  9.600216   9.600216  0.0  0.00   \n",
       "1      A  0.444444    ...     5.750848  9.726424   3.211865 -0.1 -0.04   \n",
       "2      A  1.000000    ...     1.550819  0.163623  12.531935  0.1  0.50   \n",
       "3      A  0.333333    ...     2.203439  0.328290   0.511023 -0.1 -0.50   \n",
       "4      A  1.000000    ...     2.803004  0.827978   0.630203  0.0  0.00   \n",
       "\n",
       "      HTOV1     HTOV2     HTOV3     HTOV4     HTOV5  \n",
       "0  0.396880  1.145887  0.421142  0.406116  1.439137  \n",
       "1  0.628241  0.617270  0.890586  2.788352  1.279634  \n",
       "2  2.625565  0.419386  0.502452  1.093746  0.228218  \n",
       "3  0.532591  1.455946  0.672186  0.730198  0.130826  \n",
       "4  1.039954  0.420590  0.983108  0.450562  0.274970  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_N_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "#df_ora.to_csv('URZ_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regS (11135, 25)\n",
      "regP (11818, 25)\n",
      "tele (38083, 25)\n",
      "noise (301371, 25)\n"
     ]
    }
   ],
   "source": [
    "#how much data we have\n",
    "print('regS', df_S_all.shape)\n",
    "print('regP', df_P_all.shape)\n",
    "print('tele', df_T_all.shape)\n",
    "print('noise', df_N_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features count: 15\n"
     ]
    }
   ],
   "source": [
    "# columns corresponding to input\n",
    "x_indices = ['PER', 'RECT', 'PLANS', 'INANG1', 'INANG3', 'HMXMN', 'HVRATP', 'HVRAT', 'NAB', 'TAB',  \n",
    "             'HTOV1', 'HTOV2', 'HTOV3', 'HTOV4', 'HTOV5']\n",
    "print('features count:', len(x_indices))\n",
    "# columns corresponding to output\n",
    "y_indices = ['CLASS_PHASE']\n",
    "\n",
    "metadata = ['ARID','STA','TIME','IPHASE','CLASS_IPHASE','PHASE','CLASS_PHASE','RETIME','SOURCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for first phase of the cascade: N vs TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9133, 25) (9133, 25) (9133, 25)\n",
      "(27399, 25)\n",
      "(27399, 25)\n"
     ]
    }
   ],
   "source": [
    "#counts of all classes\n",
    "ns = df_S_all.shape[0]\n",
    "np = df_P_all.shape[0]\n",
    "nt = df_T_all.shape[0]\n",
    "nn = df_N_all.shape[0]\n",
    "\n",
    "#those from automatic\n",
    "nsa = df_S_all[df_S_all['SOURCE'] != 'M'].shape[0]\n",
    "npa = df_P_all[df_P_all['SOURCE'] != 'M'].shape[0]\n",
    "nta = df_T_all[df_T_all['SOURCE'] != 'M'].shape[0]\n",
    "nna = df_N_all[df_N_all['SOURCE'] != 'M'].shape[0]\n",
    "\n",
    "\n",
    "#we build a balanced datased - the same portion of regS, regP and tele\n",
    "#we have this count of phases\n",
    "samp_count = min(nsa, npa, nta)\n",
    "\n",
    "#sample TPS dataset, random_state is a seed\n",
    "ssS = df_S_all[df_S_all['SOURCE'] != 'M'].sample(samp_count, random_state=11)\n",
    "ssP = df_P_all[df_P_all['SOURCE'] != 'M'].sample(samp_count, random_state=13)\n",
    "ssT = df_T_all[df_T_all['SOURCE'] != 'M'].sample(samp_count, random_state=17)\n",
    "TPS_data = pd.concat([ssS, ssP, ssT])\n",
    "\n",
    "#sample noise phases\n",
    "N_data = df_N_all[df_N_all['SOURCE'] != 'M'].sample(3*samp_count, random_state=23)\n",
    "\n",
    "#lets shuffle dataset\n",
    "TPS_data = TPS_data.sample(frac=1, random_state=51).reset_index(drop=True)\n",
    "N_data = N_data.sample(frac=1, random_state=101).reset_index(drop=True)\n",
    "\n",
    "print(ssS.shape, ssP.shape, ssT.shape)\n",
    "print(TPS_data.shape)\n",
    "print(N_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of data\n",
    "\n",
    "* normalization according to iwt_nnet.c : iwt_normalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPS\n",
      "Series([], Name: HVRAT, dtype: float64)\n",
      "Series([], Name: HVRATP, dtype: float64)\n",
      "Series([], Name: HMXMN, dtype: float64)\n",
      "Series([], Name: HTOV1, dtype: float64)\n",
      "Series([], Name: HTOV2, dtype: float64)\n",
      "Series([], Name: HTOV3, dtype: float64)\n",
      "Series([], Name: HTOV4, dtype: float64)\n",
      "Series([], Name: HTOV5, dtype: float64)\n",
      "NOISE\n",
      "Series([], Name: HVRAT, dtype: float64)\n",
      "Series([], Name: HVRATP, dtype: float64)\n",
      "Series([], Name: HMXMN, dtype: float64)\n",
      "Series([], Name: HTOV1, dtype: float64)\n",
      "Series([], Name: HTOV2, dtype: float64)\n",
      "Series([], Name: HTOV3, dtype: float64)\n",
      "Series([], Name: HTOV4, dtype: float64)\n",
      "Series([], Name: HTOV5, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# check on positivity of all features which are going to be log10-ed:)\n",
    "# check TPS data\n",
    "print('TPS')\n",
    "print(TPS_data['HVRAT'][TPS_data['HVRAT']<=0])\n",
    "print(TPS_data['HVRATP'][TPS_data['HVRATP']<=0])\n",
    "print(TPS_data['HMXMN'][TPS_data['HMXMN']<=0])\n",
    "print(TPS_data['HTOV1'][TPS_data['HTOV1']<=0])\n",
    "print(TPS_data['HTOV2'][TPS_data['HTOV2']<=0])\n",
    "print(TPS_data['HTOV3'][TPS_data['HTOV3']<=0])\n",
    "print(TPS_data['HTOV4'][TPS_data['HTOV4']<=0])\n",
    "print(TPS_data['HTOV5'][TPS_data['HTOV5']<=0])\n",
    "# check NOISE data\n",
    "print('NOISE')\n",
    "print(N_data['HVRAT'][N_data['HVRAT']<=0])\n",
    "print(N_data['HVRATP'][N_data['HVRATP']<=0])\n",
    "print(N_data['HMXMN'][N_data['HMXMN']<=0])\n",
    "print(N_data['HTOV1'][N_data['HTOV1']<=0])\n",
    "print(N_data['HTOV2'][N_data['HTOV2']<=0])\n",
    "print(N_data['HTOV3'][N_data['HTOV3']<=0])\n",
    "print(N_data['HTOV4'][N_data['HTOV4']<=0])\n",
    "print(N_data['HTOV5'][N_data['HTOV5']<=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize TPS\n",
    "TPS_data_norm = TPS_data.copy(deep=True)\n",
    "TPS_data_norm['INANG1'] /= 90.\n",
    "TPS_data_norm['INANG3'] /= 90.\n",
    "TPS_data_norm['HMXMN'] = numpy.log10(TPS_data['HMXMN'])\n",
    "TPS_data_norm['HVRATP'] = numpy.log10(TPS_data['HVRATP'])\n",
    "TPS_data_norm['HVRAT'] = numpy.log10(TPS_data['HVRAT'])\n",
    "TPS_data_norm['HTOV1'] = numpy.log10(TPS_data['HTOV1'])\n",
    "TPS_data_norm['HTOV2'] = numpy.log10(TPS_data['HTOV2'])\n",
    "TPS_data_norm['HTOV3'] = numpy.log10(TPS_data['HTOV3'])\n",
    "TPS_data_norm['HTOV4'] = numpy.log10(TPS_data['HTOV4'])\n",
    "TPS_data_norm['HTOV5'] = numpy.log10(TPS_data['HTOV5'])\n",
    "\n",
    "# normalize NOISE\n",
    "N_data_norm = N_data.copy(deep=True)\n",
    "N_data_norm['INANG1'] /= 90.\n",
    "N_data_norm['INANG3'] /= 90.\n",
    "N_data_norm['HMXMN'] = numpy.log10(N_data['HMXMN'])\n",
    "N_data_norm['HVRATP'] = numpy.log10(N_data['HVRATP'])\n",
    "N_data_norm['HVRAT'] = numpy.log10(N_data['HVRAT'])\n",
    "N_data_norm['HTOV1'] = numpy.log10(N_data['HTOV1'])\n",
    "N_data_norm['HTOV2'] = numpy.log10(N_data['HTOV2'])\n",
    "N_data_norm['HTOV3'] = numpy.log10(N_data['HTOV3'])\n",
    "N_data_norm['HTOV4'] = numpy.log10(N_data['HTOV4'])\n",
    "N_data_norm['HTOV5'] = numpy.log10(N_data['HTOV5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same for manually added arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvlscratch/SHI/users/hofman/ML/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train/test ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count= 20549 test count= 6850\n"
     ]
    }
   ],
   "source": [
    "train_test_split_ratio = 0.75\n",
    "samp_count_train = int(TPS_data_norm.shape[0] * train_test_split_ratio)\n",
    "samp_count_test = TPS_data_norm.shape[0] - samp_count_train\n",
    "print('train count=', samp_count_train, 'test count=', samp_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPS train: (20549, 25) TPS test: (6850, 25)\n",
      "N train:   (20549, 25) N test:   (6850, 25)\n"
     ]
    }
   ],
   "source": [
    "TPS_train = TPS_data_norm[:samp_count_train]\n",
    "TPS_test = TPS_data_norm[samp_count_train:]\n",
    "\n",
    "N_train = N_data_norm[:samp_count_train]\n",
    "N_test = N_data_norm[samp_count_train:]\n",
    "\n",
    "print('TPS train:',TPS_train.shape,'TPS test:',TPS_test.shape)\n",
    "print('N train:  ',N_train.shape,  'N test:  ',N_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check interclass balance of TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T in TPS train:    (6879, 25)\n",
      "regP in TPS train: (6885, 25)\n",
      "regS in TPS train: (6785, 25)\n",
      "T in TPS test:     (2254, 25)\n",
      "regP in TPS test:  (2248, 25)\n",
      "regS in TPS test:  (2348, 25)\n"
     ]
    }
   ],
   "source": [
    "print('T in TPS train:   ', TPS_train[TPS_train['CLASS_PHASE']=='tele'].shape)\n",
    "print('regP in TPS train:', TPS_train[TPS_train['CLASS_PHASE']=='regP'].shape)\n",
    "print('regS in TPS train:', TPS_train[TPS_train['CLASS_PHASE']=='regS'].shape)\n",
    "\n",
    "print('T in TPS test:    ', TPS_test[TPS_test['CLASS_PHASE']=='tele'].shape)\n",
    "print('regP in TPS test: ', TPS_test[TPS_test['CLASS_PHASE']=='regP'].shape)\n",
    "print('regS in TPS test: ', TPS_test[TPS_test['CLASS_PHASE']=='regS'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form train and test sets and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([TPS_train, N_train]).sample(frac=1, random_state=31).reset_index(drop=True)\n",
    "test =  pd.concat([TPS_test, N_test]).sample(frac=1, random_state=33).reset_index(drop=True)\n",
    "#train.apply(pd.to_numeric, errors='ignore')\n",
    "#test.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41098, 15) (41098, 2) (13700, 15) (13700, 2)\n",
      "(13700, 9)\n"
     ]
    }
   ],
   "source": [
    "train_X = train[x_indices].values.astype(float)\n",
    "train_Y = train[y_indices]\n",
    "\n",
    "test_X = test[x_indices].values.astype(float)\n",
    "test_Y = test[y_indices]\n",
    "\n",
    "train_Y_ = numpy.array(numpy.where(train_Y == 'N', 1, 0), dtype=float)\n",
    "test_Y_ = numpy.array(numpy.where(test_Y == 'N', 1, 0), dtype=float)\n",
    "\n",
    "#convert to categorical\n",
    "train_Y = keras.utils.to_categorical(train_Y_)\n",
    "test_Y = keras.utils.to_categorical(test_Y_)\n",
    "\n",
    "test_metadata = test[metadata]\n",
    "\n",
    "print(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)\n",
    "print(test_metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verification, that if node1 < 0.5 => N (nodes start at 1: node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>PER</th>\n",
       "      <th>RECT</th>\n",
       "      <th>PLANS</th>\n",
       "      <th>INANG1</th>\n",
       "      <th>INANG3</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.876298</td>\n",
       "      <td>0.850086</td>\n",
       "      <td>0.992758</td>\n",
       "      <td>0.224657</td>\n",
       "      <td>0.367041</td>\n",
       "      <td>1.148829</td>\n",
       "      <td>0.695760</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>-0.104905</td>\n",
       "      <td>-0.435932</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>0.438466</td>\n",
       "      <td>0.293249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.724983</td>\n",
       "      <td>0.854786</td>\n",
       "      <td>0.919896</td>\n",
       "      <td>0.818256</td>\n",
       "      <td>0.443154</td>\n",
       "      <td>0.119121</td>\n",
       "      <td>0.119121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.379387</td>\n",
       "      <td>0.232105</td>\n",
       "      <td>0.387892</td>\n",
       "      <td>-0.076344</td>\n",
       "      <td>0.207745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853112</td>\n",
       "      <td>0.929160</td>\n",
       "      <td>0.645099</td>\n",
       "      <td>0.821087</td>\n",
       "      <td>0.557919</td>\n",
       "      <td>0.088165</td>\n",
       "      <td>-0.062487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293835</td>\n",
       "      <td>-0.475917</td>\n",
       "      <td>0.062625</td>\n",
       "      <td>-0.106332</td>\n",
       "      <td>-0.609955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.963301</td>\n",
       "      <td>0.978250</td>\n",
       "      <td>0.143203</td>\n",
       "      <td>0.843323</td>\n",
       "      <td>0.299923</td>\n",
       "      <td>-1.193141</td>\n",
       "      <td>-0.982010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>-0.325329</td>\n",
       "      <td>-1.110690</td>\n",
       "      <td>-0.928083</td>\n",
       "      <td>-1.152685</td>\n",
       "      <td>-1.283656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.765570</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.907409</td>\n",
       "      <td>0.913729</td>\n",
       "      <td>0.311822</td>\n",
       "      <td>0.352552</td>\n",
       "      <td>0.136282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213465</td>\n",
       "      <td>0.628614</td>\n",
       "      <td>0.641160</td>\n",
       "      <td>0.073407</td>\n",
       "      <td>0.112197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>N</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.925275</td>\n",
       "      <td>0.912522</td>\n",
       "      <td>0.979026</td>\n",
       "      <td>0.127511</td>\n",
       "      <td>0.622694</td>\n",
       "      <td>0.900261</td>\n",
       "      <td>0.823713</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.510000</td>\n",
       "      <td>0.097697</td>\n",
       "      <td>-0.450898</td>\n",
       "      <td>0.163709</td>\n",
       "      <td>0.721411</td>\n",
       "      <td>0.427589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.908256</td>\n",
       "      <td>0.129385</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.237398</td>\n",
       "      <td>-1.140370</td>\n",
       "      <td>-0.767501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.237752</td>\n",
       "      <td>-0.323295</td>\n",
       "      <td>0.077786</td>\n",
       "      <td>-0.792014</td>\n",
       "      <td>-1.187756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899306</td>\n",
       "      <td>0.910752</td>\n",
       "      <td>0.224614</td>\n",
       "      <td>0.871148</td>\n",
       "      <td>0.461925</td>\n",
       "      <td>-0.767314</td>\n",
       "      <td>-0.533612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025681</td>\n",
       "      <td>-0.322970</td>\n",
       "      <td>-0.288431</td>\n",
       "      <td>-0.350781</td>\n",
       "      <td>-0.471402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.700953</td>\n",
       "      <td>0.691383</td>\n",
       "      <td>0.345555</td>\n",
       "      <td>0.905487</td>\n",
       "      <td>0.295192</td>\n",
       "      <td>-0.344796</td>\n",
       "      <td>-0.452101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.220504</td>\n",
       "      <td>0.049985</td>\n",
       "      <td>-0.049138</td>\n",
       "      <td>-0.278167</td>\n",
       "      <td>-1.459330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.867906</td>\n",
       "      <td>0.971856</td>\n",
       "      <td>0.316954</td>\n",
       "      <td>0.928498</td>\n",
       "      <td>1.501361</td>\n",
       "      <td>0.520125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.313979</td>\n",
       "      <td>0.107318</td>\n",
       "      <td>0.327013</td>\n",
       "      <td>0.284634</td>\n",
       "      <td>0.545984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.986179</td>\n",
       "      <td>0.977575</td>\n",
       "      <td>0.178845</td>\n",
       "      <td>0.920053</td>\n",
       "      <td>0.371750</td>\n",
       "      <td>-1.253880</td>\n",
       "      <td>-1.220250</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>-0.041187</td>\n",
       "      <td>-0.346442</td>\n",
       "      <td>-0.629941</td>\n",
       "      <td>-0.919258</td>\n",
       "      <td>-1.196308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>N</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.734188</td>\n",
       "      <td>0.938525</td>\n",
       "      <td>0.777554</td>\n",
       "      <td>0.255878</td>\n",
       "      <td>0.206232</td>\n",
       "      <td>0.584392</td>\n",
       "      <td>0.710212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>-0.769543</td>\n",
       "      <td>-0.101597</td>\n",
       "      <td>-0.235174</td>\n",
       "      <td>-0.001043</td>\n",
       "      <td>-0.471295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.912541</td>\n",
       "      <td>0.920225</td>\n",
       "      <td>0.878451</td>\n",
       "      <td>0.765400</td>\n",
       "      <td>0.580792</td>\n",
       "      <td>0.532951</td>\n",
       "      <td>0.532951</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.480000</td>\n",
       "      <td>-0.019586</td>\n",
       "      <td>-0.069236</td>\n",
       "      <td>0.184375</td>\n",
       "      <td>-0.051256</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.878749</td>\n",
       "      <td>0.904278</td>\n",
       "      <td>0.113274</td>\n",
       "      <td>0.763504</td>\n",
       "      <td>0.316315</td>\n",
       "      <td>-0.861839</td>\n",
       "      <td>-0.658732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.268005</td>\n",
       "      <td>0.257363</td>\n",
       "      <td>-0.005301</td>\n",
       "      <td>-0.342838</td>\n",
       "      <td>-0.538245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.824342</td>\n",
       "      <td>0.903932</td>\n",
       "      <td>0.899939</td>\n",
       "      <td>0.476613</td>\n",
       "      <td>0.327688</td>\n",
       "      <td>0.484665</td>\n",
       "      <td>0.332493</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.280000</td>\n",
       "      <td>0.674882</td>\n",
       "      <td>-0.045471</td>\n",
       "      <td>0.255222</td>\n",
       "      <td>0.169060</td>\n",
       "      <td>-0.185281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.969375</td>\n",
       "      <td>0.218731</td>\n",
       "      <td>0.954861</td>\n",
       "      <td>0.148082</td>\n",
       "      <td>-1.012494</td>\n",
       "      <td>-1.012927</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.346275</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>-0.326577</td>\n",
       "      <td>-0.172151</td>\n",
       "      <td>-0.870078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.780692</td>\n",
       "      <td>0.670635</td>\n",
       "      <td>0.062865</td>\n",
       "      <td>0.954765</td>\n",
       "      <td>0.159496</td>\n",
       "      <td>-0.648723</td>\n",
       "      <td>-0.287122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223462</td>\n",
       "      <td>-0.680324</td>\n",
       "      <td>-0.152228</td>\n",
       "      <td>-0.289804</td>\n",
       "      <td>-0.625312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.931269</td>\n",
       "      <td>0.935904</td>\n",
       "      <td>0.069461</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.243540</td>\n",
       "      <td>-1.122903</td>\n",
       "      <td>-1.035537</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.152422</td>\n",
       "      <td>-0.176262</td>\n",
       "      <td>-0.042912</td>\n",
       "      <td>-1.012709</td>\n",
       "      <td>-0.863007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.859081</td>\n",
       "      <td>0.907076</td>\n",
       "      <td>0.917780</td>\n",
       "      <td>0.060567</td>\n",
       "      <td>0.290952</td>\n",
       "      <td>1.015878</td>\n",
       "      <td>0.980417</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.435149</td>\n",
       "      <td>0.158710</td>\n",
       "      <td>0.658482</td>\n",
       "      <td>-0.154930</td>\n",
       "      <td>0.382831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>N</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.996662</td>\n",
       "      <td>0.995492</td>\n",
       "      <td>0.983444</td>\n",
       "      <td>0.969830</td>\n",
       "      <td>1.224282</td>\n",
       "      <td>2.074247</td>\n",
       "      <td>1.664387</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.863486</td>\n",
       "      <td>-0.299639</td>\n",
       "      <td>-0.236510</td>\n",
       "      <td>0.731972</td>\n",
       "      <td>1.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.753457</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.806548</td>\n",
       "      <td>0.245103</td>\n",
       "      <td>0.312072</td>\n",
       "      <td>0.215959</td>\n",
       "      <td>0.578478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263411</td>\n",
       "      <td>0.051648</td>\n",
       "      <td>0.293614</td>\n",
       "      <td>-0.283589</td>\n",
       "      <td>-0.217364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.931038</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.786631</td>\n",
       "      <td>0.380105</td>\n",
       "      <td>0.509318</td>\n",
       "      <td>0.527989</td>\n",
       "      <td>0.571050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.090361</td>\n",
       "      <td>0.505168</td>\n",
       "      <td>0.356008</td>\n",
       "      <td>0.130399</td>\n",
       "      <td>0.116619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.707708</td>\n",
       "      <td>0.633611</td>\n",
       "      <td>0.982681</td>\n",
       "      <td>0.306785</td>\n",
       "      <td>0.316673</td>\n",
       "      <td>0.316716</td>\n",
       "      <td>0.373572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073358</td>\n",
       "      <td>-0.915903</td>\n",
       "      <td>0.100977</td>\n",
       "      <td>0.144344</td>\n",
       "      <td>0.194749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>N</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.680049</td>\n",
       "      <td>0.638114</td>\n",
       "      <td>0.805975</td>\n",
       "      <td>0.332990</td>\n",
       "      <td>0.343870</td>\n",
       "      <td>0.287133</td>\n",
       "      <td>0.302722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.184749</td>\n",
       "      <td>-0.044022</td>\n",
       "      <td>0.374648</td>\n",
       "      <td>0.178626</td>\n",
       "      <td>-0.390075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.754964</td>\n",
       "      <td>0.800923</td>\n",
       "      <td>0.477826</td>\n",
       "      <td>0.950568</td>\n",
       "      <td>0.062317</td>\n",
       "      <td>-0.210353</td>\n",
       "      <td>-0.310613</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.316197</td>\n",
       "      <td>-0.298547</td>\n",
       "      <td>-0.070630</td>\n",
       "      <td>-0.755072</td>\n",
       "      <td>-0.629255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>N</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.812922</td>\n",
       "      <td>0.939796</td>\n",
       "      <td>0.063287</td>\n",
       "      <td>0.953799</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>-0.715136</td>\n",
       "      <td>-0.715136</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.160000</td>\n",
       "      <td>-0.311101</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>-0.288539</td>\n",
       "      <td>-0.411280</td>\n",
       "      <td>-0.815173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.971444</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.953304</td>\n",
       "      <td>0.413106</td>\n",
       "      <td>-1.368718</td>\n",
       "      <td>-1.093702</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>-0.634399</td>\n",
       "      <td>0.737721</td>\n",
       "      <td>-1.001268</td>\n",
       "      <td>-1.171678</td>\n",
       "      <td>-1.213101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.985945</td>\n",
       "      <td>0.976439</td>\n",
       "      <td>0.209482</td>\n",
       "      <td>0.792417</td>\n",
       "      <td>0.351810</td>\n",
       "      <td>-1.134672</td>\n",
       "      <td>-1.115811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.718916</td>\n",
       "      <td>-0.576084</td>\n",
       "      <td>0.453048</td>\n",
       "      <td>-0.123293</td>\n",
       "      <td>-1.299971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.968763</td>\n",
       "      <td>0.997662</td>\n",
       "      <td>0.108221</td>\n",
       "      <td>0.891779</td>\n",
       "      <td>0.096675</td>\n",
       "      <td>-1.329117</td>\n",
       "      <td>-1.329117</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.248270</td>\n",
       "      <td>-0.707070</td>\n",
       "      <td>-0.995287</td>\n",
       "      <td>-0.987146</td>\n",
       "      <td>-1.417225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.859498</td>\n",
       "      <td>0.679806</td>\n",
       "      <td>0.175511</td>\n",
       "      <td>0.867893</td>\n",
       "      <td>0.275445</td>\n",
       "      <td>-0.736345</td>\n",
       "      <td>-0.122630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.178860</td>\n",
       "      <td>-0.017908</td>\n",
       "      <td>-0.628846</td>\n",
       "      <td>-0.376661</td>\n",
       "      <td>-0.519171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41068</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.979126</td>\n",
       "      <td>0.974495</td>\n",
       "      <td>0.137906</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>0.294462</td>\n",
       "      <td>-1.341358</td>\n",
       "      <td>-1.341358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.150916</td>\n",
       "      <td>-0.033555</td>\n",
       "      <td>0.285617</td>\n",
       "      <td>-0.362343</td>\n",
       "      <td>-1.348405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41069</th>\n",
       "      <td>N</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.770160</td>\n",
       "      <td>0.793866</td>\n",
       "      <td>0.953620</td>\n",
       "      <td>0.994465</td>\n",
       "      <td>0.623806</td>\n",
       "      <td>0.238763</td>\n",
       "      <td>0.238763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586060</td>\n",
       "      <td>-0.272697</td>\n",
       "      <td>-0.032401</td>\n",
       "      <td>-0.275759</td>\n",
       "      <td>-0.175966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41070</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933443</td>\n",
       "      <td>0.976155</td>\n",
       "      <td>0.327840</td>\n",
       "      <td>0.700661</td>\n",
       "      <td>0.488105</td>\n",
       "      <td>-0.607916</td>\n",
       "      <td>-0.433530</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>-0.851795</td>\n",
       "      <td>-0.732145</td>\n",
       "      <td>-0.057819</td>\n",
       "      <td>-0.423548</td>\n",
       "      <td>0.140452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41071</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944471</td>\n",
       "      <td>0.948240</td>\n",
       "      <td>0.352494</td>\n",
       "      <td>0.951740</td>\n",
       "      <td>0.677736</td>\n",
       "      <td>-0.611654</td>\n",
       "      <td>-0.611654</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.162976</td>\n",
       "      <td>-0.294189</td>\n",
       "      <td>-0.281135</td>\n",
       "      <td>-0.036755</td>\n",
       "      <td>0.350213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41072</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.855989</td>\n",
       "      <td>0.864540</td>\n",
       "      <td>0.942939</td>\n",
       "      <td>0.142915</td>\n",
       "      <td>0.345672</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.368014</td>\n",
       "      <td>-0.366299</td>\n",
       "      <td>0.331258</td>\n",
       "      <td>0.722327</td>\n",
       "      <td>0.267139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41073</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.957179</td>\n",
       "      <td>0.957634</td>\n",
       "      <td>0.065360</td>\n",
       "      <td>0.984487</td>\n",
       "      <td>0.266696</td>\n",
       "      <td>-1.316695</td>\n",
       "      <td>-1.316695</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>-0.735186</td>\n",
       "      <td>-0.428312</td>\n",
       "      <td>-0.335086</td>\n",
       "      <td>-1.214100</td>\n",
       "      <td>-1.083459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41074</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889126</td>\n",
       "      <td>0.745868</td>\n",
       "      <td>0.929716</td>\n",
       "      <td>0.256858</td>\n",
       "      <td>0.448673</td>\n",
       "      <td>0.750763</td>\n",
       "      <td>0.449158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.201351</td>\n",
       "      <td>-0.667217</td>\n",
       "      <td>0.283092</td>\n",
       "      <td>0.620555</td>\n",
       "      <td>0.333290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41075</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936230</td>\n",
       "      <td>0.888872</td>\n",
       "      <td>0.491175</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.176604</td>\n",
       "      <td>-0.284502</td>\n",
       "      <td>-0.223870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.355425</td>\n",
       "      <td>-0.447954</td>\n",
       "      <td>0.466965</td>\n",
       "      <td>-0.102587</td>\n",
       "      <td>-0.265179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41076</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.903400</td>\n",
       "      <td>0.886285</td>\n",
       "      <td>0.142902</td>\n",
       "      <td>0.947745</td>\n",
       "      <td>0.374766</td>\n",
       "      <td>-0.911206</td>\n",
       "      <td>-0.886849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094249</td>\n",
       "      <td>-1.298802</td>\n",
       "      <td>-0.950364</td>\n",
       "      <td>-0.890631</td>\n",
       "      <td>-1.465074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41077</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.927132</td>\n",
       "      <td>0.898334</td>\n",
       "      <td>0.952704</td>\n",
       "      <td>0.318458</td>\n",
       "      <td>0.384152</td>\n",
       "      <td>1.735059</td>\n",
       "      <td>0.659627</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.415000</td>\n",
       "      <td>-0.293091</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0.207825</td>\n",
       "      <td>-0.011761</td>\n",
       "      <td>0.489973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41078</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.825852</td>\n",
       "      <td>0.896448</td>\n",
       "      <td>0.118127</td>\n",
       "      <td>0.776636</td>\n",
       "      <td>0.299054</td>\n",
       "      <td>-0.714412</td>\n",
       "      <td>-0.283508</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>-0.358782</td>\n",
       "      <td>0.212885</td>\n",
       "      <td>-0.373943</td>\n",
       "      <td>0.071338</td>\n",
       "      <td>-0.774494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41079</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.875150</td>\n",
       "      <td>0.924862</td>\n",
       "      <td>0.351293</td>\n",
       "      <td>0.912484</td>\n",
       "      <td>0.418951</td>\n",
       "      <td>-0.518906</td>\n",
       "      <td>-0.518906</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.072135</td>\n",
       "      <td>-0.698909</td>\n",
       "      <td>-0.634793</td>\n",
       "      <td>-0.624205</td>\n",
       "      <td>-0.291191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41080</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.984796</td>\n",
       "      <td>0.971734</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>0.941612</td>\n",
       "      <td>1.022445</td>\n",
       "      <td>1.387870</td>\n",
       "      <td>-1.288770</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.647298</td>\n",
       "      <td>1.079941</td>\n",
       "      <td>0.082927</td>\n",
       "      <td>-0.909959</td>\n",
       "      <td>-1.041931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41081</th>\n",
       "      <td>N</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.668361</td>\n",
       "      <td>0.813554</td>\n",
       "      <td>0.606784</td>\n",
       "      <td>0.839175</td>\n",
       "      <td>0.257744</td>\n",
       "      <td>0.074377</td>\n",
       "      <td>-0.194323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170536</td>\n",
       "      <td>0.179047</td>\n",
       "      <td>0.060131</td>\n",
       "      <td>-0.384326</td>\n",
       "      <td>-0.161193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41082</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>0.735538</td>\n",
       "      <td>0.938476</td>\n",
       "      <td>0.461720</td>\n",
       "      <td>0.498774</td>\n",
       "      <td>0.443338</td>\n",
       "      <td>0.217856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.247652</td>\n",
       "      <td>0.336819</td>\n",
       "      <td>0.069614</td>\n",
       "      <td>-0.149640</td>\n",
       "      <td>-0.030781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41083</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952051</td>\n",
       "      <td>0.944616</td>\n",
       "      <td>0.679677</td>\n",
       "      <td>0.692976</td>\n",
       "      <td>0.725076</td>\n",
       "      <td>0.172814</td>\n",
       "      <td>0.172814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.319340</td>\n",
       "      <td>-0.589120</td>\n",
       "      <td>0.410986</td>\n",
       "      <td>-0.182170</td>\n",
       "      <td>-0.472858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41084</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805931</td>\n",
       "      <td>0.621675</td>\n",
       "      <td>0.247833</td>\n",
       "      <td>0.754707</td>\n",
       "      <td>0.338340</td>\n",
       "      <td>-0.557695</td>\n",
       "      <td>-0.021838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012022</td>\n",
       "      <td>-0.368609</td>\n",
       "      <td>-0.329644</td>\n",
       "      <td>0.089716</td>\n",
       "      <td>-0.192997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41085</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.765341</td>\n",
       "      <td>0.732147</td>\n",
       "      <td>0.464856</td>\n",
       "      <td>0.370299</td>\n",
       "      <td>0.324277</td>\n",
       "      <td>-0.236159</td>\n",
       "      <td>0.240921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119073</td>\n",
       "      <td>-0.399019</td>\n",
       "      <td>0.018548</td>\n",
       "      <td>0.151814</td>\n",
       "      <td>0.220779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41086</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.792455</td>\n",
       "      <td>0.680765</td>\n",
       "      <td>0.926618</td>\n",
       "      <td>0.253702</td>\n",
       "      <td>0.353719</td>\n",
       "      <td>0.600697</td>\n",
       "      <td>0.385511</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.310000</td>\n",
       "      <td>-0.378095</td>\n",
       "      <td>-0.235923</td>\n",
       "      <td>0.322643</td>\n",
       "      <td>0.329945</td>\n",
       "      <td>-0.021874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41087</th>\n",
       "      <td>regS</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941271</td>\n",
       "      <td>0.785843</td>\n",
       "      <td>0.714827</td>\n",
       "      <td>0.714812</td>\n",
       "      <td>0.474138</td>\n",
       "      <td>0.284462</td>\n",
       "      <td>0.276263</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.273333</td>\n",
       "      <td>0.403520</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>0.238170</td>\n",
       "      <td>0.175213</td>\n",
       "      <td>0.476639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41088</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.983793</td>\n",
       "      <td>0.999431</td>\n",
       "      <td>0.104955</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.994336</td>\n",
       "      <td>-1.522490</td>\n",
       "      <td>-1.461532</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.236667</td>\n",
       "      <td>-0.244699</td>\n",
       "      <td>-0.175320</td>\n",
       "      <td>0.204403</td>\n",
       "      <td>-0.568774</td>\n",
       "      <td>-0.827451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41089</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.853753</td>\n",
       "      <td>0.676357</td>\n",
       "      <td>0.872275</td>\n",
       "      <td>0.388237</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.580782</td>\n",
       "      <td>0.215390</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.320000</td>\n",
       "      <td>0.303563</td>\n",
       "      <td>-0.039166</td>\n",
       "      <td>0.023164</td>\n",
       "      <td>0.078532</td>\n",
       "      <td>-0.127373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41090</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.874694</td>\n",
       "      <td>0.955306</td>\n",
       "      <td>0.686920</td>\n",
       "      <td>0.321280</td>\n",
       "      <td>0.397482</td>\n",
       "      <td>0.309303</td>\n",
       "      <td>0.309303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.072007</td>\n",
       "      <td>0.165425</td>\n",
       "      <td>0.124637</td>\n",
       "      <td>-0.614163</td>\n",
       "      <td>-0.756393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41091</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.851833</td>\n",
       "      <td>0.945768</td>\n",
       "      <td>0.626774</td>\n",
       "      <td>0.430388</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.132129</td>\n",
       "      <td>0.132129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381796</td>\n",
       "      <td>-0.449600</td>\n",
       "      <td>-0.065097</td>\n",
       "      <td>-0.573143</td>\n",
       "      <td>-0.633071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41092</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.907732</td>\n",
       "      <td>0.833940</td>\n",
       "      <td>0.910396</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.522917</td>\n",
       "      <td>0.690091</td>\n",
       "      <td>-0.154159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175161</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>-0.153314</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>-0.376151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41093</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927422</td>\n",
       "      <td>0.995504</td>\n",
       "      <td>0.342540</td>\n",
       "      <td>0.994559</td>\n",
       "      <td>0.672688</td>\n",
       "      <td>-0.621618</td>\n",
       "      <td>-0.621618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015847</td>\n",
       "      <td>-0.059413</td>\n",
       "      <td>-0.232174</td>\n",
       "      <td>-0.200008</td>\n",
       "      <td>-0.403299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41094</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.818709</td>\n",
       "      <td>0.860883</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.418564</td>\n",
       "      <td>0.513162</td>\n",
       "      <td>0.328310</td>\n",
       "      <td>0.538771</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.090875</td>\n",
       "      <td>-0.281240</td>\n",
       "      <td>0.163335</td>\n",
       "      <td>0.176027</td>\n",
       "      <td>0.389756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41095</th>\n",
       "      <td>tele</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966637</td>\n",
       "      <td>0.997232</td>\n",
       "      <td>0.150558</td>\n",
       "      <td>0.880549</td>\n",
       "      <td>0.206444</td>\n",
       "      <td>-1.197021</td>\n",
       "      <td>-1.197021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.686582</td>\n",
       "      <td>-0.992538</td>\n",
       "      <td>-1.072372</td>\n",
       "      <td>-0.787548</td>\n",
       "      <td>-0.535877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41096</th>\n",
       "      <td>N</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921727</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.407504</td>\n",
       "      <td>0.887150</td>\n",
       "      <td>0.856020</td>\n",
       "      <td>-0.415922</td>\n",
       "      <td>-0.884345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.437333</td>\n",
       "      <td>-0.873966</td>\n",
       "      <td>0.052172</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>-0.217876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41097</th>\n",
       "      <td>N</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.968305</td>\n",
       "      <td>0.961828</td>\n",
       "      <td>0.152711</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.426619</td>\n",
       "      <td>-1.206102</td>\n",
       "      <td>-1.206102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>-0.701543</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>-0.350160</td>\n",
       "      <td>-1.225379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41098 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CLASS_PHASE       PER      RECT     PLANS    INANG1    INANG3     HMXMN  \\\n",
       "0            tele  0.333333  0.876298  0.850086  0.992758  0.224657  0.367041   \n",
       "1               N  0.166667  0.724983  0.854786  0.919896  0.818256  0.443154   \n",
       "2               N  1.000000  0.853112  0.929160  0.645099  0.821087  0.557919   \n",
       "3            tele  0.333333  0.963301  0.978250  0.143203  0.843323  0.299923   \n",
       "4            regS  0.166667  0.765570  0.680203  0.907409  0.913729  0.311822   \n",
       "5               N  0.333333  0.925275  0.912522  0.979026  0.127511  0.622694   \n",
       "6            tele  0.333333  0.950553  0.908256  0.129385  0.966102  0.237398   \n",
       "7               N  1.000000  0.899306  0.910752  0.224614  0.871148  0.461925   \n",
       "8            tele  0.333333  0.700953  0.691383  0.345555  0.905487  0.295192   \n",
       "9            tele  0.333333  0.984735  0.867906  0.971856  0.316954  0.928498   \n",
       "10           regP  0.166667  0.986179  0.977575  0.178845  0.920053  0.371750   \n",
       "11              N  0.444444  0.734188  0.938525  0.777554  0.255878  0.206232   \n",
       "12              N  0.166667  0.912541  0.920225  0.878451  0.765400  0.580792   \n",
       "13           regP  0.444444  0.878749  0.904278  0.113274  0.763504  0.316315   \n",
       "14           regS  0.444444  0.824342  0.903932  0.899939  0.476613  0.327688   \n",
       "15              N  0.166667  0.967664  0.969375  0.218731  0.954861  0.148082   \n",
       "16           regP  0.166667  0.780692  0.670635  0.062865  0.954765  0.159496   \n",
       "17           tele  0.333333  0.931269  0.935904  0.069461  0.946631  0.243540   \n",
       "18           regS  0.333333  0.859081  0.907076  0.917780  0.060567  0.290952   \n",
       "19              N  0.333333  0.996662  0.995492  0.983444  0.969830  1.224282   \n",
       "20              N  0.166667  0.753457  0.845140  0.806548  0.245103  0.312072   \n",
       "21           tele  0.166667  0.931038  0.804700  0.786631  0.380105  0.509318   \n",
       "22           regS  0.166667  0.707708  0.633611  0.982681  0.306785  0.316673   \n",
       "23              N  0.444444  0.680049  0.638114  0.805975  0.332990  0.343870   \n",
       "24              N  0.166667  0.754964  0.800923  0.477826  0.950568  0.062317   \n",
       "25              N  0.444444  0.812922  0.939796  0.063287  0.953799  0.455867   \n",
       "26           tele  0.333333  0.971444  0.942842  0.105174  0.953304  0.413106   \n",
       "27              N  0.166667  0.985945  0.976439  0.209482  0.792417  0.351810   \n",
       "28           tele  0.333333  0.968763  0.997662  0.108221  0.891779  0.096675   \n",
       "29           tele  0.444444  0.859498  0.679806  0.175511  0.867893  0.275445   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "41068           N  0.166667  0.979126  0.974495  0.137906  0.899167  0.294462   \n",
       "41069           N  0.333333  0.770160  0.793866  0.953620  0.994465  0.623806   \n",
       "41070           N  1.000000  0.933443  0.976155  0.327840  0.700661  0.488105   \n",
       "41071           N  1.000000  0.944471  0.948240  0.352494  0.951740  0.677736   \n",
       "41072        regS  0.333333  0.855989  0.864540  0.942939  0.142915  0.345672   \n",
       "41073        regP  0.444444  0.957179  0.957634  0.065360  0.984487  0.266696   \n",
       "41074        regS  0.333333  0.889126  0.745868  0.929716  0.256858  0.448673   \n",
       "41075           N  1.000000  0.936230  0.888872  0.491175  0.845800  0.176604   \n",
       "41076        tele  0.444444  0.903400  0.886285  0.142902  0.947745  0.374766   \n",
       "41077        tele  0.333333  0.927132  0.898334  0.952704  0.318458  0.384152   \n",
       "41078        regP  0.166667  0.825852  0.896448  0.118127  0.776636  0.299054   \n",
       "41079        tele  0.666667  0.875150  0.924862  0.351293  0.912484  0.418951   \n",
       "41080        tele  0.166667  0.984796  0.971734  0.998596  0.941612  1.022445   \n",
       "41081           N  0.333333  0.668361  0.813554  0.606784  0.839175  0.257744   \n",
       "41082           N  0.166667  0.837836  0.735538  0.938476  0.461720  0.498774   \n",
       "41083           N  1.000000  0.952051  0.944616  0.679677  0.692976  0.725076   \n",
       "41084           N  1.000000  0.805931  0.621675  0.247833  0.754707  0.338340   \n",
       "41085        tele  0.333333  0.765341  0.732147  0.464856  0.370299  0.324277   \n",
       "41086        regS  0.333333  0.792455  0.680765  0.926618  0.253702  0.353719   \n",
       "41087        regS  1.000000  0.941271  0.785843  0.714827  0.714812  0.474138   \n",
       "41088        regP  0.166667  0.983793  0.999431  0.104955  0.998228  0.994336   \n",
       "41089        regS  0.166667  0.853753  0.676357  0.872275  0.388237  0.289720   \n",
       "41090           N  0.166667  0.874694  0.955306  0.686920  0.321280  0.397482   \n",
       "41091           N  0.166667  0.851833  0.945768  0.626774  0.430388  0.488963   \n",
       "41092        regS  0.444444  0.907732  0.833940  0.910396  0.938447  0.522917   \n",
       "41093           N  1.000000  0.927422  0.995504  0.342540  0.994559  0.672688   \n",
       "41094        regS  0.166667  0.818709  0.860883  0.985714  0.418564  0.513162   \n",
       "41095        tele  1.000000  0.966637  0.997232  0.150558  0.880549  0.206444   \n",
       "41096           N  1.000000  0.921727  0.926136  0.407504  0.887150  0.856020   \n",
       "41097           N  0.166667  0.968305  0.961828  0.152711  0.928736  0.426619   \n",
       "\n",
       "         HVRATP     HVRAT  NAB       TAB     HTOV1     HTOV2     HTOV3  \\\n",
       "0      1.148829  0.695760  0.1  0.210000 -0.104905 -0.435932  0.226950   \n",
       "1      0.119121  0.119121  0.0  0.000000 -0.379387  0.232105  0.387892   \n",
       "2      0.088165 -0.062487  0.0  0.000000  0.293835 -0.475917  0.062625   \n",
       "3     -1.193141 -0.982010  0.1  0.230000 -0.325329 -1.110690 -0.928083   \n",
       "4      0.352552  0.136282  0.0  0.000000  0.213465  0.628614  0.641160   \n",
       "5      0.900261  0.823713 -0.1 -0.510000  0.097697 -0.450898  0.163709   \n",
       "6     -1.140370 -0.767501  0.0  0.000000 -0.237752 -0.323295  0.077786   \n",
       "7     -0.767314 -0.533612  0.0  0.000000  0.025681 -0.322970 -0.288431   \n",
       "8     -0.344796 -0.452101  0.0  0.000000 -0.220504  0.049985 -0.049138   \n",
       "9      1.501361  0.520125  0.1  0.450000 -0.313979  0.107318  0.327013   \n",
       "10    -1.253880 -1.220250  0.1  0.310000 -0.041187 -0.346442 -0.629941   \n",
       "11     0.584392  0.710212  0.0  0.160000 -0.769543 -0.101597 -0.235174   \n",
       "12     0.532951  0.532951 -0.1 -0.480000 -0.019586 -0.069236  0.184375   \n",
       "13    -0.861839 -0.658732  0.0  0.000000 -0.268005  0.257363 -0.005301   \n",
       "14     0.484665  0.332493 -0.1 -0.280000  0.674882 -0.045471  0.255222   \n",
       "15    -1.012494 -1.012927  0.1  0.240000  0.346275 -0.006098 -0.326577   \n",
       "16    -0.648723 -0.287122  0.0  0.000000  0.223462 -0.680324 -0.152228   \n",
       "17    -1.122903 -1.035537  0.1  0.190000  0.152422 -0.176262 -0.042912   \n",
       "18     1.015878  0.980417  0.1  0.100000  0.435149  0.158710  0.658482   \n",
       "19     2.074247  1.664387  0.1  0.050000  0.863486 -0.299639 -0.236510   \n",
       "20     0.215959  0.578478  0.0  0.000000  0.263411  0.051648  0.293614   \n",
       "21     0.527989  0.571050  0.0  0.005000  0.090361  0.505168  0.356008   \n",
       "22     0.316716  0.373572  0.0  0.000000  0.073358 -0.915903  0.100977   \n",
       "23     0.287133  0.302722  0.0  0.000000 -0.184749 -0.044022  0.374648   \n",
       "24    -0.210353 -0.310613  0.1  0.060000  0.316197 -0.298547 -0.070630   \n",
       "25    -0.715136 -0.715136 -0.2 -0.160000 -0.311101 -0.196864 -0.288539   \n",
       "26    -1.368718 -1.093702  0.1  0.110000 -0.634399  0.737721 -1.001268   \n",
       "27    -1.134672 -1.115811  0.0  0.000000 -0.718916 -0.576084  0.453048   \n",
       "28    -1.329117 -1.329117  0.1  0.230000  0.248270 -0.707070 -0.995287   \n",
       "29    -0.736345 -0.122630  0.0  0.000000 -0.178860 -0.017908 -0.628846   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "41068 -1.341358 -1.341358  0.0  0.000000 -0.150916 -0.033555  0.285617   \n",
       "41069  0.238763  0.238763  0.0  0.000000  0.586060 -0.272697 -0.032401   \n",
       "41070 -0.607916 -0.433530  0.1  0.360000 -0.851795 -0.732145 -0.057819   \n",
       "41071 -0.611654 -0.611654  0.2  0.395000  0.162976 -0.294189 -0.281135   \n",
       "41072  0.801667  0.801667  0.0  0.000000 -0.368014 -0.366299  0.331258   \n",
       "41073 -1.316695 -1.316695  0.1  0.290000 -0.735186 -0.428312 -0.335086   \n",
       "41074  0.750763  0.449158  0.0 -0.150000 -0.201351 -0.667217  0.283092   \n",
       "41075 -0.284502 -0.223870  0.0  0.000000 -0.355425 -0.447954  0.466965   \n",
       "41076 -0.911206 -0.886849  0.0  0.000000  0.094249 -1.298802 -0.950364   \n",
       "41077  1.735059  0.659627 -0.1 -0.415000 -0.293091  0.538998  0.207825   \n",
       "41078 -0.714412 -0.283508  0.1  0.460000 -0.358782  0.212885 -0.373943   \n",
       "41079 -0.518906 -0.518906  0.1  0.200000  0.072135 -0.698909 -0.634793   \n",
       "41080  1.387870 -1.288770  0.1  0.160000  0.647298  1.079941  0.082927   \n",
       "41081  0.074377 -0.194323  0.0  0.000000  0.170536  0.179047  0.060131   \n",
       "41082  0.443338  0.217856  0.0  0.000000 -0.247652  0.336819  0.069614   \n",
       "41083  0.172814  0.172814  0.0  0.000000 -0.319340 -0.589120  0.410986   \n",
       "41084 -0.557695 -0.021838  0.0  0.000000 -0.012022 -0.368609 -0.329644   \n",
       "41085 -0.236159  0.240921  0.0  0.000000  0.119073 -0.399019  0.018548   \n",
       "41086  0.600697  0.385511 -0.1 -0.310000 -0.378095 -0.235923  0.322643   \n",
       "41087  0.284462  0.276263  0.3  0.273333  0.403520  0.035794  0.238170   \n",
       "41088 -1.522490 -1.461532  0.3  0.236667 -0.244699 -0.175320  0.204403   \n",
       "41089  0.580782  0.215390 -0.2 -0.320000  0.303563 -0.039166  0.023164   \n",
       "41090  0.309303  0.309303  0.0  0.000000 -0.072007  0.165425  0.124637   \n",
       "41091  0.132129  0.132129  0.0  0.000000  0.381796 -0.449600 -0.065097   \n",
       "41092  0.690091 -0.154159  0.0  0.000000  0.175161  0.261322 -0.153314   \n",
       "41093 -0.621618 -0.621618  0.0  0.000000  0.015847 -0.059413 -0.232174   \n",
       "41094  0.328310  0.538771  0.3  0.123333  0.090875 -0.281240  0.163335   \n",
       "41095 -1.197021 -1.197021  0.0  0.000000 -0.686582 -0.992538 -1.072372   \n",
       "41096 -0.415922 -0.884345  0.0  0.000000 -0.437333 -0.873966  0.052172   \n",
       "41097 -1.206102 -1.206102  0.0  0.000000  0.032020 -0.701543  0.011510   \n",
       "\n",
       "          HTOV4     HTOV5  \n",
       "0      0.438466  0.293249  \n",
       "1     -0.076344  0.207745  \n",
       "2     -0.106332 -0.609955  \n",
       "3     -1.152685 -1.283656  \n",
       "4      0.073407  0.112197  \n",
       "5      0.721411  0.427589  \n",
       "6     -0.792014 -1.187756  \n",
       "7     -0.350781 -0.471402  \n",
       "8     -0.278167 -1.459330  \n",
       "9      0.284634  0.545984  \n",
       "10    -0.919258 -1.196308  \n",
       "11    -0.001043 -0.471295  \n",
       "12    -0.051256  0.028006  \n",
       "13    -0.342838 -0.538245  \n",
       "14     0.169060 -0.185281  \n",
       "15    -0.172151 -0.870078  \n",
       "16    -0.289804 -0.625312  \n",
       "17    -1.012709 -0.863007  \n",
       "18    -0.154930  0.382831  \n",
       "19     0.731972  1.781500  \n",
       "20    -0.283589 -0.217364  \n",
       "21     0.130399  0.116619  \n",
       "22     0.144344  0.194749  \n",
       "23     0.178626 -0.390075  \n",
       "24    -0.755072 -0.629255  \n",
       "25    -0.411280 -0.815173  \n",
       "26    -1.171678 -1.213101  \n",
       "27    -0.123293 -1.299971  \n",
       "28    -0.987146 -1.417225  \n",
       "29    -0.376661 -0.519171  \n",
       "...         ...       ...  \n",
       "41068 -0.362343 -1.348405  \n",
       "41069 -0.275759 -0.175966  \n",
       "41070 -0.423548  0.140452  \n",
       "41071 -0.036755  0.350213  \n",
       "41072  0.722327  0.267139  \n",
       "41073 -1.214100 -1.083459  \n",
       "41074  0.620555  0.333290  \n",
       "41075 -0.102587 -0.265179  \n",
       "41076 -0.890631 -1.465074  \n",
       "41077 -0.011761  0.489973  \n",
       "41078  0.071338 -0.774494  \n",
       "41079 -0.624205 -0.291191  \n",
       "41080 -0.909959 -1.041931  \n",
       "41081 -0.384326 -0.161193  \n",
       "41082 -0.149640 -0.030781  \n",
       "41083 -0.182170 -0.472858  \n",
       "41084  0.089716 -0.192997  \n",
       "41085  0.151814  0.220779  \n",
       "41086  0.329945 -0.021874  \n",
       "41087  0.175213  0.476639  \n",
       "41088 -0.568774 -0.827451  \n",
       "41089  0.078532 -0.127373  \n",
       "41090 -0.614163 -0.756393  \n",
       "41091 -0.573143 -0.633071  \n",
       "41092  0.008080 -0.376151  \n",
       "41093 -0.200008 -0.403299  \n",
       "41094  0.176027  0.389756  \n",
       "41095 -0.787548 -0.535877  \n",
       "41096  0.039506 -0.217876  \n",
       "41097 -0.350160 -1.225379  \n",
       "\n",
       "[41098 rows x 16 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[y_indices+x_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth for all 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvlscratch/SHI/users/hofman/ML/env/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 0, 1, 0, 0, 0, 3, 3, 1, 1, 3, 1, 0, 2, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['N', 'regP', 'regS', 'tele'])\n",
    "test_Y_GT = le.transform(test[y_indices])\n",
    "train_Y_GT = le.transform(train[y_indices])\n",
    "test_Y_GT[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = len(x_indices)\n",
    "numpy.random.seed(11)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=n_input, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1., clipvalue=0.5)\n",
    "adam = Adam(lr=0.0001) #, clipnorm, clipvalue=0.5)\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy', \n",
    "    optimizer = 'adam',  # adam, sgd\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 6)                 96        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_keys = ('acc', 'val_acc', 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {k : [] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41098 samples, validate on 13700 samples\n",
      "Epoch 1/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.7015 - acc: 0.4353 - val_loss: 0.6968 - val_acc: 0.4432\n",
      "Epoch 2/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.6952 - acc: 0.4490 - val_loss: 0.6930 - val_acc: 0.4691\n",
      "Epoch 3/500\n",
      "41098/41098 [==============================] - 0s 9us/step - loss: 0.6918 - acc: 0.5099 - val_loss: 0.6901 - val_acc: 0.5542\n",
      "Epoch 4/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.6882 - acc: 0.5903 - val_loss: 0.6863 - val_acc: 0.6161\n",
      "Epoch 5/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.6832 - acc: 0.6353 - val_loss: 0.6809 - val_acc: 0.6381\n",
      "Epoch 6/500\n",
      "41098/41098 [==============================] - 0s 9us/step - loss: 0.6762 - acc: 0.6419 - val_loss: 0.6737 - val_acc: 0.6369\n",
      "Epoch 7/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.6678 - acc: 0.6585 - val_loss: 0.6654 - val_acc: 0.6492\n",
      "Epoch 8/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.6584 - acc: 0.6660 - val_loss: 0.6561 - val_acc: 0.6571\n",
      "Epoch 9/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.6483 - acc: 0.6740 - val_loss: 0.6461 - val_acc: 0.6691\n",
      "Epoch 10/500\n",
      "41098/41098 [==============================] - 0s 9us/step - loss: 0.6375 - acc: 0.6831 - val_loss: 0.6356 - val_acc: 0.6755\n",
      "Epoch 11/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.6265 - acc: 0.6903 - val_loss: 0.6249 - val_acc: 0.6834\n",
      "Epoch 12/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.6155 - acc: 0.6958 - val_loss: 0.6142 - val_acc: 0.6889\n",
      "Epoch 13/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.6046 - acc: 0.7009 - val_loss: 0.6038 - val_acc: 0.6943\n",
      "Epoch 14/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.5940 - acc: 0.7060 - val_loss: 0.5937 - val_acc: 0.6999\n",
      "Epoch 15/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5838 - acc: 0.7117 - val_loss: 0.5841 - val_acc: 0.7043\n",
      "Epoch 16/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5742 - acc: 0.7180 - val_loss: 0.5749 - val_acc: 0.7116\n",
      "Epoch 17/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5651 - acc: 0.7236 - val_loss: 0.5663 - val_acc: 0.7164\n",
      "Epoch 18/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.5565 - acc: 0.7277 - val_loss: 0.5580 - val_acc: 0.7216\n",
      "Epoch 19/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5483 - acc: 0.7305 - val_loss: 0.5502 - val_acc: 0.7247\n",
      "Epoch 20/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5406 - acc: 0.7345 - val_loss: 0.5428 - val_acc: 0.7275\n",
      "Epoch 21/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5333 - acc: 0.7369 - val_loss: 0.5357 - val_acc: 0.7311\n",
      "Epoch 22/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.5263 - acc: 0.7400 - val_loss: 0.5291 - val_acc: 0.7357\n",
      "Epoch 23/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5198 - acc: 0.7442 - val_loss: 0.5228 - val_acc: 0.7402\n",
      "Epoch 24/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5136 - acc: 0.7481 - val_loss: 0.5169 - val_acc: 0.7452\n",
      "Epoch 25/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5078 - acc: 0.7540 - val_loss: 0.5113 - val_acc: 0.7522\n",
      "Epoch 26/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.5023 - acc: 0.7596 - val_loss: 0.5061 - val_acc: 0.7603\n",
      "Epoch 27/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4972 - acc: 0.7669 - val_loss: 0.5011 - val_acc: 0.7681\n",
      "Epoch 28/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4923 - acc: 0.7749 - val_loss: 0.4965 - val_acc: 0.7726\n",
      "Epoch 29/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4877 - acc: 0.7782 - val_loss: 0.4921 - val_acc: 0.7760\n",
      "Epoch 30/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4834 - acc: 0.7825 - val_loss: 0.4879 - val_acc: 0.7780\n",
      "Epoch 31/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4793 - acc: 0.7857 - val_loss: 0.4841 - val_acc: 0.7819\n",
      "Epoch 32/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4755 - acc: 0.7885 - val_loss: 0.4804 - val_acc: 0.7840\n",
      "Epoch 33/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4719 - acc: 0.7903 - val_loss: 0.4770 - val_acc: 0.7868\n",
      "Epoch 34/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4685 - acc: 0.7919 - val_loss: 0.4738 - val_acc: 0.7880\n",
      "Epoch 35/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4653 - acc: 0.7936 - val_loss: 0.4707 - val_acc: 0.7897\n",
      "Epoch 36/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4623 - acc: 0.7948 - val_loss: 0.4679 - val_acc: 0.7916\n",
      "Epoch 37/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4594 - acc: 0.7963 - val_loss: 0.4651 - val_acc: 0.7925\n",
      "Epoch 38/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4567 - acc: 0.7974 - val_loss: 0.4624 - val_acc: 0.7938\n",
      "Epoch 39/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4540 - acc: 0.7989 - val_loss: 0.4599 - val_acc: 0.7949\n",
      "Epoch 40/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4515 - acc: 0.7998 - val_loss: 0.4575 - val_acc: 0.7957\n",
      "Epoch 41/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4491 - acc: 0.8007 - val_loss: 0.4553 - val_acc: 0.7969\n",
      "Epoch 42/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4468 - acc: 0.8017 - val_loss: 0.4531 - val_acc: 0.7979\n",
      "Epoch 43/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4447 - acc: 0.8028 - val_loss: 0.4511 - val_acc: 0.7984\n",
      "Epoch 44/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4426 - acc: 0.8037 - val_loss: 0.4491 - val_acc: 0.7993\n",
      "Epoch 45/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4406 - acc: 0.8048 - val_loss: 0.4472 - val_acc: 0.7998\n",
      "Epoch 46/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4387 - acc: 0.8055 - val_loss: 0.4454 - val_acc: 0.8003\n",
      "Epoch 47/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4369 - acc: 0.8066 - val_loss: 0.4437 - val_acc: 0.8007\n",
      "Epoch 48/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4350 - acc: 0.8075 - val_loss: 0.4420 - val_acc: 0.8024\n",
      "Epoch 49/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4334 - acc: 0.8081 - val_loss: 0.4404 - val_acc: 0.8031\n",
      "Epoch 50/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4317 - acc: 0.8086 - val_loss: 0.4389 - val_acc: 0.8032\n",
      "Epoch 51/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4301 - acc: 0.8094 - val_loss: 0.4374 - val_acc: 0.8043\n",
      "Epoch 52/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4286 - acc: 0.8104 - val_loss: 0.4360 - val_acc: 0.8050\n",
      "Epoch 53/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4272 - acc: 0.8111 - val_loss: 0.4346 - val_acc: 0.8050\n",
      "Epoch 54/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4257 - acc: 0.8118 - val_loss: 0.4333 - val_acc: 0.8053\n",
      "Epoch 55/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4244 - acc: 0.8124 - val_loss: 0.4321 - val_acc: 0.8050\n",
      "Epoch 56/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4231 - acc: 0.8133 - val_loss: 0.4309 - val_acc: 0.8054\n",
      "Epoch 57/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4218 - acc: 0.8139 - val_loss: 0.4297 - val_acc: 0.8058\n",
      "Epoch 58/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4206 - acc: 0.8147 - val_loss: 0.4286 - val_acc: 0.8064\n",
      "Epoch 59/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4194 - acc: 0.8153 - val_loss: 0.4274 - val_acc: 0.8064\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4182 - acc: 0.8160 - val_loss: 0.4265 - val_acc: 0.8061\n",
      "Epoch 61/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4172 - acc: 0.8167 - val_loss: 0.4254 - val_acc: 0.8063\n",
      "Epoch 62/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4160 - acc: 0.8171 - val_loss: 0.4245 - val_acc: 0.8065\n",
      "Epoch 63/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4150 - acc: 0.8177 - val_loss: 0.4235 - val_acc: 0.8067\n",
      "Epoch 64/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4140 - acc: 0.8179 - val_loss: 0.4225 - val_acc: 0.8070\n",
      "Epoch 65/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4131 - acc: 0.8184 - val_loss: 0.4217 - val_acc: 0.8073\n",
      "Epoch 66/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4121 - acc: 0.8186 - val_loss: 0.4209 - val_acc: 0.8072\n",
      "Epoch 67/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4112 - acc: 0.8190 - val_loss: 0.4200 - val_acc: 0.8082\n",
      "Epoch 68/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4103 - acc: 0.8197 - val_loss: 0.4192 - val_acc: 0.8080\n",
      "Epoch 69/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4095 - acc: 0.8200 - val_loss: 0.4185 - val_acc: 0.8082\n",
      "Epoch 70/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4087 - acc: 0.8207 - val_loss: 0.4177 - val_acc: 0.8092\n",
      "Epoch 71/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4078 - acc: 0.8211 - val_loss: 0.4170 - val_acc: 0.8097\n",
      "Epoch 72/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.4070 - acc: 0.8212 - val_loss: 0.4163 - val_acc: 0.8101\n",
      "Epoch 73/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4063 - acc: 0.8213 - val_loss: 0.4156 - val_acc: 0.8101\n",
      "Epoch 74/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.4055 - acc: 0.8217 - val_loss: 0.4149 - val_acc: 0.8105\n",
      "Epoch 75/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4048 - acc: 0.8219 - val_loss: 0.4143 - val_acc: 0.8105\n",
      "Epoch 76/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4042 - acc: 0.8221 - val_loss: 0.4137 - val_acc: 0.8108\n",
      "Epoch 77/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4035 - acc: 0.8224 - val_loss: 0.4131 - val_acc: 0.8113\n",
      "Epoch 78/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4028 - acc: 0.8227 - val_loss: 0.4125 - val_acc: 0.8115\n",
      "Epoch 79/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4022 - acc: 0.8226 - val_loss: 0.4119 - val_acc: 0.8122\n",
      "Epoch 80/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4015 - acc: 0.8231 - val_loss: 0.4113 - val_acc: 0.8119\n",
      "Epoch 81/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.4009 - acc: 0.8228 - val_loss: 0.4108 - val_acc: 0.8126\n",
      "Epoch 82/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.4003 - acc: 0.8231 - val_loss: 0.4103 - val_acc: 0.8124\n",
      "Epoch 83/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3998 - acc: 0.8231 - val_loss: 0.4098 - val_acc: 0.8127\n",
      "Epoch 84/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3992 - acc: 0.8232 - val_loss: 0.4092 - val_acc: 0.8131\n",
      "Epoch 85/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3987 - acc: 0.8236 - val_loss: 0.4088 - val_acc: 0.8131\n",
      "Epoch 86/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3981 - acc: 0.8237 - val_loss: 0.4083 - val_acc: 0.8136\n",
      "Epoch 87/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3977 - acc: 0.8237 - val_loss: 0.4079 - val_acc: 0.8139\n",
      "Epoch 88/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3971 - acc: 0.8239 - val_loss: 0.4075 - val_acc: 0.8144\n",
      "Epoch 89/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3966 - acc: 0.8241 - val_loss: 0.4070 - val_acc: 0.8145\n",
      "Epoch 90/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3962 - acc: 0.8240 - val_loss: 0.4066 - val_acc: 0.8146\n",
      "Epoch 91/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3957 - acc: 0.8245 - val_loss: 0.4062 - val_acc: 0.8147\n",
      "Epoch 92/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3952 - acc: 0.8245 - val_loss: 0.4058 - val_acc: 0.8149\n",
      "Epoch 93/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3948 - acc: 0.8246 - val_loss: 0.4054 - val_acc: 0.8148\n",
      "Epoch 94/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3944 - acc: 0.8248 - val_loss: 0.4049 - val_acc: 0.8149\n",
      "Epoch 95/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3939 - acc: 0.8246 - val_loss: 0.4045 - val_acc: 0.8144\n",
      "Epoch 96/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3935 - acc: 0.8248 - val_loss: 0.4042 - val_acc: 0.8144\n",
      "Epoch 97/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3931 - acc: 0.8250 - val_loss: 0.4038 - val_acc: 0.8150\n",
      "Epoch 98/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3927 - acc: 0.8251 - val_loss: 0.4035 - val_acc: 0.8149\n",
      "Epoch 99/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3923 - acc: 0.8253 - val_loss: 0.4031 - val_acc: 0.8153\n",
      "Epoch 100/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3919 - acc: 0.8254 - val_loss: 0.4028 - val_acc: 0.8155\n",
      "Epoch 101/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3915 - acc: 0.8255 - val_loss: 0.4024 - val_acc: 0.8156\n",
      "Epoch 102/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3911 - acc: 0.8256 - val_loss: 0.4021 - val_acc: 0.8163\n",
      "Epoch 103/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3908 - acc: 0.8255 - val_loss: 0.4018 - val_acc: 0.8162\n",
      "Epoch 104/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3904 - acc: 0.8257 - val_loss: 0.4014 - val_acc: 0.8164\n",
      "Epoch 105/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3901 - acc: 0.8258 - val_loss: 0.4011 - val_acc: 0.8166\n",
      "Epoch 106/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3897 - acc: 0.8259 - val_loss: 0.4008 - val_acc: 0.8167\n",
      "Epoch 107/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3894 - acc: 0.8258 - val_loss: 0.4005 - val_acc: 0.8168\n",
      "Epoch 108/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3890 - acc: 0.8260 - val_loss: 0.4002 - val_acc: 0.8171\n",
      "Epoch 109/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3887 - acc: 0.8260 - val_loss: 0.4000 - val_acc: 0.8175\n",
      "Epoch 110/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3884 - acc: 0.8264 - val_loss: 0.3996 - val_acc: 0.8174\n",
      "Epoch 111/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3881 - acc: 0.8265 - val_loss: 0.3994 - val_acc: 0.8176\n",
      "Epoch 112/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3878 - acc: 0.8265 - val_loss: 0.3990 - val_acc: 0.8177\n",
      "Epoch 113/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3875 - acc: 0.8266 - val_loss: 0.3989 - val_acc: 0.8174\n",
      "Epoch 114/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3872 - acc: 0.8266 - val_loss: 0.3986 - val_acc: 0.8180\n",
      "Epoch 115/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3869 - acc: 0.8270 - val_loss: 0.3983 - val_acc: 0.8186\n",
      "Epoch 116/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3866 - acc: 0.8270 - val_loss: 0.3980 - val_acc: 0.8185\n",
      "Epoch 117/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3864 - acc: 0.8271 - val_loss: 0.3978 - val_acc: 0.8183\n",
      "Epoch 118/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3861 - acc: 0.8272 - val_loss: 0.3976 - val_acc: 0.8183\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3858 - acc: 0.8274 - val_loss: 0.3973 - val_acc: 0.8188\n",
      "Epoch 120/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3856 - acc: 0.8275 - val_loss: 0.3971 - val_acc: 0.8189\n",
      "Epoch 121/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3853 - acc: 0.8274 - val_loss: 0.3968 - val_acc: 0.8192\n",
      "Epoch 122/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3850 - acc: 0.8276 - val_loss: 0.3966 - val_acc: 0.8194\n",
      "Epoch 123/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3848 - acc: 0.8278 - val_loss: 0.3966 - val_acc: 0.8192\n",
      "Epoch 124/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3846 - acc: 0.8281 - val_loss: 0.3963 - val_acc: 0.8198\n",
      "Epoch 125/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3844 - acc: 0.8280 - val_loss: 0.3960 - val_acc: 0.8192\n",
      "Epoch 126/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3841 - acc: 0.8281 - val_loss: 0.3957 - val_acc: 0.8191\n",
      "Epoch 127/500\n",
      "41098/41098 [==============================] - 0s 9us/step - loss: 0.3839 - acc: 0.8284 - val_loss: 0.3954 - val_acc: 0.8199\n",
      "Epoch 128/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3837 - acc: 0.8285 - val_loss: 0.3953 - val_acc: 0.8199\n",
      "Epoch 129/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3834 - acc: 0.8284 - val_loss: 0.3951 - val_acc: 0.8199\n",
      "Epoch 130/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3832 - acc: 0.8285 - val_loss: 0.3949 - val_acc: 0.8197\n",
      "Epoch 131/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3830 - acc: 0.8285 - val_loss: 0.3947 - val_acc: 0.8198\n",
      "Epoch 132/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3828 - acc: 0.8285 - val_loss: 0.3945 - val_acc: 0.8199\n",
      "Epoch 133/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3825 - acc: 0.8285 - val_loss: 0.3944 - val_acc: 0.8208\n",
      "Epoch 134/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3823 - acc: 0.8289 - val_loss: 0.3941 - val_acc: 0.8200\n",
      "Epoch 135/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3821 - acc: 0.8291 - val_loss: 0.3939 - val_acc: 0.8201\n",
      "Epoch 136/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3819 - acc: 0.8292 - val_loss: 0.3938 - val_acc: 0.8209\n",
      "Epoch 137/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3817 - acc: 0.8292 - val_loss: 0.3935 - val_acc: 0.8205\n",
      "Epoch 138/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3815 - acc: 0.8292 - val_loss: 0.3934 - val_acc: 0.8204\n",
      "Epoch 139/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3813 - acc: 0.8293 - val_loss: 0.3932 - val_acc: 0.8208\n",
      "Epoch 140/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3811 - acc: 0.8294 - val_loss: 0.3930 - val_acc: 0.8209\n",
      "Epoch 141/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3810 - acc: 0.8294 - val_loss: 0.3929 - val_acc: 0.8214\n",
      "Epoch 142/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3808 - acc: 0.8296 - val_loss: 0.3927 - val_acc: 0.8214\n",
      "Epoch 143/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3806 - acc: 0.8296 - val_loss: 0.3925 - val_acc: 0.8211\n",
      "Epoch 144/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3804 - acc: 0.8296 - val_loss: 0.3924 - val_acc: 0.8214\n",
      "Epoch 145/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3802 - acc: 0.8300 - val_loss: 0.3922 - val_acc: 0.8214\n",
      "Epoch 146/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3801 - acc: 0.8299 - val_loss: 0.3921 - val_acc: 0.8210\n",
      "Epoch 147/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3799 - acc: 0.8302 - val_loss: 0.3920 - val_acc: 0.8217\n",
      "Epoch 148/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3797 - acc: 0.8298 - val_loss: 0.3917 - val_acc: 0.8218\n",
      "Epoch 149/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3796 - acc: 0.8303 - val_loss: 0.3916 - val_acc: 0.8219\n",
      "Epoch 150/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3794 - acc: 0.8303 - val_loss: 0.3916 - val_acc: 0.8216\n",
      "Epoch 151/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3793 - acc: 0.8304 - val_loss: 0.3914 - val_acc: 0.8225\n",
      "Epoch 152/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3791 - acc: 0.8304 - val_loss: 0.3913 - val_acc: 0.8218\n",
      "Epoch 153/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3789 - acc: 0.8305 - val_loss: 0.3909 - val_acc: 0.8218\n",
      "Epoch 154/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3788 - acc: 0.8306 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Epoch 155/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3786 - acc: 0.8307 - val_loss: 0.3908 - val_acc: 0.8224\n",
      "Epoch 156/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3785 - acc: 0.8305 - val_loss: 0.3906 - val_acc: 0.8224\n",
      "Epoch 157/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3783 - acc: 0.8306 - val_loss: 0.3905 - val_acc: 0.8219\n",
      "Epoch 158/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3782 - acc: 0.8306 - val_loss: 0.3903 - val_acc: 0.8226\n",
      "Epoch 159/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3780 - acc: 0.8307 - val_loss: 0.3903 - val_acc: 0.8224\n",
      "Epoch 160/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3779 - acc: 0.8307 - val_loss: 0.3902 - val_acc: 0.8223\n",
      "Epoch 161/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3777 - acc: 0.8308 - val_loss: 0.3899 - val_acc: 0.8227\n",
      "Epoch 162/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3776 - acc: 0.8307 - val_loss: 0.3899 - val_acc: 0.8223\n",
      "Epoch 163/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3774 - acc: 0.8307 - val_loss: 0.3897 - val_acc: 0.8226\n",
      "Epoch 164/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3773 - acc: 0.8310 - val_loss: 0.3896 - val_acc: 0.8226\n",
      "Epoch 165/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3772 - acc: 0.8309 - val_loss: 0.3895 - val_acc: 0.8227\n",
      "Epoch 166/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3770 - acc: 0.8309 - val_loss: 0.3893 - val_acc: 0.8227\n",
      "Epoch 167/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3769 - acc: 0.8312 - val_loss: 0.3892 - val_acc: 0.8231\n",
      "Epoch 168/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3768 - acc: 0.8311 - val_loss: 0.3890 - val_acc: 0.8233\n",
      "Epoch 169/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3766 - acc: 0.8312 - val_loss: 0.3890 - val_acc: 0.8229\n",
      "Epoch 170/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3765 - acc: 0.8313 - val_loss: 0.3888 - val_acc: 0.8231\n",
      "Epoch 171/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3764 - acc: 0.8313 - val_loss: 0.3886 - val_acc: 0.8233\n",
      "Epoch 172/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3762 - acc: 0.8312 - val_loss: 0.3885 - val_acc: 0.8234\n",
      "Epoch 173/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3761 - acc: 0.8314 - val_loss: 0.3883 - val_acc: 0.8234\n",
      "Epoch 174/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3760 - acc: 0.8316 - val_loss: 0.3884 - val_acc: 0.8234\n",
      "Epoch 175/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3759 - acc: 0.8314 - val_loss: 0.3883 - val_acc: 0.8232\n",
      "Epoch 176/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3758 - acc: 0.8314 - val_loss: 0.3882 - val_acc: 0.8234\n",
      "Epoch 177/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3756 - acc: 0.8316 - val_loss: 0.3880 - val_acc: 0.8235\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3755 - acc: 0.8314 - val_loss: 0.3880 - val_acc: 0.8236\n",
      "Epoch 179/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3754 - acc: 0.8316 - val_loss: 0.3877 - val_acc: 0.8234\n",
      "Epoch 180/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3753 - acc: 0.8319 - val_loss: 0.3877 - val_acc: 0.8239\n",
      "Epoch 181/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3752 - acc: 0.8316 - val_loss: 0.3878 - val_acc: 0.8234\n",
      "Epoch 182/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3750 - acc: 0.8321 - val_loss: 0.3875 - val_acc: 0.8238\n",
      "Epoch 183/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3749 - acc: 0.8319 - val_loss: 0.3873 - val_acc: 0.8238\n",
      "Epoch 184/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3748 - acc: 0.8319 - val_loss: 0.3873 - val_acc: 0.8242\n",
      "Epoch 185/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3747 - acc: 0.8322 - val_loss: 0.3871 - val_acc: 0.8241\n",
      "Epoch 186/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3746 - acc: 0.8319 - val_loss: 0.3871 - val_acc: 0.8243\n",
      "Epoch 187/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3745 - acc: 0.8320 - val_loss: 0.3870 - val_acc: 0.8241\n",
      "Epoch 188/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3744 - acc: 0.8319 - val_loss: 0.3869 - val_acc: 0.8246\n",
      "Epoch 189/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3743 - acc: 0.8320 - val_loss: 0.3868 - val_acc: 0.8248\n",
      "Epoch 190/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3742 - acc: 0.8323 - val_loss: 0.3867 - val_acc: 0.8241\n",
      "Epoch 191/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3741 - acc: 0.8321 - val_loss: 0.3866 - val_acc: 0.8246\n",
      "Epoch 192/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3740 - acc: 0.8322 - val_loss: 0.3866 - val_acc: 0.8246\n",
      "Epoch 193/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3738 - acc: 0.8325 - val_loss: 0.3863 - val_acc: 0.8247\n",
      "Epoch 194/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3737 - acc: 0.8321 - val_loss: 0.3863 - val_acc: 0.8251\n",
      "Epoch 195/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3737 - acc: 0.8322 - val_loss: 0.3864 - val_acc: 0.8243\n",
      "Epoch 196/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3736 - acc: 0.8325 - val_loss: 0.3862 - val_acc: 0.8245\n",
      "Epoch 197/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3734 - acc: 0.8325 - val_loss: 0.3860 - val_acc: 0.8253\n",
      "Epoch 198/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3733 - acc: 0.8325 - val_loss: 0.3859 - val_acc: 0.8249\n",
      "Epoch 199/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3732 - acc: 0.8325 - val_loss: 0.3859 - val_acc: 0.8253\n",
      "Epoch 200/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3732 - acc: 0.8324 - val_loss: 0.3858 - val_acc: 0.8251\n",
      "Epoch 201/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3731 - acc: 0.8325 - val_loss: 0.3857 - val_acc: 0.8250\n",
      "Epoch 202/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3729 - acc: 0.8327 - val_loss: 0.3855 - val_acc: 0.8254\n",
      "Epoch 203/500\n",
      "41098/41098 [==============================] - 0s 10us/step - loss: 0.3729 - acc: 0.8323 - val_loss: 0.3856 - val_acc: 0.8252\n",
      "Epoch 204/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3728 - acc: 0.8328 - val_loss: 0.3854 - val_acc: 0.8254\n",
      "Epoch 205/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3727 - acc: 0.8329 - val_loss: 0.3852 - val_acc: 0.8251\n",
      "Epoch 206/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3726 - acc: 0.8326 - val_loss: 0.3852 - val_acc: 0.8253\n",
      "Epoch 207/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3725 - acc: 0.8327 - val_loss: 0.3852 - val_acc: 0.8255\n",
      "Epoch 208/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3724 - acc: 0.8331 - val_loss: 0.3852 - val_acc: 0.8254\n",
      "Epoch 209/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3723 - acc: 0.8329 - val_loss: 0.3852 - val_acc: 0.8254\n",
      "Epoch 210/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3722 - acc: 0.8330 - val_loss: 0.3849 - val_acc: 0.8255\n",
      "Epoch 211/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3721 - acc: 0.8330 - val_loss: 0.3849 - val_acc: 0.8256\n",
      "Epoch 212/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3720 - acc: 0.8329 - val_loss: 0.3849 - val_acc: 0.8260\n",
      "Epoch 213/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3720 - acc: 0.8331 - val_loss: 0.3848 - val_acc: 0.8261\n",
      "Epoch 214/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3719 - acc: 0.8333 - val_loss: 0.3846 - val_acc: 0.8260\n",
      "Epoch 215/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3718 - acc: 0.8332 - val_loss: 0.3846 - val_acc: 0.8264\n",
      "Epoch 216/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3717 - acc: 0.8333 - val_loss: 0.3844 - val_acc: 0.8256\n",
      "Epoch 217/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3717 - acc: 0.8332 - val_loss: 0.3845 - val_acc: 0.8264\n",
      "Epoch 218/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3716 - acc: 0.8333 - val_loss: 0.3845 - val_acc: 0.8263\n",
      "Epoch 219/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3714 - acc: 0.8332 - val_loss: 0.3843 - val_acc: 0.8258\n",
      "Epoch 220/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3714 - acc: 0.8331 - val_loss: 0.3843 - val_acc: 0.8259\n",
      "Epoch 221/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3713 - acc: 0.8333 - val_loss: 0.3841 - val_acc: 0.8257\n",
      "Epoch 222/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3712 - acc: 0.8337 - val_loss: 0.3840 - val_acc: 0.8258\n",
      "Epoch 223/500\n",
      "41098/41098 [==============================] - ETA: 0s - loss: 0.3711 - acc: 0.833 - 0s 6us/step - loss: 0.3711 - acc: 0.8335 - val_loss: 0.3840 - val_acc: 0.8264\n",
      "Epoch 224/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3710 - acc: 0.8337 - val_loss: 0.3839 - val_acc: 0.8262\n",
      "Epoch 225/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3710 - acc: 0.8335 - val_loss: 0.3840 - val_acc: 0.8266\n",
      "Epoch 226/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3709 - acc: 0.8336 - val_loss: 0.3838 - val_acc: 0.8261\n",
      "Epoch 227/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3708 - acc: 0.8334 - val_loss: 0.3837 - val_acc: 0.8265\n",
      "Epoch 228/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3708 - acc: 0.8334 - val_loss: 0.3837 - val_acc: 0.8264\n",
      "Epoch 229/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3707 - acc: 0.8336 - val_loss: 0.3836 - val_acc: 0.8264\n",
      "Epoch 230/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3706 - acc: 0.8339 - val_loss: 0.3835 - val_acc: 0.8264\n",
      "Epoch 231/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3706 - acc: 0.8336 - val_loss: 0.3835 - val_acc: 0.8264\n",
      "Epoch 232/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3705 - acc: 0.8342 - val_loss: 0.3833 - val_acc: 0.8264\n",
      "Epoch 233/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3704 - acc: 0.8339 - val_loss: 0.3834 - val_acc: 0.8261\n",
      "Epoch 234/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3703 - acc: 0.8341 - val_loss: 0.3833 - val_acc: 0.8263\n",
      "Epoch 235/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3702 - acc: 0.8339 - val_loss: 0.3832 - val_acc: 0.8265\n",
      "Epoch 236/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3702 - acc: 0.8338 - val_loss: 0.3832 - val_acc: 0.8264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3701 - acc: 0.8342 - val_loss: 0.3831 - val_acc: 0.8262\n",
      "Epoch 238/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3700 - acc: 0.8342 - val_loss: 0.3830 - val_acc: 0.8263\n",
      "Epoch 239/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3700 - acc: 0.8341 - val_loss: 0.3830 - val_acc: 0.8264\n",
      "Epoch 240/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3699 - acc: 0.8343 - val_loss: 0.3830 - val_acc: 0.8261\n",
      "Epoch 241/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3698 - acc: 0.8347 - val_loss: 0.3828 - val_acc: 0.8264\n",
      "Epoch 242/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3698 - acc: 0.8343 - val_loss: 0.3828 - val_acc: 0.8263\n",
      "Epoch 243/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3697 - acc: 0.8341 - val_loss: 0.3828 - val_acc: 0.8263\n",
      "Epoch 244/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3697 - acc: 0.8346 - val_loss: 0.3827 - val_acc: 0.8262\n",
      "Epoch 245/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3696 - acc: 0.8349 - val_loss: 0.3826 - val_acc: 0.8264\n",
      "Epoch 246/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3695 - acc: 0.8348 - val_loss: 0.3826 - val_acc: 0.8263\n",
      "Epoch 247/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3695 - acc: 0.8347 - val_loss: 0.3826 - val_acc: 0.8262\n",
      "Epoch 248/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3695 - acc: 0.8347 - val_loss: 0.3825 - val_acc: 0.8259\n",
      "Epoch 249/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3693 - acc: 0.8351 - val_loss: 0.3824 - val_acc: 0.8264\n",
      "Epoch 250/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3692 - acc: 0.8348 - val_loss: 0.3824 - val_acc: 0.8261\n",
      "Epoch 251/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3692 - acc: 0.8349 - val_loss: 0.3823 - val_acc: 0.8260\n",
      "Epoch 252/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3692 - acc: 0.8347 - val_loss: 0.3821 - val_acc: 0.8266\n",
      "Epoch 253/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3691 - acc: 0.8350 - val_loss: 0.3822 - val_acc: 0.8262\n",
      "Epoch 254/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3690 - acc: 0.8351 - val_loss: 0.3822 - val_acc: 0.8264\n",
      "Epoch 255/500\n",
      "41098/41098 [==============================] - 0s 9us/step - loss: 0.3689 - acc: 0.8350 - val_loss: 0.3821 - val_acc: 0.8259\n",
      "Epoch 256/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3689 - acc: 0.8352 - val_loss: 0.3821 - val_acc: 0.8259\n",
      "Epoch 257/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3688 - acc: 0.8350 - val_loss: 0.3820 - val_acc: 0.8261\n",
      "Epoch 258/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3687 - acc: 0.8351 - val_loss: 0.3819 - val_acc: 0.8262\n",
      "Epoch 259/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3688 - acc: 0.8350 - val_loss: 0.3818 - val_acc: 0.8266\n",
      "Epoch 260/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3687 - acc: 0.8352 - val_loss: 0.3818 - val_acc: 0.8263\n",
      "Epoch 261/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3686 - acc: 0.8353 - val_loss: 0.3817 - val_acc: 0.8261\n",
      "Epoch 262/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3685 - acc: 0.8354 - val_loss: 0.3817 - val_acc: 0.8262\n",
      "Epoch 263/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3685 - acc: 0.8353 - val_loss: 0.3817 - val_acc: 0.8263\n",
      "Epoch 264/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3684 - acc: 0.8355 - val_loss: 0.3816 - val_acc: 0.8264\n",
      "Epoch 265/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3684 - acc: 0.8351 - val_loss: 0.3815 - val_acc: 0.8262\n",
      "Epoch 266/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3683 - acc: 0.8352 - val_loss: 0.3814 - val_acc: 0.8264\n",
      "Epoch 267/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3683 - acc: 0.8353 - val_loss: 0.3814 - val_acc: 0.8264\n",
      "Epoch 268/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3682 - acc: 0.8353 - val_loss: 0.3814 - val_acc: 0.8265\n",
      "Epoch 269/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3681 - acc: 0.8352 - val_loss: 0.3813 - val_acc: 0.8264\n",
      "Epoch 270/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3681 - acc: 0.8356 - val_loss: 0.3813 - val_acc: 0.8264\n",
      "Epoch 271/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3681 - acc: 0.8355 - val_loss: 0.3813 - val_acc: 0.8265\n",
      "Epoch 272/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3680 - acc: 0.8354 - val_loss: 0.3812 - val_acc: 0.8264\n",
      "Epoch 273/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3680 - acc: 0.8354 - val_loss: 0.3812 - val_acc: 0.8265\n",
      "Epoch 274/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3679 - acc: 0.8356 - val_loss: 0.3811 - val_acc: 0.8269\n",
      "Epoch 275/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3678 - acc: 0.8354 - val_loss: 0.3810 - val_acc: 0.8267\n",
      "Epoch 276/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3678 - acc: 0.8357 - val_loss: 0.3811 - val_acc: 0.8266\n",
      "Epoch 277/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3677 - acc: 0.8356 - val_loss: 0.3810 - val_acc: 0.8269\n",
      "Epoch 278/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3677 - acc: 0.8358 - val_loss: 0.3809 - val_acc: 0.8267\n",
      "Epoch 279/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3677 - acc: 0.8361 - val_loss: 0.3810 - val_acc: 0.8264\n",
      "Epoch 280/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3676 - acc: 0.8360 - val_loss: 0.3808 - val_acc: 0.8271\n",
      "Epoch 281/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3676 - acc: 0.8359 - val_loss: 0.3808 - val_acc: 0.8267\n",
      "Epoch 282/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3675 - acc: 0.8361 - val_loss: 0.3807 - val_acc: 0.8268\n",
      "Epoch 283/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3675 - acc: 0.8357 - val_loss: 0.3807 - val_acc: 0.8269\n",
      "Epoch 284/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3674 - acc: 0.8358 - val_loss: 0.3807 - val_acc: 0.8266\n",
      "Epoch 285/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3673 - acc: 0.8360 - val_loss: 0.3806 - val_acc: 0.8273\n",
      "Epoch 286/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3673 - acc: 0.8360 - val_loss: 0.3805 - val_acc: 0.8274\n",
      "Epoch 287/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3672 - acc: 0.8358 - val_loss: 0.3805 - val_acc: 0.8275\n",
      "Epoch 288/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3672 - acc: 0.8363 - val_loss: 0.3805 - val_acc: 0.8274\n",
      "Epoch 289/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3671 - acc: 0.8362 - val_loss: 0.3805 - val_acc: 0.8267\n",
      "Epoch 290/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3671 - acc: 0.8363 - val_loss: 0.3804 - val_acc: 0.8270\n",
      "Epoch 291/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3671 - acc: 0.8363 - val_loss: 0.3803 - val_acc: 0.8276\n",
      "Epoch 292/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3670 - acc: 0.8363 - val_loss: 0.3803 - val_acc: 0.8274\n",
      "Epoch 293/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3670 - acc: 0.8361 - val_loss: 0.3802 - val_acc: 0.8273\n",
      "Epoch 294/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3669 - acc: 0.8363 - val_loss: 0.3802 - val_acc: 0.8273\n",
      "Epoch 295/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3669 - acc: 0.8362 - val_loss: 0.3802 - val_acc: 0.8275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3668 - acc: 0.8364 - val_loss: 0.3802 - val_acc: 0.8277\n",
      "Epoch 297/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3668 - acc: 0.8363 - val_loss: 0.3801 - val_acc: 0.8275\n",
      "Epoch 298/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3667 - acc: 0.8361 - val_loss: 0.3801 - val_acc: 0.8275\n",
      "Epoch 299/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3667 - acc: 0.8363 - val_loss: 0.3800 - val_acc: 0.8278\n",
      "Epoch 300/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3666 - acc: 0.8363 - val_loss: 0.3800 - val_acc: 0.8279\n",
      "Epoch 301/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3666 - acc: 0.8365 - val_loss: 0.3800 - val_acc: 0.8276\n",
      "Epoch 302/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3666 - acc: 0.8364 - val_loss: 0.3799 - val_acc: 0.8276\n",
      "Epoch 303/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3665 - acc: 0.8363 - val_loss: 0.3799 - val_acc: 0.8276\n",
      "Epoch 304/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3665 - acc: 0.8363 - val_loss: 0.3799 - val_acc: 0.8277\n",
      "Epoch 305/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3665 - acc: 0.8364 - val_loss: 0.3798 - val_acc: 0.8277\n",
      "Epoch 306/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3664 - acc: 0.8363 - val_loss: 0.3796 - val_acc: 0.8274\n",
      "Epoch 307/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3664 - acc: 0.8362 - val_loss: 0.3797 - val_acc: 0.8277\n",
      "Epoch 308/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3663 - acc: 0.8363 - val_loss: 0.3798 - val_acc: 0.8279\n",
      "Epoch 309/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3662 - acc: 0.8365 - val_loss: 0.3796 - val_acc: 0.8276\n",
      "Epoch 310/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3662 - acc: 0.8367 - val_loss: 0.3796 - val_acc: 0.8276\n",
      "Epoch 311/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3662 - acc: 0.8365 - val_loss: 0.3796 - val_acc: 0.8276\n",
      "Epoch 312/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3661 - acc: 0.8367 - val_loss: 0.3794 - val_acc: 0.8275\n",
      "Epoch 313/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3661 - acc: 0.8366 - val_loss: 0.3795 - val_acc: 0.8278\n",
      "Epoch 314/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3661 - acc: 0.8368 - val_loss: 0.3793 - val_acc: 0.8272\n",
      "Epoch 315/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3661 - acc: 0.8367 - val_loss: 0.3793 - val_acc: 0.8274\n",
      "Epoch 316/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3660 - acc: 0.8366 - val_loss: 0.3792 - val_acc: 0.8274\n",
      "Epoch 317/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3659 - acc: 0.8364 - val_loss: 0.3795 - val_acc: 0.8274\n",
      "Epoch 318/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3660 - acc: 0.8365 - val_loss: 0.3792 - val_acc: 0.8276\n",
      "Epoch 319/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3659 - acc: 0.8367 - val_loss: 0.3792 - val_acc: 0.8278\n",
      "Epoch 320/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3658 - acc: 0.8366 - val_loss: 0.3793 - val_acc: 0.8273\n",
      "Epoch 321/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3658 - acc: 0.8366 - val_loss: 0.3791 - val_acc: 0.8276\n",
      "Epoch 322/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3657 - acc: 0.8368 - val_loss: 0.3790 - val_acc: 0.8278\n",
      "Epoch 323/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3657 - acc: 0.8366 - val_loss: 0.3791 - val_acc: 0.8275\n",
      "Epoch 324/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3657 - acc: 0.8368 - val_loss: 0.3790 - val_acc: 0.8278\n",
      "Epoch 325/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3657 - acc: 0.8365 - val_loss: 0.3789 - val_acc: 0.8282\n",
      "Epoch 326/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3657 - acc: 0.8370 - val_loss: 0.3789 - val_acc: 0.8279\n",
      "Epoch 327/500\n",
      "41098/41098 [==============================] - ETA: 0s - loss: 0.3660 - acc: 0.836 - 0s 5us/step - loss: 0.3656 - acc: 0.8368 - val_loss: 0.3790 - val_acc: 0.8275\n",
      "Epoch 328/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3655 - acc: 0.8368 - val_loss: 0.3789 - val_acc: 0.8276\n",
      "Epoch 329/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3655 - acc: 0.8370 - val_loss: 0.3788 - val_acc: 0.8281\n",
      "Epoch 330/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3655 - acc: 0.8369 - val_loss: 0.3788 - val_acc: 0.8281\n",
      "Epoch 331/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3654 - acc: 0.8365 - val_loss: 0.3787 - val_acc: 0.8277\n",
      "Epoch 332/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3654 - acc: 0.8368 - val_loss: 0.3789 - val_acc: 0.8273\n",
      "Epoch 333/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3653 - acc: 0.8368 - val_loss: 0.3787 - val_acc: 0.8280\n",
      "Epoch 334/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3653 - acc: 0.8368 - val_loss: 0.3786 - val_acc: 0.8273\n",
      "Epoch 335/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3653 - acc: 0.8371 - val_loss: 0.3786 - val_acc: 0.8275\n",
      "Epoch 336/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3652 - acc: 0.8371 - val_loss: 0.3786 - val_acc: 0.8277\n",
      "Epoch 337/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3652 - acc: 0.8367 - val_loss: 0.3786 - val_acc: 0.8281\n",
      "Epoch 338/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3652 - acc: 0.8371 - val_loss: 0.3785 - val_acc: 0.8281\n",
      "Epoch 339/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3651 - acc: 0.8375 - val_loss: 0.3784 - val_acc: 0.8281\n",
      "Epoch 340/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3651 - acc: 0.8371 - val_loss: 0.3784 - val_acc: 0.8278\n",
      "Epoch 341/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3651 - acc: 0.8369 - val_loss: 0.3784 - val_acc: 0.8280\n",
      "Epoch 342/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3650 - acc: 0.8370 - val_loss: 0.3782 - val_acc: 0.8278\n",
      "Epoch 343/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3650 - acc: 0.8373 - val_loss: 0.3782 - val_acc: 0.8281\n",
      "Epoch 344/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3650 - acc: 0.8374 - val_loss: 0.3783 - val_acc: 0.8277\n",
      "Epoch 345/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3649 - acc: 0.8371 - val_loss: 0.3783 - val_acc: 0.8280\n",
      "Epoch 346/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3649 - acc: 0.8369 - val_loss: 0.3782 - val_acc: 0.8279\n",
      "Epoch 347/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3649 - acc: 0.8374 - val_loss: 0.3782 - val_acc: 0.8281\n",
      "Epoch 348/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3648 - acc: 0.8372 - val_loss: 0.3781 - val_acc: 0.8280\n",
      "Epoch 349/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3648 - acc: 0.8370 - val_loss: 0.3781 - val_acc: 0.8282\n",
      "Epoch 350/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3648 - acc: 0.8369 - val_loss: 0.3782 - val_acc: 0.8282\n",
      "Epoch 351/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3647 - acc: 0.8373 - val_loss: 0.3780 - val_acc: 0.8283\n",
      "Epoch 352/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3647 - acc: 0.8373 - val_loss: 0.3779 - val_acc: 0.8283\n",
      "Epoch 353/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3646 - acc: 0.8372 - val_loss: 0.3778 - val_acc: 0.8281\n",
      "Epoch 354/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3646 - acc: 0.8371 - val_loss: 0.3779 - val_acc: 0.8282\n",
      "Epoch 355/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3646 - acc: 0.8373 - val_loss: 0.3778 - val_acc: 0.8284\n",
      "Epoch 356/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3645 - acc: 0.8371 - val_loss: 0.3778 - val_acc: 0.8285\n",
      "Epoch 357/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3645 - acc: 0.8372 - val_loss: 0.3777 - val_acc: 0.8286\n",
      "Epoch 358/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3645 - acc: 0.8377 - val_loss: 0.3777 - val_acc: 0.8287\n",
      "Epoch 359/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3644 - acc: 0.8373 - val_loss: 0.3776 - val_acc: 0.8288\n",
      "Epoch 360/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3644 - acc: 0.8373 - val_loss: 0.3777 - val_acc: 0.8286\n",
      "Epoch 361/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3644 - acc: 0.8370 - val_loss: 0.3776 - val_acc: 0.8288\n",
      "Epoch 362/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3643 - acc: 0.8375 - val_loss: 0.3776 - val_acc: 0.8288\n",
      "Epoch 363/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3643 - acc: 0.8373 - val_loss: 0.3776 - val_acc: 0.8289\n",
      "Epoch 364/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3643 - acc: 0.8374 - val_loss: 0.3775 - val_acc: 0.8289\n",
      "Epoch 365/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3643 - acc: 0.8374 - val_loss: 0.3776 - val_acc: 0.8292\n",
      "Epoch 366/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3642 - acc: 0.8376 - val_loss: 0.3774 - val_acc: 0.8288\n",
      "Epoch 367/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3642 - acc: 0.8379 - val_loss: 0.3775 - val_acc: 0.8291\n",
      "Epoch 368/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3642 - acc: 0.8376 - val_loss: 0.3774 - val_acc: 0.8291\n",
      "Epoch 369/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3641 - acc: 0.8377 - val_loss: 0.3773 - val_acc: 0.8289\n",
      "Epoch 370/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3640 - acc: 0.8378 - val_loss: 0.3773 - val_acc: 0.8289\n",
      "Epoch 371/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3640 - acc: 0.8379 - val_loss: 0.3772 - val_acc: 0.8292\n",
      "Epoch 372/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3640 - acc: 0.8377 - val_loss: 0.3772 - val_acc: 0.8292\n",
      "Epoch 373/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3640 - acc: 0.8378 - val_loss: 0.3771 - val_acc: 0.8289\n",
      "Epoch 374/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3640 - acc: 0.8380 - val_loss: 0.3773 - val_acc: 0.8292\n",
      "Epoch 375/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3640 - acc: 0.8379 - val_loss: 0.3772 - val_acc: 0.8292\n",
      "Epoch 376/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3639 - acc: 0.8379 - val_loss: 0.3772 - val_acc: 0.8293\n",
      "Epoch 377/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3638 - acc: 0.8380 - val_loss: 0.3771 - val_acc: 0.8293\n",
      "Epoch 378/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3638 - acc: 0.8376 - val_loss: 0.3770 - val_acc: 0.8290\n",
      "Epoch 379/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3638 - acc: 0.8378 - val_loss: 0.3770 - val_acc: 0.8292\n",
      "Epoch 380/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3638 - acc: 0.8377 - val_loss: 0.3770 - val_acc: 0.8294\n",
      "Epoch 381/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3638 - acc: 0.8376 - val_loss: 0.3768 - val_acc: 0.8292\n",
      "Epoch 382/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3638 - acc: 0.8377 - val_loss: 0.3769 - val_acc: 0.8295\n",
      "Epoch 383/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3637 - acc: 0.8379 - val_loss: 0.3767 - val_acc: 0.8293\n",
      "Epoch 384/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3636 - acc: 0.8378 - val_loss: 0.3768 - val_acc: 0.8297\n",
      "Epoch 385/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3636 - acc: 0.8380 - val_loss: 0.3767 - val_acc: 0.8293\n",
      "Epoch 386/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3636 - acc: 0.8381 - val_loss: 0.3768 - val_acc: 0.8296\n",
      "Epoch 387/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3636 - acc: 0.8381 - val_loss: 0.3768 - val_acc: 0.8297\n",
      "Epoch 388/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3635 - acc: 0.8382 - val_loss: 0.3766 - val_acc: 0.8294\n",
      "Epoch 389/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3635 - acc: 0.8379 - val_loss: 0.3767 - val_acc: 0.8295\n",
      "Epoch 390/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3635 - acc: 0.8380 - val_loss: 0.3765 - val_acc: 0.8297\n",
      "Epoch 391/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3635 - acc: 0.8379 - val_loss: 0.3765 - val_acc: 0.8297\n",
      "Epoch 392/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3634 - acc: 0.8381 - val_loss: 0.3765 - val_acc: 0.8296\n",
      "Epoch 393/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3634 - acc: 0.8382 - val_loss: 0.3765 - val_acc: 0.8299\n",
      "Epoch 394/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3634 - acc: 0.8380 - val_loss: 0.3764 - val_acc: 0.8298\n",
      "Epoch 395/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3633 - acc: 0.8380 - val_loss: 0.3765 - val_acc: 0.8298\n",
      "Epoch 396/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3633 - acc: 0.8380 - val_loss: 0.3764 - val_acc: 0.8295\n",
      "Epoch 397/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3633 - acc: 0.8380 - val_loss: 0.3763 - val_acc: 0.8294\n",
      "Epoch 398/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3632 - acc: 0.8380 - val_loss: 0.3763 - val_acc: 0.8304\n",
      "Epoch 399/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3632 - acc: 0.8382 - val_loss: 0.3764 - val_acc: 0.8304\n",
      "Epoch 400/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3632 - acc: 0.8384 - val_loss: 0.3762 - val_acc: 0.8298\n",
      "Epoch 401/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3631 - acc: 0.8385 - val_loss: 0.3761 - val_acc: 0.8298\n",
      "Epoch 402/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3631 - acc: 0.8387 - val_loss: 0.3764 - val_acc: 0.8304\n",
      "Epoch 403/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3631 - acc: 0.8386 - val_loss: 0.3761 - val_acc: 0.8300\n",
      "Epoch 404/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3631 - acc: 0.8383 - val_loss: 0.3761 - val_acc: 0.8300\n",
      "Epoch 405/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3630 - acc: 0.8383 - val_loss: 0.3761 - val_acc: 0.8308\n",
      "Epoch 406/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3630 - acc: 0.8382 - val_loss: 0.3761 - val_acc: 0.8304\n",
      "Epoch 407/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3629 - acc: 0.8385 - val_loss: 0.3760 - val_acc: 0.8303\n",
      "Epoch 408/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3630 - acc: 0.8386 - val_loss: 0.3760 - val_acc: 0.8304\n",
      "Epoch 409/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3630 - acc: 0.8384 - val_loss: 0.3760 - val_acc: 0.8305\n",
      "Epoch 410/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3629 - acc: 0.8382 - val_loss: 0.3760 - val_acc: 0.8304\n",
      "Epoch 411/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3628 - acc: 0.8387 - val_loss: 0.3758 - val_acc: 0.8303\n",
      "Epoch 412/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3629 - acc: 0.8385 - val_loss: 0.3758 - val_acc: 0.8307\n",
      "Epoch 413/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3628 - acc: 0.8384 - val_loss: 0.3758 - val_acc: 0.8306\n",
      "Epoch 414/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3628 - acc: 0.8383 - val_loss: 0.3757 - val_acc: 0.8306\n",
      "Epoch 415/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3627 - acc: 0.8388 - val_loss: 0.3758 - val_acc: 0.8304\n",
      "Epoch 416/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3627 - acc: 0.8385 - val_loss: 0.3757 - val_acc: 0.8303\n",
      "Epoch 417/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3627 - acc: 0.8388 - val_loss: 0.3758 - val_acc: 0.8305\n",
      "Epoch 418/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3627 - acc: 0.8386 - val_loss: 0.3756 - val_acc: 0.8304\n",
      "Epoch 419/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3626 - acc: 0.8386 - val_loss: 0.3756 - val_acc: 0.8301\n",
      "Epoch 420/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3626 - acc: 0.8387 - val_loss: 0.3755 - val_acc: 0.8305\n",
      "Epoch 421/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3626 - acc: 0.8388 - val_loss: 0.3755 - val_acc: 0.8304\n",
      "Epoch 422/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3626 - acc: 0.8387 - val_loss: 0.3755 - val_acc: 0.8307\n",
      "Epoch 423/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3625 - acc: 0.8387 - val_loss: 0.3754 - val_acc: 0.8308\n",
      "Epoch 424/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3625 - acc: 0.8388 - val_loss: 0.3754 - val_acc: 0.8309\n",
      "Epoch 425/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3625 - acc: 0.8389 - val_loss: 0.3753 - val_acc: 0.8307\n",
      "Epoch 426/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3624 - acc: 0.8389 - val_loss: 0.3753 - val_acc: 0.8312\n",
      "Epoch 427/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3624 - acc: 0.8389 - val_loss: 0.3754 - val_acc: 0.8305\n",
      "Epoch 428/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3624 - acc: 0.8385 - val_loss: 0.3751 - val_acc: 0.8307\n",
      "Epoch 429/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3624 - acc: 0.8383 - val_loss: 0.3755 - val_acc: 0.8305\n",
      "Epoch 430/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3623 - acc: 0.8390 - val_loss: 0.3752 - val_acc: 0.8312\n",
      "Epoch 431/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3623 - acc: 0.8388 - val_loss: 0.3751 - val_acc: 0.8310\n",
      "Epoch 432/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3623 - acc: 0.8387 - val_loss: 0.3753 - val_acc: 0.8307\n",
      "Epoch 433/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3622 - acc: 0.8389 - val_loss: 0.3750 - val_acc: 0.8312\n",
      "Epoch 434/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3622 - acc: 0.8391 - val_loss: 0.3751 - val_acc: 0.8310\n",
      "Epoch 435/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3622 - acc: 0.8391 - val_loss: 0.3750 - val_acc: 0.8310\n",
      "Epoch 436/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3622 - acc: 0.8388 - val_loss: 0.3751 - val_acc: 0.8307\n",
      "Epoch 437/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3622 - acc: 0.8390 - val_loss: 0.3749 - val_acc: 0.8312\n",
      "Epoch 438/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3621 - acc: 0.8389 - val_loss: 0.3749 - val_acc: 0.8311\n",
      "Epoch 439/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3621 - acc: 0.8389 - val_loss: 0.3749 - val_acc: 0.8310\n",
      "Epoch 440/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3620 - acc: 0.8390 - val_loss: 0.3748 - val_acc: 0.8316\n",
      "Epoch 441/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3620 - acc: 0.8391 - val_loss: 0.3748 - val_acc: 0.8309\n",
      "Epoch 442/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3620 - acc: 0.8389 - val_loss: 0.3748 - val_acc: 0.8310\n",
      "Epoch 443/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3619 - acc: 0.8391 - val_loss: 0.3748 - val_acc: 0.8309\n",
      "Epoch 444/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3619 - acc: 0.8392 - val_loss: 0.3748 - val_acc: 0.8309\n",
      "Epoch 445/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3619 - acc: 0.8389 - val_loss: 0.3747 - val_acc: 0.8315\n",
      "Epoch 446/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3619 - acc: 0.8391 - val_loss: 0.3746 - val_acc: 0.8311\n",
      "Epoch 447/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3619 - acc: 0.8390 - val_loss: 0.3745 - val_acc: 0.8314\n",
      "Epoch 448/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3619 - acc: 0.8390 - val_loss: 0.3748 - val_acc: 0.8312\n",
      "Epoch 449/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3619 - acc: 0.8390 - val_loss: 0.3746 - val_acc: 0.8312\n",
      "Epoch 450/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3618 - acc: 0.8390 - val_loss: 0.3746 - val_acc: 0.8312\n",
      "Epoch 451/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3617 - acc: 0.8389 - val_loss: 0.3745 - val_acc: 0.8314\n",
      "Epoch 452/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3617 - acc: 0.8392 - val_loss: 0.3745 - val_acc: 0.8312\n",
      "Epoch 453/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3617 - acc: 0.8391 - val_loss: 0.3744 - val_acc: 0.8313\n",
      "Epoch 454/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3616 - acc: 0.8393 - val_loss: 0.3743 - val_acc: 0.8312\n",
      "Epoch 455/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3616 - acc: 0.8390 - val_loss: 0.3741 - val_acc: 0.8311\n",
      "Epoch 456/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3616 - acc: 0.8392 - val_loss: 0.3744 - val_acc: 0.8312\n",
      "Epoch 457/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3616 - acc: 0.8393 - val_loss: 0.3743 - val_acc: 0.8310\n",
      "Epoch 458/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3616 - acc: 0.8394 - val_loss: 0.3742 - val_acc: 0.8310\n",
      "Epoch 459/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3615 - acc: 0.8397 - val_loss: 0.3743 - val_acc: 0.8309\n",
      "Epoch 460/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3615 - acc: 0.8392 - val_loss: 0.3743 - val_acc: 0.8307\n",
      "Epoch 461/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3615 - acc: 0.8391 - val_loss: 0.3742 - val_acc: 0.8309\n",
      "Epoch 462/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3614 - acc: 0.8395 - val_loss: 0.3743 - val_acc: 0.8311\n",
      "Epoch 463/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3615 - acc: 0.8396 - val_loss: 0.3740 - val_acc: 0.8309\n",
      "Epoch 464/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3614 - acc: 0.8396 - val_loss: 0.3741 - val_acc: 0.8309\n",
      "Epoch 465/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3614 - acc: 0.8395 - val_loss: 0.3741 - val_acc: 0.8309\n",
      "Epoch 466/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3614 - acc: 0.8395 - val_loss: 0.3739 - val_acc: 0.8315\n",
      "Epoch 467/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3613 - acc: 0.8393 - val_loss: 0.3740 - val_acc: 0.8311\n",
      "Epoch 468/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3613 - acc: 0.8395 - val_loss: 0.3740 - val_acc: 0.8314\n",
      "Epoch 469/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3613 - acc: 0.8396 - val_loss: 0.3738 - val_acc: 0.8313\n",
      "Epoch 470/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3612 - acc: 0.8395 - val_loss: 0.3737 - val_acc: 0.8312\n",
      "Epoch 471/500\n",
      "41098/41098 [==============================] - 0s 9us/step - loss: 0.3612 - acc: 0.8396 - val_loss: 0.3738 - val_acc: 0.8312\n",
      "Epoch 472/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3612 - acc: 0.8395 - val_loss: 0.3736 - val_acc: 0.8314\n",
      "Epoch 473/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3612 - acc: 0.8393 - val_loss: 0.3737 - val_acc: 0.8313\n",
      "Epoch 474/500\n",
      "41098/41098 [==============================] - 0s 8us/step - loss: 0.3612 - acc: 0.8391 - val_loss: 0.3737 - val_acc: 0.8312\n",
      "Epoch 475/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3612 - acc: 0.8397 - val_loss: 0.3736 - val_acc: 0.8311\n",
      "Epoch 476/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3611 - acc: 0.8395 - val_loss: 0.3736 - val_acc: 0.8312\n",
      "Epoch 477/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3610 - acc: 0.8395 - val_loss: 0.3736 - val_acc: 0.8311\n",
      "Epoch 478/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3610 - acc: 0.8395 - val_loss: 0.3736 - val_acc: 0.8309\n",
      "Epoch 479/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3610 - acc: 0.8396 - val_loss: 0.3735 - val_acc: 0.8312\n",
      "Epoch 480/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3610 - acc: 0.8394 - val_loss: 0.3736 - val_acc: 0.8311\n",
      "Epoch 481/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3610 - acc: 0.8398 - val_loss: 0.3735 - val_acc: 0.8313\n",
      "Epoch 482/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3610 - acc: 0.8395 - val_loss: 0.3736 - val_acc: 0.8307\n",
      "Epoch 483/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3609 - acc: 0.8395 - val_loss: 0.3733 - val_acc: 0.8314\n",
      "Epoch 484/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3609 - acc: 0.8394 - val_loss: 0.3732 - val_acc: 0.8309\n",
      "Epoch 485/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3609 - acc: 0.8396 - val_loss: 0.3733 - val_acc: 0.8312\n",
      "Epoch 486/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3609 - acc: 0.8396 - val_loss: 0.3732 - val_acc: 0.8315\n",
      "Epoch 487/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3608 - acc: 0.8395 - val_loss: 0.3732 - val_acc: 0.8313\n",
      "Epoch 488/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3607 - acc: 0.8397 - val_loss: 0.3731 - val_acc: 0.8313\n",
      "Epoch 489/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3607 - acc: 0.8394 - val_loss: 0.3732 - val_acc: 0.8313\n",
      "Epoch 490/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3607 - acc: 0.8395 - val_loss: 0.3732 - val_acc: 0.8313\n",
      "Epoch 491/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3607 - acc: 0.8398 - val_loss: 0.3731 - val_acc: 0.8312\n",
      "Epoch 492/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3606 - acc: 0.8395 - val_loss: 0.3732 - val_acc: 0.8311\n",
      "Epoch 493/500\n",
      "41098/41098 [==============================] - 0s 7us/step - loss: 0.3606 - acc: 0.8396 - val_loss: 0.3730 - val_acc: 0.8316\n",
      "Epoch 494/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3606 - acc: 0.8395 - val_loss: 0.3730 - val_acc: 0.8313\n",
      "Epoch 495/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3606 - acc: 0.8395 - val_loss: 0.3730 - val_acc: 0.8313\n",
      "Epoch 496/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3605 - acc: 0.8393 - val_loss: 0.3729 - val_acc: 0.8313\n",
      "Epoch 497/500\n",
      "41098/41098 [==============================] - 0s 4us/step - loss: 0.3605 - acc: 0.8395 - val_loss: 0.3728 - val_acc: 0.8314\n",
      "Epoch 498/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3605 - acc: 0.8395 - val_loss: 0.3729 - val_acc: 0.8312\n",
      "Epoch 499/500\n",
      "41098/41098 [==============================] - 0s 6us/step - loss: 0.3605 - acc: 0.8396 - val_loss: 0.3728 - val_acc: 0.8316\n",
      "Epoch 500/500\n",
      "41098/41098 [==============================] - 0s 5us/step - loss: 0.3605 - acc: 0.8395 - val_loss: 0.3727 - val_acc: 0.8319\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_X[:], train_Y[:], \n",
    "          epochs=500, \n",
    "          batch_size=1024, \n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          #validation_split=0.1)\n",
    "          validation_data=(test_X, test_Y))\n",
    "\n",
    "history = {k : history[k] + h.history[k] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6850, 2), (6850, 2))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[train_Y==0].shape, train_Y[train_Y==1].shape\n",
    "test_Y[test_Y[:,0]==1].shape, test_Y[test_Y[:,1]==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3820282198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history, semilog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2206204  0.7795533 ]\n",
      " [0.13186947 0.8680657 ]\n",
      " [0.4344645  0.5650122 ]\n",
      " [0.3533134  0.6464893 ]\n",
      " [0.7490016  0.25091422]\n",
      " [0.03128402 0.96872073]\n",
      " [0.19450949 0.8054433 ]\n",
      " [0.2560959  0.7439529 ]\n",
      " [0.7416212  0.25845933]\n",
      " [0.8907972  0.10906365]]\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted values\n",
    "Y_pred_ = model.predict(test_X)\n",
    "# predictions are outputted as floats from [0,1]\n",
    "print(Y_pred_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999331, 1.0009164)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_.sum(axis=1).min(), Y_pred_.sum(axis=1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = numpy.argmax(Y_pred_, axis=1)\n",
    "Y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5711 1139]\n",
      " [1163 5687]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# we must translate it to 0, 1 based on thresholding at 0.5\n",
    "# where < 0.5 set to 0, to 1 otherwise\n",
    "#Y_pred = numpy.where(Y_pred < 0.5, 0, 1)\n",
    "\n",
    "# calculate confusion matrix\n",
    "conf_mat = confusion_matrix(test_Y_, Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17354  3195]\n",
      " [ 3405 17144]]\n"
     ]
    }
   ],
   "source": [
    "## confusion matrix on Train?\n",
    "Y_pred = numpy.argmax(model.predict(train_X), axis=1)\n",
    "conf_mat = confusion_matrix(train_Y_, Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13700/13700 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3726936337025496, 0.8318978102189781]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval on test data\n",
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41098/41098 [==============================] - 0s 11us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36035532819145155, 0.8393839116229692]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model and weights, NC = nodes correct\n",
    "model.save('URZ_model_15-6-2_norm_NC_NTPS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network TP vs S \n",
    "\n",
    "* we need a new dataset for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for TP vs S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20549, 25) (6850, 25)\n"
     ]
    }
   ],
   "source": [
    "print(TPS_train.shape, TPS_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20549, 15) (20549, 2) (6850, 15) (6850, 2)\n"
     ]
    }
   ],
   "source": [
    "train_X_TPS = TPS_train[x_indices].values.astype(float)\n",
    "train_Y_TPS = TPS_train[y_indices]\n",
    "\n",
    "test_X_TPS = TPS_test[x_indices].values.astype(float)\n",
    "test_Y_TPS = TPS_test[y_indices]\n",
    "\n",
    "#regS = 0, T/regP = 1\n",
    "train_Y_TPS_ = numpy.array(numpy.where(train_Y_TPS['CLASS_PHASE'] == 'regS', 1, 0), dtype=float)\n",
    "test_Y_TPS_ = numpy.array(numpy.where(test_Y_TPS['CLASS_PHASE'] == 'regS', 1, 0), dtype=float)\n",
    "\n",
    "#convert to categorical\n",
    "train_Y_TPS = keras.utils.to_categorical(train_Y_TPS_)\n",
    "test_Y_TPS = keras.utils.to_categorical(test_Y_TPS_)\n",
    "\n",
    "print(train_X_TPS.shape, train_Y_TPS.shape, test_X_TPS.shape, test_Y_TPS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if node3 < 0.5 => S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>PER</th>\n",
       "      <th>RECT</th>\n",
       "      <th>PLANS</th>\n",
       "      <th>INANG1</th>\n",
       "      <th>INANG3</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.829703</td>\n",
       "      <td>0.733932</td>\n",
       "      <td>0.656072</td>\n",
       "      <td>0.356845</td>\n",
       "      <td>0.304759</td>\n",
       "      <td>0.201781</td>\n",
       "      <td>0.275765</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.136094</td>\n",
       "      <td>-0.464298</td>\n",
       "      <td>-0.132930</td>\n",
       "      <td>0.049345</td>\n",
       "      <td>-0.133097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.895752</td>\n",
       "      <td>0.977953</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.983441</td>\n",
       "      <td>0.838955</td>\n",
       "      <td>-0.981637</td>\n",
       "      <td>-0.810078</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>-0.228505</td>\n",
       "      <td>-0.099400</td>\n",
       "      <td>-0.743478</td>\n",
       "      <td>-1.013762</td>\n",
       "      <td>-0.880014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.983845</td>\n",
       "      <td>0.967418</td>\n",
       "      <td>0.167897</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.402774</td>\n",
       "      <td>-1.273636</td>\n",
       "      <td>-1.329216</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>-0.310373</td>\n",
       "      <td>-0.182101</td>\n",
       "      <td>-0.163527</td>\n",
       "      <td>-0.844041</td>\n",
       "      <td>-1.027759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.986203</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>0.154322</td>\n",
       "      <td>0.822576</td>\n",
       "      <td>0.349078</td>\n",
       "      <td>-1.349539</td>\n",
       "      <td>-1.154446</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.897362</td>\n",
       "      <td>0.112187</td>\n",
       "      <td>-0.106137</td>\n",
       "      <td>-0.735110</td>\n",
       "      <td>-1.124374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.982295</td>\n",
       "      <td>0.974180</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.979892</td>\n",
       "      <td>0.239376</td>\n",
       "      <td>-1.227335</td>\n",
       "      <td>-1.227335</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.211810</td>\n",
       "      <td>0.194740</td>\n",
       "      <td>-0.119883</td>\n",
       "      <td>-0.102045</td>\n",
       "      <td>-1.348465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.838278</td>\n",
       "      <td>0.922653</td>\n",
       "      <td>0.627375</td>\n",
       "      <td>0.661725</td>\n",
       "      <td>0.363572</td>\n",
       "      <td>-0.041421</td>\n",
       "      <td>-0.287866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.362823</td>\n",
       "      <td>0.087695</td>\n",
       "      <td>-0.192810</td>\n",
       "      <td>-0.354543</td>\n",
       "      <td>-1.052923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.891537</td>\n",
       "      <td>0.949444</td>\n",
       "      <td>0.065309</td>\n",
       "      <td>0.955195</td>\n",
       "      <td>0.469561</td>\n",
       "      <td>-0.942423</td>\n",
       "      <td>-0.942423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>0.485974</td>\n",
       "      <td>-0.230462</td>\n",
       "      <td>-0.982911</td>\n",
       "      <td>-1.088941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.782548</td>\n",
       "      <td>0.860514</td>\n",
       "      <td>0.616115</td>\n",
       "      <td>0.994184</td>\n",
       "      <td>0.255811</td>\n",
       "      <td>-0.017308</td>\n",
       "      <td>-0.354208</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>-0.293590</td>\n",
       "      <td>-0.216389</td>\n",
       "      <td>-0.443438</td>\n",
       "      <td>-0.238399</td>\n",
       "      <td>-0.893283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.933073</td>\n",
       "      <td>0.968623</td>\n",
       "      <td>0.061147</td>\n",
       "      <td>0.966952</td>\n",
       "      <td>0.551731</td>\n",
       "      <td>-1.142040</td>\n",
       "      <td>-0.986672</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.346875</td>\n",
       "      <td>-0.220367</td>\n",
       "      <td>-0.494649</td>\n",
       "      <td>-0.768719</td>\n",
       "      <td>-0.691848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.922974</td>\n",
       "      <td>0.869321</td>\n",
       "      <td>0.798127</td>\n",
       "      <td>0.635730</td>\n",
       "      <td>0.635628</td>\n",
       "      <td>0.432552</td>\n",
       "      <td>-0.259575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.569255</td>\n",
       "      <td>-0.207097</td>\n",
       "      <td>-0.120775</td>\n",
       "      <td>-1.436423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.965188</td>\n",
       "      <td>0.913537</td>\n",
       "      <td>0.098021</td>\n",
       "      <td>0.994014</td>\n",
       "      <td>0.278660</td>\n",
       "      <td>-1.325541</td>\n",
       "      <td>-1.164561</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.467754</td>\n",
       "      <td>-0.602633</td>\n",
       "      <td>-0.624759</td>\n",
       "      <td>-1.387865</td>\n",
       "      <td>-1.366298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.920022</td>\n",
       "      <td>0.888794</td>\n",
       "      <td>0.262075</td>\n",
       "      <td>0.836832</td>\n",
       "      <td>0.188466</td>\n",
       "      <td>-0.756622</td>\n",
       "      <td>-0.617293</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.082380</td>\n",
       "      <td>-0.508367</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>-0.450086</td>\n",
       "      <td>-0.763970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.780581</td>\n",
       "      <td>0.719557</td>\n",
       "      <td>0.895922</td>\n",
       "      <td>0.973385</td>\n",
       "      <td>0.203312</td>\n",
       "      <td>0.481612</td>\n",
       "      <td>0.123168</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.420000</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>-0.377011</td>\n",
       "      <td>-0.027235</td>\n",
       "      <td>0.070459</td>\n",
       "      <td>0.098154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.986480</td>\n",
       "      <td>0.960638</td>\n",
       "      <td>0.179946</td>\n",
       "      <td>0.944354</td>\n",
       "      <td>0.524717</td>\n",
       "      <td>-1.251078</td>\n",
       "      <td>-1.150116</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.618987</td>\n",
       "      <td>-0.208024</td>\n",
       "      <td>0.131079</td>\n",
       "      <td>-0.575793</td>\n",
       "      <td>-1.623556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.101944</td>\n",
       "      <td>0.998957</td>\n",
       "      <td>1.629443</td>\n",
       "      <td>-1.684446</td>\n",
       "      <td>-1.684446</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>-0.361433</td>\n",
       "      <td>-0.975744</td>\n",
       "      <td>-0.693025</td>\n",
       "      <td>-0.523485</td>\n",
       "      <td>-1.350391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.964426</td>\n",
       "      <td>0.977247</td>\n",
       "      <td>0.922645</td>\n",
       "      <td>0.135102</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>1.056174</td>\n",
       "      <td>1.360088</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.270000</td>\n",
       "      <td>-0.473737</td>\n",
       "      <td>0.562816</td>\n",
       "      <td>-0.039937</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.183171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.731635</td>\n",
       "      <td>0.858930</td>\n",
       "      <td>0.871370</td>\n",
       "      <td>0.381045</td>\n",
       "      <td>0.247134</td>\n",
       "      <td>0.453225</td>\n",
       "      <td>0.453225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234795</td>\n",
       "      <td>0.108528</td>\n",
       "      <td>-0.032878</td>\n",
       "      <td>0.423865</td>\n",
       "      <td>0.187285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.932365</td>\n",
       "      <td>0.937364</td>\n",
       "      <td>0.945865</td>\n",
       "      <td>0.081223</td>\n",
       "      <td>0.440639</td>\n",
       "      <td>1.280378</td>\n",
       "      <td>1.080003</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.303333</td>\n",
       "      <td>0.057331</td>\n",
       "      <td>-0.485178</td>\n",
       "      <td>0.036946</td>\n",
       "      <td>0.784938</td>\n",
       "      <td>0.352634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tele</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979725</td>\n",
       "      <td>0.956633</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>0.823108</td>\n",
       "      <td>0.336388</td>\n",
       "      <td>-0.617228</td>\n",
       "      <td>-0.847361</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.306154</td>\n",
       "      <td>-0.808939</td>\n",
       "      <td>-0.228066</td>\n",
       "      <td>-0.195246</td>\n",
       "      <td>-0.556634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.851317</td>\n",
       "      <td>0.817745</td>\n",
       "      <td>0.867319</td>\n",
       "      <td>0.558351</td>\n",
       "      <td>0.418358</td>\n",
       "      <td>0.462492</td>\n",
       "      <td>0.462492</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.180000</td>\n",
       "      <td>0.349265</td>\n",
       "      <td>-0.293232</td>\n",
       "      <td>0.203673</td>\n",
       "      <td>0.151699</td>\n",
       "      <td>0.618117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.840431</td>\n",
       "      <td>0.889002</td>\n",
       "      <td>0.969667</td>\n",
       "      <td>0.257697</td>\n",
       "      <td>0.384405</td>\n",
       "      <td>0.789529</td>\n",
       "      <td>0.789529</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.812345</td>\n",
       "      <td>-0.041515</td>\n",
       "      <td>0.531149</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.315548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.782956</td>\n",
       "      <td>0.922833</td>\n",
       "      <td>0.893738</td>\n",
       "      <td>0.525769</td>\n",
       "      <td>0.368126</td>\n",
       "      <td>0.530044</td>\n",
       "      <td>0.365187</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>-0.062118</td>\n",
       "      <td>-0.179612</td>\n",
       "      <td>-0.119026</td>\n",
       "      <td>0.169634</td>\n",
       "      <td>0.200568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.973031</td>\n",
       "      <td>0.934159</td>\n",
       "      <td>0.974222</td>\n",
       "      <td>0.912662</td>\n",
       "      <td>1.179741</td>\n",
       "      <td>0.988480</td>\n",
       "      <td>-0.996128</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>1.255939</td>\n",
       "      <td>1.116827</td>\n",
       "      <td>1.424757</td>\n",
       "      <td>1.718208</td>\n",
       "      <td>0.555674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.978958</td>\n",
       "      <td>0.984967</td>\n",
       "      <td>0.091801</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.141551</td>\n",
       "      <td>-1.495710</td>\n",
       "      <td>-1.495710</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.310000</td>\n",
       "      <td>0.525264</td>\n",
       "      <td>-0.549910</td>\n",
       "      <td>0.034540</td>\n",
       "      <td>-1.231753</td>\n",
       "      <td>-1.064305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.907362</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.547685</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>2.557585</td>\n",
       "      <td>-0.211605</td>\n",
       "      <td>-0.010491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.875615</td>\n",
       "      <td>-0.026910</td>\n",
       "      <td>-0.159326</td>\n",
       "      <td>0.053913</td>\n",
       "      <td>-0.011011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.821569</td>\n",
       "      <td>0.779235</td>\n",
       "      <td>0.550132</td>\n",
       "      <td>0.488919</td>\n",
       "      <td>0.454480</td>\n",
       "      <td>-0.073833</td>\n",
       "      <td>-0.073833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.688271</td>\n",
       "      <td>-0.071651</td>\n",
       "      <td>-0.311226</td>\n",
       "      <td>-0.074948</td>\n",
       "      <td>-0.811466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.994873</td>\n",
       "      <td>0.977616</td>\n",
       "      <td>0.986941</td>\n",
       "      <td>0.905217</td>\n",
       "      <td>1.382541</td>\n",
       "      <td>1.757578</td>\n",
       "      <td>-0.696409</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.225773</td>\n",
       "      <td>1.141321</td>\n",
       "      <td>1.238418</td>\n",
       "      <td>1.914813</td>\n",
       "      <td>0.178448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.945048</td>\n",
       "      <td>0.900128</td>\n",
       "      <td>0.140150</td>\n",
       "      <td>0.939869</td>\n",
       "      <td>0.318258</td>\n",
       "      <td>-1.094083</td>\n",
       "      <td>-0.908601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952029</td>\n",
       "      <td>0.441782</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.345351</td>\n",
       "      <td>-1.135835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.907732</td>\n",
       "      <td>0.833940</td>\n",
       "      <td>0.910396</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.522917</td>\n",
       "      <td>0.690091</td>\n",
       "      <td>-0.154159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175161</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>-0.153314</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>-0.376151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.958260</td>\n",
       "      <td>0.954002</td>\n",
       "      <td>0.087231</td>\n",
       "      <td>0.888122</td>\n",
       "      <td>0.276527</td>\n",
       "      <td>-1.286918</td>\n",
       "      <td>-0.514136</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.657559</td>\n",
       "      <td>-0.041817</td>\n",
       "      <td>-0.298847</td>\n",
       "      <td>-0.769287</td>\n",
       "      <td>-0.980751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20519</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.862300</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.965825</td>\n",
       "      <td>0.145684</td>\n",
       "      <td>0.399848</td>\n",
       "      <td>0.868541</td>\n",
       "      <td>0.671853</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.116667</td>\n",
       "      <td>-0.018958</td>\n",
       "      <td>0.176718</td>\n",
       "      <td>0.467523</td>\n",
       "      <td>0.830444</td>\n",
       "      <td>-0.018948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20520</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.817555</td>\n",
       "      <td>0.903455</td>\n",
       "      <td>0.913536</td>\n",
       "      <td>0.166450</td>\n",
       "      <td>0.297071</td>\n",
       "      <td>0.841847</td>\n",
       "      <td>0.841847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012898</td>\n",
       "      <td>-0.189536</td>\n",
       "      <td>-0.133765</td>\n",
       "      <td>0.693717</td>\n",
       "      <td>0.380754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20521</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.927863</td>\n",
       "      <td>0.946830</td>\n",
       "      <td>0.990287</td>\n",
       "      <td>0.129494</td>\n",
       "      <td>0.570420</td>\n",
       "      <td>1.006200</td>\n",
       "      <td>1.028407</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>-0.282934</td>\n",
       "      <td>-0.049967</td>\n",
       "      <td>0.818559</td>\n",
       "      <td>0.683810</td>\n",
       "      <td>-0.105641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20522</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.790852</td>\n",
       "      <td>0.951996</td>\n",
       "      <td>0.196908</td>\n",
       "      <td>0.241194</td>\n",
       "      <td>0.153164</td>\n",
       "      <td>-0.574719</td>\n",
       "      <td>0.681176</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>-0.204061</td>\n",
       "      <td>-0.035066</td>\n",
       "      <td>0.224621</td>\n",
       "      <td>0.607564</td>\n",
       "      <td>0.394717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20523</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.872433</td>\n",
       "      <td>0.882941</td>\n",
       "      <td>0.943491</td>\n",
       "      <td>0.063190</td>\n",
       "      <td>0.334470</td>\n",
       "      <td>1.060257</td>\n",
       "      <td>0.894434</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>-0.661382</td>\n",
       "      <td>-0.466698</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.646729</td>\n",
       "      <td>0.545636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20524</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.829769</td>\n",
       "      <td>0.681364</td>\n",
       "      <td>0.544387</td>\n",
       "      <td>0.751539</td>\n",
       "      <td>0.315041</td>\n",
       "      <td>-0.135514</td>\n",
       "      <td>-0.199418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>-0.256422</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>-0.190338</td>\n",
       "      <td>0.032239</td>\n",
       "      <td>-1.143045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20525</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.916077</td>\n",
       "      <td>0.870980</td>\n",
       "      <td>0.129773</td>\n",
       "      <td>0.964130</td>\n",
       "      <td>0.665348</td>\n",
       "      <td>-0.973008</td>\n",
       "      <td>-0.973008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>-0.518384</td>\n",
       "      <td>-0.229680</td>\n",
       "      <td>-0.595395</td>\n",
       "      <td>-0.459522</td>\n",
       "      <td>-0.948421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20526</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.982854</td>\n",
       "      <td>0.980151</td>\n",
       "      <td>0.212690</td>\n",
       "      <td>0.867554</td>\n",
       "      <td>0.364365</td>\n",
       "      <td>-1.106910</td>\n",
       "      <td>-1.222415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232451</td>\n",
       "      <td>-0.179140</td>\n",
       "      <td>-0.419577</td>\n",
       "      <td>-0.758129</td>\n",
       "      <td>-0.618048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20527</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.690042</td>\n",
       "      <td>0.689347</td>\n",
       "      <td>0.992699</td>\n",
       "      <td>0.113722</td>\n",
       "      <td>0.249848</td>\n",
       "      <td>0.494287</td>\n",
       "      <td>0.494287</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>-0.541690</td>\n",
       "      <td>-0.241055</td>\n",
       "      <td>-0.369383</td>\n",
       "      <td>0.140847</td>\n",
       "      <td>0.343669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20528</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.880107</td>\n",
       "      <td>0.604325</td>\n",
       "      <td>0.776688</td>\n",
       "      <td>0.510449</td>\n",
       "      <td>0.425996</td>\n",
       "      <td>0.469538</td>\n",
       "      <td>-0.057472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.219750</td>\n",
       "      <td>-0.237609</td>\n",
       "      <td>-0.243115</td>\n",
       "      <td>0.571118</td>\n",
       "      <td>0.160088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20529</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.733932</td>\n",
       "      <td>0.656038</td>\n",
       "      <td>0.779047</td>\n",
       "      <td>0.270507</td>\n",
       "      <td>0.327299</td>\n",
       "      <td>0.175528</td>\n",
       "      <td>0.406496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500231</td>\n",
       "      <td>0.480947</td>\n",
       "      <td>0.101621</td>\n",
       "      <td>0.198921</td>\n",
       "      <td>0.396998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20530</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.685798</td>\n",
       "      <td>0.757360</td>\n",
       "      <td>0.832738</td>\n",
       "      <td>0.247435</td>\n",
       "      <td>0.247586</td>\n",
       "      <td>0.434735</td>\n",
       "      <td>0.434735</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.662870</td>\n",
       "      <td>-0.346606</td>\n",
       "      <td>0.086197</td>\n",
       "      <td>0.278969</td>\n",
       "      <td>-0.039616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20531</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.962077</td>\n",
       "      <td>0.927708</td>\n",
       "      <td>0.154064</td>\n",
       "      <td>0.936396</td>\n",
       "      <td>0.228487</td>\n",
       "      <td>-1.161732</td>\n",
       "      <td>-0.931759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>0.335773</td>\n",
       "      <td>-0.038356</td>\n",
       "      <td>-0.397424</td>\n",
       "      <td>-0.920494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20532</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.923949</td>\n",
       "      <td>0.918772</td>\n",
       "      <td>0.499630</td>\n",
       "      <td>0.572965</td>\n",
       "      <td>0.792706</td>\n",
       "      <td>0.792706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.550710</td>\n",
       "      <td>0.514392</td>\n",
       "      <td>0.322395</td>\n",
       "      <td>0.354767</td>\n",
       "      <td>0.800960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20533</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.785798</td>\n",
       "      <td>0.921016</td>\n",
       "      <td>0.140908</td>\n",
       "      <td>0.868644</td>\n",
       "      <td>0.353315</td>\n",
       "      <td>-0.610604</td>\n",
       "      <td>-0.367762</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>1.052076</td>\n",
       "      <td>0.356068</td>\n",
       "      <td>0.042597</td>\n",
       "      <td>-0.360066</td>\n",
       "      <td>-0.692383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20534</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.952481</td>\n",
       "      <td>0.960376</td>\n",
       "      <td>0.110387</td>\n",
       "      <td>0.892685</td>\n",
       "      <td>0.116087</td>\n",
       "      <td>-1.194507</td>\n",
       "      <td>-1.194507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.231460</td>\n",
       "      <td>-0.472326</td>\n",
       "      <td>-0.062120</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>-0.668591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.950727</td>\n",
       "      <td>0.847848</td>\n",
       "      <td>0.084477</td>\n",
       "      <td>0.824106</td>\n",
       "      <td>0.308328</td>\n",
       "      <td>-1.230047</td>\n",
       "      <td>-0.761863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.835937</td>\n",
       "      <td>-0.698695</td>\n",
       "      <td>-0.947556</td>\n",
       "      <td>-0.826191</td>\n",
       "      <td>-1.189958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20536</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.973802</td>\n",
       "      <td>0.934937</td>\n",
       "      <td>0.135336</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>-1.301136</td>\n",
       "      <td>-1.079480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.019485</td>\n",
       "      <td>-0.382241</td>\n",
       "      <td>-0.359022</td>\n",
       "      <td>-0.257290</td>\n",
       "      <td>-0.628889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20537</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>0.691536</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.533525</td>\n",
       "      <td>0.282581</td>\n",
       "      <td>-0.880835</td>\n",
       "      <td>0.133828</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>-0.202520</td>\n",
       "      <td>0.519481</td>\n",
       "      <td>-0.135793</td>\n",
       "      <td>0.340057</td>\n",
       "      <td>0.547186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20538</th>\n",
       "      <td>regP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941208</td>\n",
       "      <td>0.816890</td>\n",
       "      <td>0.354163</td>\n",
       "      <td>0.982532</td>\n",
       "      <td>0.168560</td>\n",
       "      <td>-0.576419</td>\n",
       "      <td>-0.763343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.162618</td>\n",
       "      <td>-0.357404</td>\n",
       "      <td>-0.706159</td>\n",
       "      <td>-0.690479</td>\n",
       "      <td>-0.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20539</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.881219</td>\n",
       "      <td>0.910499</td>\n",
       "      <td>0.346177</td>\n",
       "      <td>0.722513</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>-0.511573</td>\n",
       "      <td>-0.457307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.918704</td>\n",
       "      <td>-0.060654</td>\n",
       "      <td>-0.510686</td>\n",
       "      <td>-0.996540</td>\n",
       "      <td>-0.796534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20540</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.589057</td>\n",
       "      <td>0.757207</td>\n",
       "      <td>0.332556</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.438801</td>\n",
       "      <td>0.243198</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.365000</td>\n",
       "      <td>0.049060</td>\n",
       "      <td>0.650874</td>\n",
       "      <td>0.241179</td>\n",
       "      <td>0.379292</td>\n",
       "      <td>0.318197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20541</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.782078</td>\n",
       "      <td>0.740776</td>\n",
       "      <td>0.977789</td>\n",
       "      <td>0.153316</td>\n",
       "      <td>0.226065</td>\n",
       "      <td>0.817801</td>\n",
       "      <td>0.522411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.245940</td>\n",
       "      <td>-0.418118</td>\n",
       "      <td>-0.168725</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>0.309917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.965739</td>\n",
       "      <td>0.970824</td>\n",
       "      <td>0.171642</td>\n",
       "      <td>0.871831</td>\n",
       "      <td>0.417233</td>\n",
       "      <td>-1.135489</td>\n",
       "      <td>-0.914534</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>-0.509908</td>\n",
       "      <td>-0.108766</td>\n",
       "      <td>-0.629126</td>\n",
       "      <td>-0.814799</td>\n",
       "      <td>-1.148184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20543</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.982840</td>\n",
       "      <td>0.992383</td>\n",
       "      <td>0.159445</td>\n",
       "      <td>0.910715</td>\n",
       "      <td>0.018456</td>\n",
       "      <td>-1.298581</td>\n",
       "      <td>-0.943641</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.928762</td>\n",
       "      <td>-1.063490</td>\n",
       "      <td>-0.873349</td>\n",
       "      <td>-0.925118</td>\n",
       "      <td>-1.565706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20544</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.686791</td>\n",
       "      <td>0.769233</td>\n",
       "      <td>0.324196</td>\n",
       "      <td>0.913431</td>\n",
       "      <td>0.453008</td>\n",
       "      <td>-0.357380</td>\n",
       "      <td>-0.357380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.103441</td>\n",
       "      <td>0.303119</td>\n",
       "      <td>-0.335505</td>\n",
       "      <td>-0.229667</td>\n",
       "      <td>-0.831997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20545</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.993218</td>\n",
       "      <td>0.969688</td>\n",
       "      <td>0.170729</td>\n",
       "      <td>0.791223</td>\n",
       "      <td>0.449488</td>\n",
       "      <td>-1.349302</td>\n",
       "      <td>-1.103415</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>-0.213983</td>\n",
       "      <td>0.117369</td>\n",
       "      <td>-0.443685</td>\n",
       "      <td>-1.089743</td>\n",
       "      <td>-1.113523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20546</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.929992</td>\n",
       "      <td>0.916806</td>\n",
       "      <td>0.174894</td>\n",
       "      <td>0.843026</td>\n",
       "      <td>0.198346</td>\n",
       "      <td>-0.948384</td>\n",
       "      <td>-0.948384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339135</td>\n",
       "      <td>-0.363711</td>\n",
       "      <td>-0.403891</td>\n",
       "      <td>-1.062191</td>\n",
       "      <td>-1.085736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20547</th>\n",
       "      <td>regS</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.891119</td>\n",
       "      <td>0.910690</td>\n",
       "      <td>0.981022</td>\n",
       "      <td>0.181680</td>\n",
       "      <td>0.428104</td>\n",
       "      <td>0.970899</td>\n",
       "      <td>0.970899</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>0.166738</td>\n",
       "      <td>0.565318</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>0.800681</td>\n",
       "      <td>0.559771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20548</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.979784</td>\n",
       "      <td>0.181978</td>\n",
       "      <td>0.942053</td>\n",
       "      <td>0.204177</td>\n",
       "      <td>-1.302858</td>\n",
       "      <td>-1.198127</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>-0.570280</td>\n",
       "      <td>-0.005549</td>\n",
       "      <td>-0.148968</td>\n",
       "      <td>-0.449955</td>\n",
       "      <td>-0.920584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20549 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CLASS_PHASE       PER      RECT     PLANS    INANG1    INANG3     HMXMN  \\\n",
       "0            regS  0.333333  0.829703  0.733932  0.656072  0.356845  0.304759   \n",
       "1            tele  0.333333  0.895752  0.977953  0.006972  0.983441  0.838955   \n",
       "2            regP  0.166667  0.983845  0.967418  0.167897  0.999756  0.402774   \n",
       "3            tele  0.166667  0.986203  0.974063  0.154322  0.822576  0.349078   \n",
       "4            regP  0.166667  0.982295  0.974180  0.177759  0.979892  0.239376   \n",
       "5            regP  0.166667  0.838278  0.922653  0.627375  0.661725  0.363572   \n",
       "6            tele  0.166667  0.891537  0.949444  0.065309  0.955195  0.469561   \n",
       "7            regP  0.166667  0.782548  0.860514  0.616115  0.994184  0.255811   \n",
       "8            tele  0.444444  0.933073  0.968623  0.061147  0.966952  0.551731   \n",
       "9            tele  0.166667  0.922974  0.869321  0.798127  0.635730  0.635628   \n",
       "10           regP  0.166667  0.965188  0.913537  0.098021  0.994014  0.278660   \n",
       "11           regP  0.333333  0.920022  0.888794  0.262075  0.836832  0.188466   \n",
       "12           regS  0.333333  0.780581  0.719557  0.895922  0.973385  0.203312   \n",
       "13           regP  0.166667  0.986480  0.960638  0.179946  0.944354  0.524717   \n",
       "14           regP  0.166667  0.992356  0.999967  0.101944  0.998957  1.629443   \n",
       "15           tele  0.444444  0.964426  0.977247  0.922645  0.135102  0.705774   \n",
       "16           regS  0.333333  0.731635  0.858930  0.871370  0.381045  0.247134   \n",
       "17           regS  0.333333  0.932365  0.937364  0.945865  0.081223  0.440639   \n",
       "18           tele  1.000000  0.979725  0.956633  0.373444  0.823108  0.336388   \n",
       "19           regS  0.333333  0.851317  0.817745  0.867319  0.558351  0.418358   \n",
       "20           regS  0.333333  0.840431  0.889002  0.969667  0.257697  0.384405   \n",
       "21           regS  0.333333  0.782956  0.922833  0.893738  0.525769  0.368126   \n",
       "22           regP  0.166667  0.973031  0.934159  0.974222  0.912662  1.179741   \n",
       "23           regP  0.444444  0.978958  0.984967  0.091801  0.908891  0.141551   \n",
       "24           regS  0.444444  0.907362  0.999999  0.547685  0.999720  2.557585   \n",
       "25           regP  0.333333  0.821569  0.779235  0.550132  0.488919  0.454480   \n",
       "26           regP  0.166667  0.994873  0.977616  0.986941  0.905217  1.382541   \n",
       "27           tele  0.333333  0.945048  0.900128  0.140150  0.939869  0.318258   \n",
       "28           regS  0.444444  0.907732  0.833940  0.910396  0.938447  0.522917   \n",
       "29           tele  0.333333  0.958260  0.954002  0.087231  0.888122  0.276527   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "20519        regS  0.333333  0.862300  0.812143  0.965825  0.145684  0.399848   \n",
       "20520        regS  0.333333  0.817555  0.903455  0.913536  0.166450  0.297071   \n",
       "20521        tele  0.444444  0.927863  0.946830  0.990287  0.129494  0.570420   \n",
       "20522        tele  0.166667  0.790852  0.951996  0.196908  0.241194  0.153164   \n",
       "20523        regS  0.333333  0.872433  0.882941  0.943491  0.063190  0.334470   \n",
       "20524        tele  0.166667  0.829769  0.681364  0.544387  0.751539  0.315041   \n",
       "20525        regP  0.166667  0.916077  0.870980  0.129773  0.964130  0.665348   \n",
       "20526        tele  0.166667  0.982854  0.980151  0.212690  0.867554  0.364365   \n",
       "20527        regP  0.166667  0.690042  0.689347  0.992699  0.113722  0.249848   \n",
       "20528        regS  0.333333  0.880107  0.604325  0.776688  0.510449  0.425996   \n",
       "20529        regS  0.166667  0.733932  0.656038  0.779047  0.270507  0.327299   \n",
       "20530        regS  0.444444  0.685798  0.757360  0.832738  0.247435  0.247586   \n",
       "20531        tele  0.166667  0.962077  0.927708  0.154064  0.936396  0.228487   \n",
       "20532        tele  0.666667  0.929451  0.923949  0.918772  0.499630  0.572965   \n",
       "20533        regP  0.333333  0.785798  0.921016  0.140908  0.868644  0.353315   \n",
       "20534        regP  0.166667  0.952481  0.960376  0.110387  0.892685  0.116087   \n",
       "20535        tele  0.444444  0.950727  0.847848  0.084477  0.824106  0.308328   \n",
       "20536        regP  0.166667  0.973802  0.934937  0.135336  0.829110  0.160964   \n",
       "20537        regS  0.166667  0.911300  0.691536  0.179528  0.533525  0.282581   \n",
       "20538        regP  1.000000  0.941208  0.816890  0.354163  0.982532  0.168560   \n",
       "20539        tele  0.666667  0.881219  0.910499  0.346177  0.722513  0.724409   \n",
       "20540        regS  0.166667  0.853636  0.589057  0.757207  0.332556  0.210526   \n",
       "20541        regS  0.444444  0.782078  0.740776  0.977789  0.153316  0.226065   \n",
       "20542        regP  0.166667  0.965739  0.970824  0.171642  0.871831  0.417233   \n",
       "20543        tele  0.666667  0.982840  0.992383  0.159445  0.910715  0.018456   \n",
       "20544        regP  0.166667  0.686791  0.769233  0.324196  0.913431  0.453008   \n",
       "20545        tele  0.166667  0.993218  0.969688  0.170729  0.791223  0.449488   \n",
       "20546        tele  0.444444  0.929992  0.916806  0.174894  0.843026  0.198346   \n",
       "20547        regS  0.444444  0.891119  0.910690  0.981022  0.181680  0.428104   \n",
       "20548        regP  0.166667  0.993644  0.979784  0.181978  0.942053  0.204177   \n",
       "\n",
       "         HVRATP     HVRAT  NAB       TAB     HTOV1     HTOV2     HTOV3  \\\n",
       "0      0.201781  0.275765 -0.2 -0.175000 -0.136094 -0.464298 -0.132930   \n",
       "1     -0.981637 -0.810078  0.1  0.090000 -0.228505 -0.099400 -0.743478   \n",
       "2     -1.273636 -1.329216  0.2  0.315000 -0.310373 -0.182101 -0.163527   \n",
       "3     -1.349539 -1.154446  0.1  0.140000  0.897362  0.112187 -0.106137   \n",
       "4     -1.227335 -1.227335  0.1  0.070000  0.211810  0.194740 -0.119883   \n",
       "5     -0.041421 -0.287866  0.0  0.000000 -0.362823  0.087695 -0.192810   \n",
       "6     -0.942423 -0.942423  0.0  0.000000  0.015178  0.485974 -0.230462   \n",
       "7     -0.017308 -0.354208  0.3  0.363333 -0.293590 -0.216389 -0.443438   \n",
       "8     -1.142040 -0.986672  0.1  0.080000  0.346875 -0.220367 -0.494649   \n",
       "9      0.432552 -0.259575  0.0  0.000000  0.009300  0.569255 -0.207097   \n",
       "10    -1.325541 -1.164561  0.1  0.250000  0.467754 -0.602633 -0.624759   \n",
       "11    -0.756622 -0.617293  0.2  0.305000  0.082380 -0.508367  0.006105   \n",
       "12     0.481612  0.123168 -0.2 -0.420000  0.005756 -0.377011 -0.027235   \n",
       "13    -1.251078 -1.150116  0.1  0.050000 -0.618987 -0.208024  0.131079   \n",
       "14    -1.684446 -1.684446  0.1  0.230000 -0.361433 -0.975744 -0.693025   \n",
       "15     1.056174  1.360088 -0.1 -0.270000 -0.473737  0.562816 -0.039937   \n",
       "16     0.453225  0.453225  0.0  0.000000  0.234795  0.108528 -0.032878   \n",
       "17     1.280378  1.080003  0.3  0.303333  0.057331 -0.485178  0.036946   \n",
       "18    -0.617228 -0.847361  0.1  0.100000 -0.306154 -0.808939 -0.228066   \n",
       "19     0.462492  0.462492 -0.1 -0.180000  0.349265 -0.293232  0.203673   \n",
       "20     0.789529  0.789529  0.1  0.170000  0.812345 -0.041515  0.531149   \n",
       "21     0.530044  0.365187  0.1  0.110000 -0.062118 -0.179612 -0.119026   \n",
       "22     0.988480 -0.996128 -0.1 -0.350000  1.255939  1.116827  1.424757   \n",
       "23    -1.495710 -1.495710 -0.1 -0.310000  0.525264 -0.549910  0.034540   \n",
       "24    -0.211605 -0.010491  0.0 -0.150000 -0.875615 -0.026910 -0.159326   \n",
       "25    -0.073833 -0.073833  0.0  0.000000 -0.688271 -0.071651 -0.311226   \n",
       "26     1.757578 -0.696409  0.1  0.060000  0.225773  1.141321  1.238418   \n",
       "27    -1.094083 -0.908601  0.0  0.000000  0.952029  0.441782 -0.006021   \n",
       "28     0.690091 -0.154159  0.0  0.000000  0.175161  0.261322 -0.153314   \n",
       "29    -1.286918 -0.514136  0.1  0.100000 -0.657559 -0.041817 -0.298847   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "20519  0.868541  0.671853  0.2 -0.116667 -0.018958  0.176718  0.467523   \n",
       "20520  0.841847  0.841847  0.0  0.000000 -0.012898 -0.189536 -0.133765   \n",
       "20521  1.006200  1.028407  0.3  0.386667 -0.282934 -0.049967  0.818559   \n",
       "20522 -0.574719  0.681176  0.1  0.180000 -0.204061 -0.035066  0.224621   \n",
       "20523  1.060257  0.894434  0.1  0.110000 -0.661382 -0.466698  0.005814   \n",
       "20524 -0.135514 -0.199418  0.0 -0.340000 -0.256422  0.032010 -0.190338   \n",
       "20525 -0.973008 -0.973008  0.2  0.395000 -0.518384 -0.229680 -0.595395   \n",
       "20526 -1.106910 -1.222415  0.0  0.000000  0.232451 -0.179140 -0.419577   \n",
       "20527  0.494287  0.494287  0.2  0.090000 -0.541690 -0.241055 -0.369383   \n",
       "20528  0.469538 -0.057472  0.0  0.000000 -0.219750 -0.237609 -0.243115   \n",
       "20529  0.175528  0.406496  0.0  0.000000  0.500231  0.480947  0.101621   \n",
       "20530  0.434735  0.434735 -0.1 -0.450000 -0.662870 -0.346606  0.086197   \n",
       "20531 -1.161732 -0.931759  0.0  0.000000  0.004672  0.335773 -0.038356   \n",
       "20532  0.792706  0.792706  0.0  0.000000 -0.550710  0.514392  0.322395   \n",
       "20533 -0.610604 -0.367762  0.2  0.260000  1.052076  0.356068  0.042597   \n",
       "20534 -1.194507 -1.194507  0.0  0.000000 -0.231460 -0.472326 -0.062120   \n",
       "20535 -1.230047 -0.761863  0.0  0.000000 -0.835937 -0.698695 -0.947556   \n",
       "20536 -1.301136 -1.079480  0.0  0.000000 -0.019485 -0.382241 -0.359022   \n",
       "20537 -0.880835  0.133828 -0.1  0.060000 -0.202520  0.519481 -0.135793   \n",
       "20538 -0.576419 -0.763343  0.0  0.000000 -0.162618 -0.357404 -0.706159   \n",
       "20539 -0.511573 -0.457307  0.0  0.000000  0.918704 -0.060654 -0.510686   \n",
       "20540  0.438801  0.243198 -0.2 -0.365000  0.049060  0.650874  0.241179   \n",
       "20541  0.817801  0.522411  0.0  0.000000 -0.245940 -0.418118 -0.168725   \n",
       "20542 -1.135489 -0.914534  0.2  0.350000 -0.509908 -0.108766 -0.629126   \n",
       "20543 -1.298581 -0.943641  0.1  0.170000  0.928762 -1.063490 -0.873349   \n",
       "20544 -0.357380 -0.357380  0.0  0.000000 -0.103441  0.303119 -0.335505   \n",
       "20545 -1.349302 -1.103415  0.2  0.390000 -0.213983  0.117369 -0.443685   \n",
       "20546 -0.948384 -0.948384  0.0  0.000000  0.339135 -0.363711 -0.403891   \n",
       "20547  0.970899  0.970899 -0.2 -0.190000  0.166738  0.565318  0.017769   \n",
       "20548 -1.302858 -1.198127  0.1  0.110000 -0.570280 -0.005549 -0.148968   \n",
       "\n",
       "          HTOV4     HTOV5  \n",
       "0      0.049345 -0.133097  \n",
       "1     -1.013762 -0.880014  \n",
       "2     -0.844041 -1.027759  \n",
       "3     -0.735110 -1.124374  \n",
       "4     -0.102045 -1.348465  \n",
       "5     -0.354543 -1.052923  \n",
       "6     -0.982911 -1.088941  \n",
       "7     -0.238399 -0.893283  \n",
       "8     -0.768719 -0.691848  \n",
       "9     -0.120775 -1.436423  \n",
       "10    -1.387865 -1.366298  \n",
       "11    -0.450086 -0.763970  \n",
       "12     0.070459  0.098154  \n",
       "13    -0.575793 -1.623556  \n",
       "14    -0.523485 -1.350391  \n",
       "15     0.922803  0.183171  \n",
       "16     0.423865  0.187285  \n",
       "17     0.784938  0.352634  \n",
       "18    -0.195246 -0.556634  \n",
       "19     0.151699  0.618117  \n",
       "20     0.626016  0.315548  \n",
       "21     0.169634  0.200568  \n",
       "22     1.718208  0.555674  \n",
       "23    -1.231753 -1.064305  \n",
       "24     0.053913 -0.011011  \n",
       "25    -0.074948 -0.811466  \n",
       "26     1.914813  0.178448  \n",
       "27    -0.345351 -1.135835  \n",
       "28     0.008080 -0.376151  \n",
       "29    -0.769287 -0.980751  \n",
       "...         ...       ...  \n",
       "20519  0.830444 -0.018948  \n",
       "20520  0.693717  0.380754  \n",
       "20521  0.683810 -0.105641  \n",
       "20522  0.607564  0.394717  \n",
       "20523  0.646729  0.545636  \n",
       "20524  0.032239 -1.143045  \n",
       "20525 -0.459522 -0.948421  \n",
       "20526 -0.758129 -0.618048  \n",
       "20527  0.140847  0.343669  \n",
       "20528  0.571118  0.160088  \n",
       "20529  0.198921  0.396998  \n",
       "20530  0.278969 -0.039616  \n",
       "20531 -0.397424 -0.920494  \n",
       "20532  0.354767  0.800960  \n",
       "20533 -0.360066 -0.692383  \n",
       "20534 -0.086093 -0.668591  \n",
       "20535 -0.826191 -1.189958  \n",
       "20536 -0.257290 -0.628889  \n",
       "20537  0.340057  0.547186  \n",
       "20538 -0.690479 -0.515900  \n",
       "20539 -0.996540 -0.796534  \n",
       "20540  0.379292  0.318197  \n",
       "20541  0.465649  0.309917  \n",
       "20542 -0.814799 -1.148184  \n",
       "20543 -0.925118 -1.565706  \n",
       "20544 -0.229667 -0.831997  \n",
       "20545 -1.089743 -1.113523  \n",
       "20546 -1.062191 -1.085736  \n",
       "20547  0.800681  0.559771  \n",
       "20548 -0.449955 -0.920584  \n",
       "\n",
       "[20549 rows x 16 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TPS_train[y_indices+x_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_TPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually added datasets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6006, 15) (6006, 2) (6006, 1)\n"
     ]
    }
   ],
   "source": [
    "#those manually added\n",
    "nsm = df_S_all[df_S_all['SOURCE'] == 'M'].shape[0]\n",
    "npm = df_P_all[df_P_all['SOURCE'] == 'M'].shape[0]\n",
    "ntm = df_T_all[df_T_all['SOURCE'] == 'M'].shape[0]\n",
    "nnm = df_N_all[df_N_all['SOURCE'] == 'M'].shape[0]\n",
    "\n",
    "#we build a balanced datased - the same portion of regS, regP and tele\n",
    "#we have this count of phases\n",
    "man_samp_count = min(nsm, npm, ntm)\n",
    "\n",
    "#sample TPS dataset, random_state is a seed\n",
    "mssS = df_S_all[df_S_all['SOURCE'] == 'M'].sample(man_samp_count, random_state=11)\n",
    "mssP = df_P_all[df_P_all['SOURCE'] == 'M'].sample(man_samp_count)\n",
    "mssT = df_T_all[df_T_all['SOURCE'] == 'M'].sample(man_samp_count)\n",
    "MTPS_data = pd.concat([mssS, mssP, mssT])\n",
    "\n",
    "\n",
    "#normalize\n",
    "\n",
    "MTPS_data_norm = MTPS_data.copy(deep=True)\n",
    "MTPS_data_norm['INANG1'] /= 90.\n",
    "MTPS_data_norm['INANG3'] /= 90.\n",
    "MTPS_data_norm['HMXMN'] = numpy.log10(MTPS_data['HMXMN'])\n",
    "MTPS_data_norm['HVRATP'] = numpy.log10(MTPS_data['HVRATP'])\n",
    "MTPS_data_norm['HVRAT'] = numpy.log10(MTPS_data['HVRAT'])\n",
    "MTPS_data_norm['HTOV1'] = numpy.log10(MTPS_data['HTOV1'])\n",
    "MTPS_data_norm['HTOV2'] = numpy.log10(MTPS_data['HTOV2'])\n",
    "MTPS_data_norm['HTOV3'] = numpy.log10(MTPS_data['HTOV3'])\n",
    "MTPS_data_norm['HTOV4'] = numpy.log10(MTPS_data['HTOV4'])\n",
    "MTPS_data_norm['HTOV5'] = numpy.log10(MTPS_data['HTOV5'])\n",
    "\n",
    "#manually added noise makes nos sense - we do not sanmple N\n",
    "\n",
    "#lets shuffle dataset\n",
    "MTPS_data_norm = MTPS_data_norm.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "manual_X = MTPS_data_norm[x_indices].values.astype(float)\n",
    "#regS = 0, T/regP = 1\n",
    "manual_Y_TPS_ = numpy.array(numpy.where(MTPS_data_norm[y_indices] == 'regS', 1, 0), dtype=float)\n",
    "manual_Y_TPS = keras.utils.to_categorical(manual_Y_TPS_)\n",
    "\n",
    "print(manual_X.shape, manual_Y_TPS.shape, manual_Y_TPS_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual dataset ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvlscratch/SHI/users/hofman/ML/env/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 2, 3, 3, 2, 2, 1, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_Y_GT = le.transform(MTPS_data_norm[y_indices])\n",
    "manual_Y_GT[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_TPS = {k : [] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = len(x_indices)\n",
    "numpy.random.seed(11)\n",
    "\n",
    "# create model\n",
    "model_TPS = Sequential()\n",
    "model_TPS.add(Dense(6, input_dim=n_input, activation='sigmoid'))\n",
    "model_TPS.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_TPS.compile(\n",
    "    loss = 'binary_crossentropy', \n",
    "    optimizer = 'adam',  # adam, sgd\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 6)                 96        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_TPS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model_TPS.fit(train_X_TPS[:], train_Y_TPS[:], \n",
    "          epochs=5000, \n",
    "          batch_size=1024, \n",
    "          verbose=0,\n",
    "          shuffle=True,\n",
    "          #validation_split=0.1)\n",
    "          validation_data=([test_X_TPS, test_Y_TPS]))\n",
    "\n",
    "history_TPS = {k : history_TPS[k] + h.history[k] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f381f641f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history_TPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NC = nodes correct\n",
    "model_TPS.save('URZ_model_15-6-2_norm_NC_TPS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6850/6850 [==============================] - 0s 8us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21553656952550812, 0.9101459854014599]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TPS.evaluate(test_X_TPS, test_Y_TPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4019  483]\n",
      " [ 132 2216]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = numpy.reshape(numpy.argmax(model_TPS.predict(test_X_TPS), axis=1), (test_X_TPS.shape[0],1))\n",
    "\n",
    "# calculate confusion matrix\n",
    "C = confusion_matrix(test_Y_TPS_, Y_pred)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.02%\n"
     ]
    }
   ],
   "source": [
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just for curiosity -  Manual associations confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3039  965]\n",
      " [ 183 1819]]\n",
      "Accuracy: 80.89%\n"
     ]
    }
   ],
   "source": [
    "Y_pred_man = numpy.argmax(model_TPS.predict(manual_X), axis=1)\n",
    "\n",
    "# calculate confusion matrix\n",
    "C = confusion_matrix(manual_Y_TPS_, Y_pred_man)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network T vs regP \n",
    "\n",
    "* we need a new dataset for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for T vs regP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude regS\n",
    "TP_train = TPS_train[TPS_train['CLASS_PHASE'] != 'regS']\n",
    "TP_test  = TPS_test [TPS_test ['CLASS_PHASE'] != 'regS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13764, 15) (13764, 2) (4502, 15) (4502, 2)\n"
     ]
    }
   ],
   "source": [
    "train_X_TP = TP_train[x_indices].values.astype(float)\n",
    "train_Y_TP = TP_train[y_indices]\n",
    "\n",
    "test_X_TP = TP_test[x_indices].values.astype(float)\n",
    "test_Y_TP = TP_test[y_indices]\n",
    "\n",
    "#regS = 0, T/regP = 1\n",
    "train_Y_TP_ = numpy.array(numpy.where(train_Y_TP['CLASS_PHASE'] == 'regP', 1, 0), dtype=float)\n",
    "test_Y_TP_ = numpy.array(numpy.where(test_Y_TP['CLASS_PHASE'] == 'regP', 1, 0), dtype=float)\n",
    "\n",
    "#convert to categorical\n",
    "train_Y_TP = keras.utils.to_categorical(train_Y_TP_)\n",
    "test_Y_TP = keras.utils.to_categorical(test_Y_TP_)\n",
    "\n",
    "print(train_X_TP.shape, train_Y_TP.shape, test_X_TP.shape, test_Y_TP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if node5 < 0.5 => P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS_PHASE</th>\n",
       "      <th>PER</th>\n",
       "      <th>RECT</th>\n",
       "      <th>PLANS</th>\n",
       "      <th>INANG1</th>\n",
       "      <th>INANG3</th>\n",
       "      <th>HMXMN</th>\n",
       "      <th>HVRATP</th>\n",
       "      <th>HVRAT</th>\n",
       "      <th>NAB</th>\n",
       "      <th>TAB</th>\n",
       "      <th>HTOV1</th>\n",
       "      <th>HTOV2</th>\n",
       "      <th>HTOV3</th>\n",
       "      <th>HTOV4</th>\n",
       "      <th>HTOV5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.895752</td>\n",
       "      <td>0.977953</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.983441</td>\n",
       "      <td>0.838955</td>\n",
       "      <td>-0.981637</td>\n",
       "      <td>-0.810078</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>-0.228505</td>\n",
       "      <td>-0.099400</td>\n",
       "      <td>-0.743478</td>\n",
       "      <td>-1.013762</td>\n",
       "      <td>-0.880014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.983845</td>\n",
       "      <td>0.967418</td>\n",
       "      <td>0.167897</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.402774</td>\n",
       "      <td>-1.273636</td>\n",
       "      <td>-1.329216</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>-0.310373</td>\n",
       "      <td>-0.182101</td>\n",
       "      <td>-0.163527</td>\n",
       "      <td>-0.844041</td>\n",
       "      <td>-1.027759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.986203</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>0.154322</td>\n",
       "      <td>0.822576</td>\n",
       "      <td>0.349078</td>\n",
       "      <td>-1.349539</td>\n",
       "      <td>-1.154446</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.897362</td>\n",
       "      <td>0.112187</td>\n",
       "      <td>-0.106137</td>\n",
       "      <td>-0.735110</td>\n",
       "      <td>-1.124374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.982295</td>\n",
       "      <td>0.974180</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.979892</td>\n",
       "      <td>0.239376</td>\n",
       "      <td>-1.227335</td>\n",
       "      <td>-1.227335</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.211810</td>\n",
       "      <td>0.194740</td>\n",
       "      <td>-0.119883</td>\n",
       "      <td>-0.102045</td>\n",
       "      <td>-1.348465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.838278</td>\n",
       "      <td>0.922653</td>\n",
       "      <td>0.627375</td>\n",
       "      <td>0.661725</td>\n",
       "      <td>0.363572</td>\n",
       "      <td>-0.041421</td>\n",
       "      <td>-0.287866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.362823</td>\n",
       "      <td>0.087695</td>\n",
       "      <td>-0.192810</td>\n",
       "      <td>-0.354543</td>\n",
       "      <td>-1.052923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.891537</td>\n",
       "      <td>0.949444</td>\n",
       "      <td>0.065309</td>\n",
       "      <td>0.955195</td>\n",
       "      <td>0.469561</td>\n",
       "      <td>-0.942423</td>\n",
       "      <td>-0.942423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>0.485974</td>\n",
       "      <td>-0.230462</td>\n",
       "      <td>-0.982911</td>\n",
       "      <td>-1.088941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.782548</td>\n",
       "      <td>0.860514</td>\n",
       "      <td>0.616115</td>\n",
       "      <td>0.994184</td>\n",
       "      <td>0.255811</td>\n",
       "      <td>-0.017308</td>\n",
       "      <td>-0.354208</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>-0.293590</td>\n",
       "      <td>-0.216389</td>\n",
       "      <td>-0.443438</td>\n",
       "      <td>-0.238399</td>\n",
       "      <td>-0.893283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.933073</td>\n",
       "      <td>0.968623</td>\n",
       "      <td>0.061147</td>\n",
       "      <td>0.966952</td>\n",
       "      <td>0.551731</td>\n",
       "      <td>-1.142040</td>\n",
       "      <td>-0.986672</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.346875</td>\n",
       "      <td>-0.220367</td>\n",
       "      <td>-0.494649</td>\n",
       "      <td>-0.768719</td>\n",
       "      <td>-0.691848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.922974</td>\n",
       "      <td>0.869321</td>\n",
       "      <td>0.798127</td>\n",
       "      <td>0.635730</td>\n",
       "      <td>0.635628</td>\n",
       "      <td>0.432552</td>\n",
       "      <td>-0.259575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.569255</td>\n",
       "      <td>-0.207097</td>\n",
       "      <td>-0.120775</td>\n",
       "      <td>-1.436423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.965188</td>\n",
       "      <td>0.913537</td>\n",
       "      <td>0.098021</td>\n",
       "      <td>0.994014</td>\n",
       "      <td>0.278660</td>\n",
       "      <td>-1.325541</td>\n",
       "      <td>-1.164561</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.467754</td>\n",
       "      <td>-0.602633</td>\n",
       "      <td>-0.624759</td>\n",
       "      <td>-1.387865</td>\n",
       "      <td>-1.366298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.920022</td>\n",
       "      <td>0.888794</td>\n",
       "      <td>0.262075</td>\n",
       "      <td>0.836832</td>\n",
       "      <td>0.188466</td>\n",
       "      <td>-0.756622</td>\n",
       "      <td>-0.617293</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.082380</td>\n",
       "      <td>-0.508367</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>-0.450086</td>\n",
       "      <td>-0.763970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.986480</td>\n",
       "      <td>0.960638</td>\n",
       "      <td>0.179946</td>\n",
       "      <td>0.944354</td>\n",
       "      <td>0.524717</td>\n",
       "      <td>-1.251078</td>\n",
       "      <td>-1.150116</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.618987</td>\n",
       "      <td>-0.208024</td>\n",
       "      <td>0.131079</td>\n",
       "      <td>-0.575793</td>\n",
       "      <td>-1.623556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.101944</td>\n",
       "      <td>0.998957</td>\n",
       "      <td>1.629443</td>\n",
       "      <td>-1.684446</td>\n",
       "      <td>-1.684446</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>-0.361433</td>\n",
       "      <td>-0.975744</td>\n",
       "      <td>-0.693025</td>\n",
       "      <td>-0.523485</td>\n",
       "      <td>-1.350391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.964426</td>\n",
       "      <td>0.977247</td>\n",
       "      <td>0.922645</td>\n",
       "      <td>0.135102</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>1.056174</td>\n",
       "      <td>1.360088</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.270000</td>\n",
       "      <td>-0.473737</td>\n",
       "      <td>0.562816</td>\n",
       "      <td>-0.039937</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.183171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tele</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979725</td>\n",
       "      <td>0.956633</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>0.823108</td>\n",
       "      <td>0.336388</td>\n",
       "      <td>-0.617228</td>\n",
       "      <td>-0.847361</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.306154</td>\n",
       "      <td>-0.808939</td>\n",
       "      <td>-0.228066</td>\n",
       "      <td>-0.195246</td>\n",
       "      <td>-0.556634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.973031</td>\n",
       "      <td>0.934159</td>\n",
       "      <td>0.974222</td>\n",
       "      <td>0.912662</td>\n",
       "      <td>1.179741</td>\n",
       "      <td>0.988480</td>\n",
       "      <td>-0.996128</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>1.255939</td>\n",
       "      <td>1.116827</td>\n",
       "      <td>1.424757</td>\n",
       "      <td>1.718208</td>\n",
       "      <td>0.555674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.978958</td>\n",
       "      <td>0.984967</td>\n",
       "      <td>0.091801</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.141551</td>\n",
       "      <td>-1.495710</td>\n",
       "      <td>-1.495710</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.310000</td>\n",
       "      <td>0.525264</td>\n",
       "      <td>-0.549910</td>\n",
       "      <td>0.034540</td>\n",
       "      <td>-1.231753</td>\n",
       "      <td>-1.064305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.821569</td>\n",
       "      <td>0.779235</td>\n",
       "      <td>0.550132</td>\n",
       "      <td>0.488919</td>\n",
       "      <td>0.454480</td>\n",
       "      <td>-0.073833</td>\n",
       "      <td>-0.073833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.688271</td>\n",
       "      <td>-0.071651</td>\n",
       "      <td>-0.311226</td>\n",
       "      <td>-0.074948</td>\n",
       "      <td>-0.811466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.994873</td>\n",
       "      <td>0.977616</td>\n",
       "      <td>0.986941</td>\n",
       "      <td>0.905217</td>\n",
       "      <td>1.382541</td>\n",
       "      <td>1.757578</td>\n",
       "      <td>-0.696409</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.225773</td>\n",
       "      <td>1.141321</td>\n",
       "      <td>1.238418</td>\n",
       "      <td>1.914813</td>\n",
       "      <td>0.178448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.945048</td>\n",
       "      <td>0.900128</td>\n",
       "      <td>0.140150</td>\n",
       "      <td>0.939869</td>\n",
       "      <td>0.318258</td>\n",
       "      <td>-1.094083</td>\n",
       "      <td>-0.908601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952029</td>\n",
       "      <td>0.441782</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.345351</td>\n",
       "      <td>-1.135835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.958260</td>\n",
       "      <td>0.954002</td>\n",
       "      <td>0.087231</td>\n",
       "      <td>0.888122</td>\n",
       "      <td>0.276527</td>\n",
       "      <td>-1.286918</td>\n",
       "      <td>-0.514136</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.657559</td>\n",
       "      <td>-0.041817</td>\n",
       "      <td>-0.298847</td>\n",
       "      <td>-0.769287</td>\n",
       "      <td>-0.980751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.982498</td>\n",
       "      <td>0.959974</td>\n",
       "      <td>0.137761</td>\n",
       "      <td>0.958832</td>\n",
       "      <td>0.170596</td>\n",
       "      <td>-1.374986</td>\n",
       "      <td>-1.229508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.469960</td>\n",
       "      <td>-0.476197</td>\n",
       "      <td>0.153526</td>\n",
       "      <td>-0.848389</td>\n",
       "      <td>-0.974664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.730835</td>\n",
       "      <td>0.739008</td>\n",
       "      <td>0.206120</td>\n",
       "      <td>0.859665</td>\n",
       "      <td>0.247941</td>\n",
       "      <td>-0.481292</td>\n",
       "      <td>-0.481292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542911</td>\n",
       "      <td>0.361733</td>\n",
       "      <td>-0.117338</td>\n",
       "      <td>-0.573643</td>\n",
       "      <td>-0.851548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.873271</td>\n",
       "      <td>0.852025</td>\n",
       "      <td>0.090158</td>\n",
       "      <td>0.989311</td>\n",
       "      <td>0.226249</td>\n",
       "      <td>-0.862271</td>\n",
       "      <td>-0.862271</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>-0.665582</td>\n",
       "      <td>-0.884704</td>\n",
       "      <td>-0.972934</td>\n",
       "      <td>-0.739612</td>\n",
       "      <td>-1.339239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.916020</td>\n",
       "      <td>0.961843</td>\n",
       "      <td>0.082562</td>\n",
       "      <td>0.954397</td>\n",
       "      <td>0.078855</td>\n",
       "      <td>-1.031214</td>\n",
       "      <td>-0.963472</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>-0.436705</td>\n",
       "      <td>-0.352964</td>\n",
       "      <td>-0.326955</td>\n",
       "      <td>-0.680704</td>\n",
       "      <td>-1.062547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.985382</td>\n",
       "      <td>0.980905</td>\n",
       "      <td>0.097206</td>\n",
       "      <td>0.923326</td>\n",
       "      <td>0.205666</td>\n",
       "      <td>-1.575556</td>\n",
       "      <td>-1.484362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.390000</td>\n",
       "      <td>0.289193</td>\n",
       "      <td>-0.232135</td>\n",
       "      <td>-0.477622</td>\n",
       "      <td>-0.971914</td>\n",
       "      <td>-1.488707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.981628</td>\n",
       "      <td>0.981399</td>\n",
       "      <td>0.157113</td>\n",
       "      <td>0.888795</td>\n",
       "      <td>0.345121</td>\n",
       "      <td>-1.295559</td>\n",
       "      <td>-1.295559</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>-0.033019</td>\n",
       "      <td>-0.614935</td>\n",
       "      <td>-0.522799</td>\n",
       "      <td>-0.919634</td>\n",
       "      <td>-1.218342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.971122</td>\n",
       "      <td>0.911523</td>\n",
       "      <td>0.153012</td>\n",
       "      <td>0.950256</td>\n",
       "      <td>0.362807</td>\n",
       "      <td>-1.226926</td>\n",
       "      <td>-0.809436</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>-0.868304</td>\n",
       "      <td>-0.595479</td>\n",
       "      <td>-0.488320</td>\n",
       "      <td>-0.640307</td>\n",
       "      <td>-0.787591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.920533</td>\n",
       "      <td>0.807879</td>\n",
       "      <td>0.136677</td>\n",
       "      <td>0.935923</td>\n",
       "      <td>0.262770</td>\n",
       "      <td>-0.973310</td>\n",
       "      <td>-0.600130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.098028</td>\n",
       "      <td>-0.188480</td>\n",
       "      <td>-0.259520</td>\n",
       "      <td>-0.946282</td>\n",
       "      <td>-0.910104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.928690</td>\n",
       "      <td>0.833274</td>\n",
       "      <td>0.173582</td>\n",
       "      <td>0.995785</td>\n",
       "      <td>0.424657</td>\n",
       "      <td>-0.954127</td>\n",
       "      <td>-0.705301</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.150261</td>\n",
       "      <td>0.141091</td>\n",
       "      <td>-0.542264</td>\n",
       "      <td>-0.169231</td>\n",
       "      <td>-0.795912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.893097</td>\n",
       "      <td>0.950949</td>\n",
       "      <td>0.622031</td>\n",
       "      <td>0.872388</td>\n",
       "      <td>0.204940</td>\n",
       "      <td>-0.046113</td>\n",
       "      <td>-0.046113</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.422479</td>\n",
       "      <td>0.068139</td>\n",
       "      <td>-0.137677</td>\n",
       "      <td>-0.319077</td>\n",
       "      <td>-0.814046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20509</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.978033</td>\n",
       "      <td>0.955111</td>\n",
       "      <td>0.169345</td>\n",
       "      <td>0.928620</td>\n",
       "      <td>0.256237</td>\n",
       "      <td>-1.224556</td>\n",
       "      <td>-1.256307</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>-0.091164</td>\n",
       "      <td>-0.024406</td>\n",
       "      <td>-0.149772</td>\n",
       "      <td>-0.356151</td>\n",
       "      <td>-0.876309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20510</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.967235</td>\n",
       "      <td>0.960333</td>\n",
       "      <td>0.150598</td>\n",
       "      <td>0.886939</td>\n",
       "      <td>0.361920</td>\n",
       "      <td>-1.201613</td>\n",
       "      <td>-1.195713</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.360384</td>\n",
       "      <td>-0.013345</td>\n",
       "      <td>0.165772</td>\n",
       "      <td>-0.676110</td>\n",
       "      <td>-1.361102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20511</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.973659</td>\n",
       "      <td>0.972876</td>\n",
       "      <td>0.135735</td>\n",
       "      <td>0.898491</td>\n",
       "      <td>0.197349</td>\n",
       "      <td>-1.297801</td>\n",
       "      <td>-1.297801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090640</td>\n",
       "      <td>0.195537</td>\n",
       "      <td>-0.010374</td>\n",
       "      <td>-0.337279</td>\n",
       "      <td>-1.240027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20512</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.947543</td>\n",
       "      <td>0.983638</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>0.925163</td>\n",
       "      <td>0.309480</td>\n",
       "      <td>-1.207161</td>\n",
       "      <td>-1.207161</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.263788</td>\n",
       "      <td>-0.089465</td>\n",
       "      <td>-0.586212</td>\n",
       "      <td>-0.938599</td>\n",
       "      <td>-0.975567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20513</th>\n",
       "      <td>tele</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900958</td>\n",
       "      <td>0.881639</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.097856</td>\n",
       "      <td>0.450772</td>\n",
       "      <td>0.701392</td>\n",
       "      <td>0.798190</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.422178</td>\n",
       "      <td>0.085037</td>\n",
       "      <td>-0.123239</td>\n",
       "      <td>0.322957</td>\n",
       "      <td>0.307694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20514</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.689682</td>\n",
       "      <td>0.704871</td>\n",
       "      <td>0.347587</td>\n",
       "      <td>0.977195</td>\n",
       "      <td>0.336129</td>\n",
       "      <td>-0.334212</td>\n",
       "      <td>-0.334212</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-0.386978</td>\n",
       "      <td>-0.185549</td>\n",
       "      <td>-0.256602</td>\n",
       "      <td>-0.112829</td>\n",
       "      <td>-1.305087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20515</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.977919</td>\n",
       "      <td>0.945716</td>\n",
       "      <td>0.196386</td>\n",
       "      <td>0.828689</td>\n",
       "      <td>0.387765</td>\n",
       "      <td>-1.133543</td>\n",
       "      <td>-0.902272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020960</td>\n",
       "      <td>-0.131150</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>0.011983</td>\n",
       "      <td>-1.061307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20517</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.930079</td>\n",
       "      <td>0.961031</td>\n",
       "      <td>0.052059</td>\n",
       "      <td>0.967392</td>\n",
       "      <td>0.061091</td>\n",
       "      <td>-1.132787</td>\n",
       "      <td>-0.812692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447594</td>\n",
       "      <td>0.357473</td>\n",
       "      <td>-0.406407</td>\n",
       "      <td>-0.703884</td>\n",
       "      <td>-0.781950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20518</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.892677</td>\n",
       "      <td>0.836554</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.888704</td>\n",
       "      <td>0.067165</td>\n",
       "      <td>-0.843159</td>\n",
       "      <td>-0.843159</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.170348</td>\n",
       "      <td>0.419794</td>\n",
       "      <td>-0.659436</td>\n",
       "      <td>-1.141316</td>\n",
       "      <td>-1.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20521</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.927863</td>\n",
       "      <td>0.946830</td>\n",
       "      <td>0.990287</td>\n",
       "      <td>0.129494</td>\n",
       "      <td>0.570420</td>\n",
       "      <td>1.006200</td>\n",
       "      <td>1.028407</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>-0.282934</td>\n",
       "      <td>-0.049967</td>\n",
       "      <td>0.818559</td>\n",
       "      <td>0.683810</td>\n",
       "      <td>-0.105641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20522</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.790852</td>\n",
       "      <td>0.951996</td>\n",
       "      <td>0.196908</td>\n",
       "      <td>0.241194</td>\n",
       "      <td>0.153164</td>\n",
       "      <td>-0.574719</td>\n",
       "      <td>0.681176</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>-0.204061</td>\n",
       "      <td>-0.035066</td>\n",
       "      <td>0.224621</td>\n",
       "      <td>0.607564</td>\n",
       "      <td>0.394717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20524</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.829769</td>\n",
       "      <td>0.681364</td>\n",
       "      <td>0.544387</td>\n",
       "      <td>0.751539</td>\n",
       "      <td>0.315041</td>\n",
       "      <td>-0.135514</td>\n",
       "      <td>-0.199418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>-0.256422</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>-0.190338</td>\n",
       "      <td>0.032239</td>\n",
       "      <td>-1.143045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20525</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.916077</td>\n",
       "      <td>0.870980</td>\n",
       "      <td>0.129773</td>\n",
       "      <td>0.964130</td>\n",
       "      <td>0.665348</td>\n",
       "      <td>-0.973008</td>\n",
       "      <td>-0.973008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>-0.518384</td>\n",
       "      <td>-0.229680</td>\n",
       "      <td>-0.595395</td>\n",
       "      <td>-0.459522</td>\n",
       "      <td>-0.948421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20526</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.982854</td>\n",
       "      <td>0.980151</td>\n",
       "      <td>0.212690</td>\n",
       "      <td>0.867554</td>\n",
       "      <td>0.364365</td>\n",
       "      <td>-1.106910</td>\n",
       "      <td>-1.222415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232451</td>\n",
       "      <td>-0.179140</td>\n",
       "      <td>-0.419577</td>\n",
       "      <td>-0.758129</td>\n",
       "      <td>-0.618048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20527</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.690042</td>\n",
       "      <td>0.689347</td>\n",
       "      <td>0.992699</td>\n",
       "      <td>0.113722</td>\n",
       "      <td>0.249848</td>\n",
       "      <td>0.494287</td>\n",
       "      <td>0.494287</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>-0.541690</td>\n",
       "      <td>-0.241055</td>\n",
       "      <td>-0.369383</td>\n",
       "      <td>0.140847</td>\n",
       "      <td>0.343669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20531</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.962077</td>\n",
       "      <td>0.927708</td>\n",
       "      <td>0.154064</td>\n",
       "      <td>0.936396</td>\n",
       "      <td>0.228487</td>\n",
       "      <td>-1.161732</td>\n",
       "      <td>-0.931759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>0.335773</td>\n",
       "      <td>-0.038356</td>\n",
       "      <td>-0.397424</td>\n",
       "      <td>-0.920494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20532</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.923949</td>\n",
       "      <td>0.918772</td>\n",
       "      <td>0.499630</td>\n",
       "      <td>0.572965</td>\n",
       "      <td>0.792706</td>\n",
       "      <td>0.792706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.550710</td>\n",
       "      <td>0.514392</td>\n",
       "      <td>0.322395</td>\n",
       "      <td>0.354767</td>\n",
       "      <td>0.800960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20533</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.785798</td>\n",
       "      <td>0.921016</td>\n",
       "      <td>0.140908</td>\n",
       "      <td>0.868644</td>\n",
       "      <td>0.353315</td>\n",
       "      <td>-0.610604</td>\n",
       "      <td>-0.367762</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>1.052076</td>\n",
       "      <td>0.356068</td>\n",
       "      <td>0.042597</td>\n",
       "      <td>-0.360066</td>\n",
       "      <td>-0.692383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20534</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.952481</td>\n",
       "      <td>0.960376</td>\n",
       "      <td>0.110387</td>\n",
       "      <td>0.892685</td>\n",
       "      <td>0.116087</td>\n",
       "      <td>-1.194507</td>\n",
       "      <td>-1.194507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.231460</td>\n",
       "      <td>-0.472326</td>\n",
       "      <td>-0.062120</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>-0.668591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.950727</td>\n",
       "      <td>0.847848</td>\n",
       "      <td>0.084477</td>\n",
       "      <td>0.824106</td>\n",
       "      <td>0.308328</td>\n",
       "      <td>-1.230047</td>\n",
       "      <td>-0.761863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.835937</td>\n",
       "      <td>-0.698695</td>\n",
       "      <td>-0.947556</td>\n",
       "      <td>-0.826191</td>\n",
       "      <td>-1.189958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20536</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.973802</td>\n",
       "      <td>0.934937</td>\n",
       "      <td>0.135336</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>-1.301136</td>\n",
       "      <td>-1.079480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.019485</td>\n",
       "      <td>-0.382241</td>\n",
       "      <td>-0.359022</td>\n",
       "      <td>-0.257290</td>\n",
       "      <td>-0.628889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20538</th>\n",
       "      <td>regP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941208</td>\n",
       "      <td>0.816890</td>\n",
       "      <td>0.354163</td>\n",
       "      <td>0.982532</td>\n",
       "      <td>0.168560</td>\n",
       "      <td>-0.576419</td>\n",
       "      <td>-0.763343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.162618</td>\n",
       "      <td>-0.357404</td>\n",
       "      <td>-0.706159</td>\n",
       "      <td>-0.690479</td>\n",
       "      <td>-0.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20539</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.881219</td>\n",
       "      <td>0.910499</td>\n",
       "      <td>0.346177</td>\n",
       "      <td>0.722513</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>-0.511573</td>\n",
       "      <td>-0.457307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.918704</td>\n",
       "      <td>-0.060654</td>\n",
       "      <td>-0.510686</td>\n",
       "      <td>-0.996540</td>\n",
       "      <td>-0.796534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.965739</td>\n",
       "      <td>0.970824</td>\n",
       "      <td>0.171642</td>\n",
       "      <td>0.871831</td>\n",
       "      <td>0.417233</td>\n",
       "      <td>-1.135489</td>\n",
       "      <td>-0.914534</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>-0.509908</td>\n",
       "      <td>-0.108766</td>\n",
       "      <td>-0.629126</td>\n",
       "      <td>-0.814799</td>\n",
       "      <td>-1.148184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20543</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.982840</td>\n",
       "      <td>0.992383</td>\n",
       "      <td>0.159445</td>\n",
       "      <td>0.910715</td>\n",
       "      <td>0.018456</td>\n",
       "      <td>-1.298581</td>\n",
       "      <td>-0.943641</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.928762</td>\n",
       "      <td>-1.063490</td>\n",
       "      <td>-0.873349</td>\n",
       "      <td>-0.925118</td>\n",
       "      <td>-1.565706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20544</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.686791</td>\n",
       "      <td>0.769233</td>\n",
       "      <td>0.324196</td>\n",
       "      <td>0.913431</td>\n",
       "      <td>0.453008</td>\n",
       "      <td>-0.357380</td>\n",
       "      <td>-0.357380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.103441</td>\n",
       "      <td>0.303119</td>\n",
       "      <td>-0.335505</td>\n",
       "      <td>-0.229667</td>\n",
       "      <td>-0.831997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20545</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.993218</td>\n",
       "      <td>0.969688</td>\n",
       "      <td>0.170729</td>\n",
       "      <td>0.791223</td>\n",
       "      <td>0.449488</td>\n",
       "      <td>-1.349302</td>\n",
       "      <td>-1.103415</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>-0.213983</td>\n",
       "      <td>0.117369</td>\n",
       "      <td>-0.443685</td>\n",
       "      <td>-1.089743</td>\n",
       "      <td>-1.113523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20546</th>\n",
       "      <td>tele</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.929992</td>\n",
       "      <td>0.916806</td>\n",
       "      <td>0.174894</td>\n",
       "      <td>0.843026</td>\n",
       "      <td>0.198346</td>\n",
       "      <td>-0.948384</td>\n",
       "      <td>-0.948384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339135</td>\n",
       "      <td>-0.363711</td>\n",
       "      <td>-0.403891</td>\n",
       "      <td>-1.062191</td>\n",
       "      <td>-1.085736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20548</th>\n",
       "      <td>regP</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.979784</td>\n",
       "      <td>0.181978</td>\n",
       "      <td>0.942053</td>\n",
       "      <td>0.204177</td>\n",
       "      <td>-1.302858</td>\n",
       "      <td>-1.198127</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>-0.570280</td>\n",
       "      <td>-0.005549</td>\n",
       "      <td>-0.148968</td>\n",
       "      <td>-0.449955</td>\n",
       "      <td>-0.920584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13764 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CLASS_PHASE       PER      RECT     PLANS    INANG1    INANG3     HMXMN  \\\n",
       "1            tele  0.333333  0.895752  0.977953  0.006972  0.983441  0.838955   \n",
       "2            regP  0.166667  0.983845  0.967418  0.167897  0.999756  0.402774   \n",
       "3            tele  0.166667  0.986203  0.974063  0.154322  0.822576  0.349078   \n",
       "4            regP  0.166667  0.982295  0.974180  0.177759  0.979892  0.239376   \n",
       "5            regP  0.166667  0.838278  0.922653  0.627375  0.661725  0.363572   \n",
       "6            tele  0.166667  0.891537  0.949444  0.065309  0.955195  0.469561   \n",
       "7            regP  0.166667  0.782548  0.860514  0.616115  0.994184  0.255811   \n",
       "8            tele  0.444444  0.933073  0.968623  0.061147  0.966952  0.551731   \n",
       "9            tele  0.166667  0.922974  0.869321  0.798127  0.635730  0.635628   \n",
       "10           regP  0.166667  0.965188  0.913537  0.098021  0.994014  0.278660   \n",
       "11           regP  0.333333  0.920022  0.888794  0.262075  0.836832  0.188466   \n",
       "13           regP  0.166667  0.986480  0.960638  0.179946  0.944354  0.524717   \n",
       "14           regP  0.166667  0.992356  0.999967  0.101944  0.998957  1.629443   \n",
       "15           tele  0.444444  0.964426  0.977247  0.922645  0.135102  0.705774   \n",
       "18           tele  1.000000  0.979725  0.956633  0.373444  0.823108  0.336388   \n",
       "22           regP  0.166667  0.973031  0.934159  0.974222  0.912662  1.179741   \n",
       "23           regP  0.444444  0.978958  0.984967  0.091801  0.908891  0.141551   \n",
       "25           regP  0.333333  0.821569  0.779235  0.550132  0.488919  0.454480   \n",
       "26           regP  0.166667  0.994873  0.977616  0.986941  0.905217  1.382541   \n",
       "27           tele  0.333333  0.945048  0.900128  0.140150  0.939869  0.318258   \n",
       "29           tele  0.333333  0.958260  0.954002  0.087231  0.888122  0.276527   \n",
       "30           regP  0.166667  0.982498  0.959974  0.137761  0.958832  0.170596   \n",
       "31           tele  0.333333  0.730835  0.739008  0.206120  0.859665  0.247941   \n",
       "33           tele  0.333333  0.873271  0.852025  0.090158  0.989311  0.226249   \n",
       "35           regP  0.333333  0.916020  0.961843  0.082562  0.954397  0.078855   \n",
       "37           tele  0.333333  0.985382  0.980905  0.097206  0.923326  0.205666   \n",
       "38           regP  0.166667  0.981628  0.981399  0.157113  0.888795  0.345121   \n",
       "39           regP  0.166667  0.971122  0.911523  0.153012  0.950256  0.362807   \n",
       "41           tele  0.333333  0.920533  0.807879  0.136677  0.935923  0.262770   \n",
       "42           regP  0.166667  0.928690  0.833274  0.173582  0.995785  0.424657   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "20508        regP  0.166667  0.893097  0.950949  0.622031  0.872388  0.204940   \n",
       "20509        regP  0.166667  0.978033  0.955111  0.169345  0.928620  0.256237   \n",
       "20510        tele  0.166667  0.967235  0.960333  0.150598  0.886939  0.361920   \n",
       "20511        regP  0.166667  0.973659  0.972876  0.135735  0.898491  0.197349   \n",
       "20512        tele  0.444444  0.947543  0.983638  0.084667  0.925163  0.309480   \n",
       "20513        tele  1.000000  0.900958  0.881639  0.882812  0.097856  0.450772   \n",
       "20514        regP  0.166667  0.689682  0.704871  0.347587  0.977195  0.336129   \n",
       "20515        tele  0.166667  0.977919  0.945716  0.196386  0.828689  0.387765   \n",
       "20517        tele  0.333333  0.930079  0.961031  0.052059  0.967392  0.061091   \n",
       "20518        tele  0.166667  0.892677  0.836554  0.161538  0.888704  0.067165   \n",
       "20521        tele  0.444444  0.927863  0.946830  0.990287  0.129494  0.570420   \n",
       "20522        tele  0.166667  0.790852  0.951996  0.196908  0.241194  0.153164   \n",
       "20524        tele  0.166667  0.829769  0.681364  0.544387  0.751539  0.315041   \n",
       "20525        regP  0.166667  0.916077  0.870980  0.129773  0.964130  0.665348   \n",
       "20526        tele  0.166667  0.982854  0.980151  0.212690  0.867554  0.364365   \n",
       "20527        regP  0.166667  0.690042  0.689347  0.992699  0.113722  0.249848   \n",
       "20531        tele  0.166667  0.962077  0.927708  0.154064  0.936396  0.228487   \n",
       "20532        tele  0.666667  0.929451  0.923949  0.918772  0.499630  0.572965   \n",
       "20533        regP  0.333333  0.785798  0.921016  0.140908  0.868644  0.353315   \n",
       "20534        regP  0.166667  0.952481  0.960376  0.110387  0.892685  0.116087   \n",
       "20535        tele  0.444444  0.950727  0.847848  0.084477  0.824106  0.308328   \n",
       "20536        regP  0.166667  0.973802  0.934937  0.135336  0.829110  0.160964   \n",
       "20538        regP  1.000000  0.941208  0.816890  0.354163  0.982532  0.168560   \n",
       "20539        tele  0.666667  0.881219  0.910499  0.346177  0.722513  0.724409   \n",
       "20542        regP  0.166667  0.965739  0.970824  0.171642  0.871831  0.417233   \n",
       "20543        tele  0.666667  0.982840  0.992383  0.159445  0.910715  0.018456   \n",
       "20544        regP  0.166667  0.686791  0.769233  0.324196  0.913431  0.453008   \n",
       "20545        tele  0.166667  0.993218  0.969688  0.170729  0.791223  0.449488   \n",
       "20546        tele  0.444444  0.929992  0.916806  0.174894  0.843026  0.198346   \n",
       "20548        regP  0.166667  0.993644  0.979784  0.181978  0.942053  0.204177   \n",
       "\n",
       "         HVRATP     HVRAT  NAB       TAB     HTOV1     HTOV2     HTOV3  \\\n",
       "1     -0.981637 -0.810078  0.1  0.090000 -0.228505 -0.099400 -0.743478   \n",
       "2     -1.273636 -1.329216  0.2  0.315000 -0.310373 -0.182101 -0.163527   \n",
       "3     -1.349539 -1.154446  0.1  0.140000  0.897362  0.112187 -0.106137   \n",
       "4     -1.227335 -1.227335  0.1  0.070000  0.211810  0.194740 -0.119883   \n",
       "5     -0.041421 -0.287866  0.0  0.000000 -0.362823  0.087695 -0.192810   \n",
       "6     -0.942423 -0.942423  0.0  0.000000  0.015178  0.485974 -0.230462   \n",
       "7     -0.017308 -0.354208  0.3  0.363333 -0.293590 -0.216389 -0.443438   \n",
       "8     -1.142040 -0.986672  0.1  0.080000  0.346875 -0.220367 -0.494649   \n",
       "9      0.432552 -0.259575  0.0  0.000000  0.009300  0.569255 -0.207097   \n",
       "10    -1.325541 -1.164561  0.1  0.250000  0.467754 -0.602633 -0.624759   \n",
       "11    -0.756622 -0.617293  0.2  0.305000  0.082380 -0.508367  0.006105   \n",
       "13    -1.251078 -1.150116  0.1  0.050000 -0.618987 -0.208024  0.131079   \n",
       "14    -1.684446 -1.684446  0.1  0.230000 -0.361433 -0.975744 -0.693025   \n",
       "15     1.056174  1.360088 -0.1 -0.270000 -0.473737  0.562816 -0.039937   \n",
       "18    -0.617228 -0.847361  0.1  0.100000 -0.306154 -0.808939 -0.228066   \n",
       "22     0.988480 -0.996128 -0.1 -0.350000  1.255939  1.116827  1.424757   \n",
       "23    -1.495710 -1.495710 -0.1 -0.310000  0.525264 -0.549910  0.034540   \n",
       "25    -0.073833 -0.073833  0.0  0.000000 -0.688271 -0.071651 -0.311226   \n",
       "26     1.757578 -0.696409  0.1  0.060000  0.225773  1.141321  1.238418   \n",
       "27    -1.094083 -0.908601  0.0  0.000000  0.952029  0.441782 -0.006021   \n",
       "29    -1.286918 -0.514136  0.1  0.100000 -0.657559 -0.041817 -0.298847   \n",
       "30    -1.374986 -1.229508  0.0  0.000000 -0.469960 -0.476197  0.153526   \n",
       "31    -0.481292 -0.481292  0.0  0.000000  0.542911  0.361733 -0.117338   \n",
       "33    -0.862271 -0.862271  0.1  0.180000 -0.665582 -0.884704 -0.972934   \n",
       "35    -1.031214 -0.963472  0.1  0.270000 -0.436705 -0.352964 -0.326955   \n",
       "37    -1.575556 -1.484362  0.0 -0.390000  0.289193 -0.232135 -0.477622   \n",
       "38    -1.295559 -1.295559  0.2  0.240000 -0.033019 -0.614935 -0.522799   \n",
       "39    -1.226926 -0.809436  0.1  0.440000 -0.868304 -0.595479 -0.488320   \n",
       "41    -0.973310 -0.600130  0.0  0.000000 -0.098028 -0.188480 -0.259520   \n",
       "42    -0.954127 -0.705301  0.2  0.300000 -0.150261  0.141091 -0.542264   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "20508 -0.046113 -0.046113  0.1  0.080000  0.422479  0.068139 -0.137677   \n",
       "20509 -1.224556 -1.256307  0.2  0.255000 -0.091164 -0.024406 -0.149772   \n",
       "20510 -1.201613 -1.195713  0.1  0.500000  0.360384 -0.013345  0.165772   \n",
       "20511 -1.297801 -1.297801  0.0  0.000000 -0.090640  0.195537 -0.010374   \n",
       "20512 -1.207161 -1.207161  0.1  0.200000  0.263788 -0.089465 -0.586212   \n",
       "20513  0.701392  0.798190  0.1  0.150000  0.422178  0.085037 -0.123239   \n",
       "20514 -0.334212 -0.334212  0.1  0.070000 -0.386978 -0.185549 -0.256602   \n",
       "20515 -1.133543 -0.902272  0.0  0.000000  0.020960 -0.131150 -0.037392   \n",
       "20517 -1.132787 -0.812692  0.0  0.000000  0.447594  0.357473 -0.406407   \n",
       "20518 -0.843159 -0.843159  0.2  0.215000  0.170348  0.419794 -0.659436   \n",
       "20521  1.006200  1.028407  0.3  0.386667 -0.282934 -0.049967  0.818559   \n",
       "20522 -0.574719  0.681176  0.1  0.180000 -0.204061 -0.035066  0.224621   \n",
       "20524 -0.135514 -0.199418  0.0 -0.340000 -0.256422  0.032010 -0.190338   \n",
       "20525 -0.973008 -0.973008  0.2  0.395000 -0.518384 -0.229680 -0.595395   \n",
       "20526 -1.106910 -1.222415  0.0  0.000000  0.232451 -0.179140 -0.419577   \n",
       "20527  0.494287  0.494287  0.2  0.090000 -0.541690 -0.241055 -0.369383   \n",
       "20531 -1.161732 -0.931759  0.0  0.000000  0.004672  0.335773 -0.038356   \n",
       "20532  0.792706  0.792706  0.0  0.000000 -0.550710  0.514392  0.322395   \n",
       "20533 -0.610604 -0.367762  0.2  0.260000  1.052076  0.356068  0.042597   \n",
       "20534 -1.194507 -1.194507  0.0  0.000000 -0.231460 -0.472326 -0.062120   \n",
       "20535 -1.230047 -0.761863  0.0  0.000000 -0.835937 -0.698695 -0.947556   \n",
       "20536 -1.301136 -1.079480  0.0  0.000000 -0.019485 -0.382241 -0.359022   \n",
       "20538 -0.576419 -0.763343  0.0  0.000000 -0.162618 -0.357404 -0.706159   \n",
       "20539 -0.511573 -0.457307  0.0  0.000000  0.918704 -0.060654 -0.510686   \n",
       "20542 -1.135489 -0.914534  0.2  0.350000 -0.509908 -0.108766 -0.629126   \n",
       "20543 -1.298581 -0.943641  0.1  0.170000  0.928762 -1.063490 -0.873349   \n",
       "20544 -0.357380 -0.357380  0.0  0.000000 -0.103441  0.303119 -0.335505   \n",
       "20545 -1.349302 -1.103415  0.2  0.390000 -0.213983  0.117369 -0.443685   \n",
       "20546 -0.948384 -0.948384  0.0  0.000000  0.339135 -0.363711 -0.403891   \n",
       "20548 -1.302858 -1.198127  0.1  0.110000 -0.570280 -0.005549 -0.148968   \n",
       "\n",
       "          HTOV4     HTOV5  \n",
       "1     -1.013762 -0.880014  \n",
       "2     -0.844041 -1.027759  \n",
       "3     -0.735110 -1.124374  \n",
       "4     -0.102045 -1.348465  \n",
       "5     -0.354543 -1.052923  \n",
       "6     -0.982911 -1.088941  \n",
       "7     -0.238399 -0.893283  \n",
       "8     -0.768719 -0.691848  \n",
       "9     -0.120775 -1.436423  \n",
       "10    -1.387865 -1.366298  \n",
       "11    -0.450086 -0.763970  \n",
       "13    -0.575793 -1.623556  \n",
       "14    -0.523485 -1.350391  \n",
       "15     0.922803  0.183171  \n",
       "18    -0.195246 -0.556634  \n",
       "22     1.718208  0.555674  \n",
       "23    -1.231753 -1.064305  \n",
       "25    -0.074948 -0.811466  \n",
       "26     1.914813  0.178448  \n",
       "27    -0.345351 -1.135835  \n",
       "29    -0.769287 -0.980751  \n",
       "30    -0.848389 -0.974664  \n",
       "31    -0.573643 -0.851548  \n",
       "33    -0.739612 -1.339239  \n",
       "35    -0.680704 -1.062547  \n",
       "37    -0.971914 -1.488707  \n",
       "38    -0.919634 -1.218342  \n",
       "39    -0.640307 -0.787591  \n",
       "41    -0.946282 -0.910104  \n",
       "42    -0.169231 -0.795912  \n",
       "...         ...       ...  \n",
       "20508 -0.319077 -0.814046  \n",
       "20509 -0.356151 -0.876309  \n",
       "20510 -0.676110 -1.361102  \n",
       "20511 -0.337279 -1.240027  \n",
       "20512 -0.938599 -0.975567  \n",
       "20513  0.322957  0.307694  \n",
       "20514 -0.112829 -1.305087  \n",
       "20515  0.011983 -1.061307  \n",
       "20517 -0.703884 -0.781950  \n",
       "20518 -1.141316 -1.830600  \n",
       "20521  0.683810 -0.105641  \n",
       "20522  0.607564  0.394717  \n",
       "20524  0.032239 -1.143045  \n",
       "20525 -0.459522 -0.948421  \n",
       "20526 -0.758129 -0.618048  \n",
       "20527  0.140847  0.343669  \n",
       "20531 -0.397424 -0.920494  \n",
       "20532  0.354767  0.800960  \n",
       "20533 -0.360066 -0.692383  \n",
       "20534 -0.086093 -0.668591  \n",
       "20535 -0.826191 -1.189958  \n",
       "20536 -0.257290 -0.628889  \n",
       "20538 -0.690479 -0.515900  \n",
       "20539 -0.996540 -0.796534  \n",
       "20542 -0.814799 -1.148184  \n",
       "20543 -0.925118 -1.565706  \n",
       "20544 -0.229667 -0.831997  \n",
       "20545 -1.089743 -1.113523  \n",
       "20546 -1.062191 -1.085736  \n",
       "20548 -0.449955 -0.920584  \n",
       "\n",
       "[13764 rows x 16 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP_train[y_indices + x_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_TP = {k : [] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = len(x_indices)\n",
    "numpy.random.seed(11)\n",
    "\n",
    "# create model\n",
    "model_TP = Sequential()\n",
    "model_TP.add(Dense(6, input_dim=n_input, activation='sigmoid'))\n",
    "model_TP.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_TP.compile(\n",
    "    loss = 'binary_crossentropy', \n",
    "    optimizer = 'adam',  # adam, sgd\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13764 samples, validate on 4502 samples\n",
      "Epoch 1/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.6641 - acc: 0.5986 - val_loss: 0.6428 - val_acc: 0.6205\n",
      "Epoch 2/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.6228 - acc: 0.6487 - val_loss: 0.6010 - val_acc: 0.6727\n",
      "Epoch 3/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.5796 - acc: 0.6999 - val_loss: 0.5666 - val_acc: 0.7086\n",
      "Epoch 4/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.5477 - acc: 0.7292 - val_loss: 0.5433 - val_acc: 0.7272\n",
      "Epoch 5/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.5271 - acc: 0.7484 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 6/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.5136 - acc: 0.7585 - val_loss: 0.5205 - val_acc: 0.7502\n",
      "Epoch 7/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.5050 - acc: 0.7636 - val_loss: 0.5145 - val_acc: 0.7541\n",
      "Epoch 8/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4986 - acc: 0.7667 - val_loss: 0.5103 - val_acc: 0.7583\n",
      "Epoch 9/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4938 - acc: 0.7726 - val_loss: 0.5073 - val_acc: 0.7598\n",
      "Epoch 10/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4899 - acc: 0.7752 - val_loss: 0.5037 - val_acc: 0.7624\n",
      "Epoch 11/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4867 - acc: 0.7775 - val_loss: 0.5010 - val_acc: 0.7651\n",
      "Epoch 12/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4839 - acc: 0.7795 - val_loss: 0.4986 - val_acc: 0.7703\n",
      "Epoch 13/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4814 - acc: 0.7805 - val_loss: 0.4971 - val_acc: 0.7650\n",
      "Epoch 14/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4791 - acc: 0.7827 - val_loss: 0.4946 - val_acc: 0.7720\n",
      "Epoch 15/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4771 - acc: 0.7852 - val_loss: 0.4934 - val_acc: 0.7701\n",
      "Epoch 16/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4752 - acc: 0.7863 - val_loss: 0.4920 - val_acc: 0.7752\n",
      "Epoch 17/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4737 - acc: 0.7877 - val_loss: 0.4899 - val_acc: 0.7757\n",
      "Epoch 18/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4722 - acc: 0.7895 - val_loss: 0.4885 - val_acc: 0.7755\n",
      "Epoch 19/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4710 - acc: 0.7905 - val_loss: 0.4874 - val_acc: 0.7778\n",
      "Epoch 20/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4696 - acc: 0.7914 - val_loss: 0.4863 - val_acc: 0.7788\n",
      "Epoch 21/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4685 - acc: 0.7918 - val_loss: 0.4855 - val_acc: 0.7777\n",
      "Epoch 22/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4675 - acc: 0.7917 - val_loss: 0.4845 - val_acc: 0.7802\n",
      "Epoch 23/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4665 - acc: 0.7931 - val_loss: 0.4838 - val_acc: 0.7780\n",
      "Epoch 24/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4656 - acc: 0.7945 - val_loss: 0.4831 - val_acc: 0.7792\n",
      "Epoch 25/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4648 - acc: 0.7949 - val_loss: 0.4827 - val_acc: 0.7802\n",
      "Epoch 26/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4642 - acc: 0.7952 - val_loss: 0.4824 - val_acc: 0.7808\n",
      "Epoch 27/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4635 - acc: 0.7963 - val_loss: 0.4821 - val_acc: 0.7799\n",
      "Epoch 28/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4630 - acc: 0.7945 - val_loss: 0.4806 - val_acc: 0.7822\n",
      "Epoch 29/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4623 - acc: 0.7943 - val_loss: 0.4802 - val_acc: 0.7812\n",
      "Epoch 30/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4616 - acc: 0.7959 - val_loss: 0.4797 - val_acc: 0.7814\n",
      "Epoch 31/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4611 - acc: 0.7971 - val_loss: 0.4803 - val_acc: 0.7808\n",
      "Epoch 32/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4607 - acc: 0.7968 - val_loss: 0.4792 - val_acc: 0.7825\n",
      "Epoch 33/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4600 - acc: 0.7974 - val_loss: 0.4805 - val_acc: 0.7812\n",
      "Epoch 34/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4599 - acc: 0.7978 - val_loss: 0.4786 - val_acc: 0.7829\n",
      "Epoch 35/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4595 - acc: 0.7969 - val_loss: 0.4781 - val_acc: 0.7822\n",
      "Epoch 36/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4590 - acc: 0.7970 - val_loss: 0.4777 - val_acc: 0.7815\n",
      "Epoch 37/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4588 - acc: 0.7973 - val_loss: 0.4777 - val_acc: 0.7821\n",
      "Epoch 38/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4584 - acc: 0.7973 - val_loss: 0.4773 - val_acc: 0.7815\n",
      "Epoch 39/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4582 - acc: 0.7976 - val_loss: 0.4776 - val_acc: 0.7824\n",
      "Epoch 40/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4579 - acc: 0.7967 - val_loss: 0.4777 - val_acc: 0.7822\n",
      "Epoch 41/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4578 - acc: 0.7977 - val_loss: 0.4772 - val_acc: 0.7827\n",
      "Epoch 42/1000\n",
      "13764/13764 [==============================] - 1s 36us/step - loss: 0.4575 - acc: 0.7965 - val_loss: 0.4770 - val_acc: 0.7823\n",
      "Epoch 43/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4572 - acc: 0.7975 - val_loss: 0.4764 - val_acc: 0.7827\n",
      "Epoch 44/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4569 - acc: 0.7979 - val_loss: 0.4763 - val_acc: 0.7822\n",
      "Epoch 45/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4567 - acc: 0.7980 - val_loss: 0.4773 - val_acc: 0.7840\n",
      "Epoch 46/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4566 - acc: 0.7984 - val_loss: 0.4762 - val_acc: 0.7820\n",
      "Epoch 47/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4564 - acc: 0.7989 - val_loss: 0.4760 - val_acc: 0.7821\n",
      "Epoch 48/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4565 - acc: 0.7984 - val_loss: 0.4758 - val_acc: 0.7823\n",
      "Epoch 49/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4562 - acc: 0.7981 - val_loss: 0.4757 - val_acc: 0.7828\n",
      "Epoch 50/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4562 - acc: 0.7983 - val_loss: 0.4756 - val_acc: 0.7827\n",
      "Epoch 51/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4559 - acc: 0.7980 - val_loss: 0.4756 - val_acc: 0.7832\n",
      "Epoch 52/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4556 - acc: 0.7984 - val_loss: 0.4755 - val_acc: 0.7831\n",
      "Epoch 53/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4556 - acc: 0.7992 - val_loss: 0.4755 - val_acc: 0.7822\n",
      "Epoch 54/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4554 - acc: 0.7981 - val_loss: 0.4752 - val_acc: 0.7831\n",
      "Epoch 55/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4554 - acc: 0.7990 - val_loss: 0.4751 - val_acc: 0.7838\n",
      "Epoch 56/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4552 - acc: 0.7984 - val_loss: 0.4751 - val_acc: 0.7825\n",
      "Epoch 57/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.7976 - val_loss: 0.4748 - val_acc: 0.7828\n",
      "Epoch 58/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.7981 - val_loss: 0.4749 - val_acc: 0.7830\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4550 - acc: 0.7990 - val_loss: 0.4748 - val_acc: 0.7828\n",
      "Epoch 60/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4547 - acc: 0.7985 - val_loss: 0.4746 - val_acc: 0.7829\n",
      "Epoch 61/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4547 - acc: 0.7989 - val_loss: 0.4748 - val_acc: 0.7827\n",
      "Epoch 62/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4544 - acc: 0.7995 - val_loss: 0.4749 - val_acc: 0.7832\n",
      "Epoch 63/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4543 - acc: 0.7986 - val_loss: 0.4750 - val_acc: 0.7831\n",
      "Epoch 64/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4542 - acc: 0.7984 - val_loss: 0.4745 - val_acc: 0.7830\n",
      "Epoch 65/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4542 - acc: 0.7987 - val_loss: 0.4763 - val_acc: 0.7842\n",
      "Epoch 66/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.7994 - val_loss: 0.4742 - val_acc: 0.7831\n",
      "Epoch 67/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.7990 - val_loss: 0.4755 - val_acc: 0.7848\n",
      "Epoch 68/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4537 - acc: 0.7985 - val_loss: 0.4744 - val_acc: 0.7833\n",
      "Epoch 69/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4537 - acc: 0.7995 - val_loss: 0.4739 - val_acc: 0.7835\n",
      "Epoch 70/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4537 - acc: 0.7993 - val_loss: 0.4743 - val_acc: 0.7842\n",
      "Epoch 71/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4535 - acc: 0.7982 - val_loss: 0.4741 - val_acc: 0.7835\n",
      "Epoch 72/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4533 - acc: 0.7985 - val_loss: 0.4739 - val_acc: 0.7829\n",
      "Epoch 73/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4531 - acc: 0.7983 - val_loss: 0.4736 - val_acc: 0.7825\n",
      "Epoch 74/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4528 - acc: 0.7994 - val_loss: 0.4765 - val_acc: 0.7840\n",
      "Epoch 75/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4529 - acc: 0.7994 - val_loss: 0.4734 - val_acc: 0.7837\n",
      "Epoch 76/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.7999 - val_loss: 0.4743 - val_acc: 0.7852\n",
      "Epoch 77/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.7997 - val_loss: 0.4737 - val_acc: 0.7830\n",
      "Epoch 78/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4526 - acc: 0.7984 - val_loss: 0.4739 - val_acc: 0.7847\n",
      "Epoch 79/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4523 - acc: 0.7991 - val_loss: 0.4731 - val_acc: 0.7829\n",
      "Epoch 80/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4522 - acc: 0.8002 - val_loss: 0.4733 - val_acc: 0.7828\n",
      "Epoch 81/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4522 - acc: 0.8002 - val_loss: 0.4731 - val_acc: 0.7835\n",
      "Epoch 82/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4517 - acc: 0.7999 - val_loss: 0.4729 - val_acc: 0.7828\n",
      "Epoch 83/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4519 - acc: 0.8003 - val_loss: 0.4728 - val_acc: 0.7842\n",
      "Epoch 84/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4513 - acc: 0.8013 - val_loss: 0.4734 - val_acc: 0.7830\n",
      "Epoch 85/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4517 - acc: 0.8000 - val_loss: 0.4737 - val_acc: 0.7851\n",
      "Epoch 86/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4516 - acc: 0.8009 - val_loss: 0.4724 - val_acc: 0.7834\n",
      "Epoch 87/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4513 - acc: 0.8011 - val_loss: 0.4729 - val_acc: 0.7840\n",
      "Epoch 88/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4510 - acc: 0.8008 - val_loss: 0.4747 - val_acc: 0.7841\n",
      "Epoch 89/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4509 - acc: 0.8006 - val_loss: 0.4725 - val_acc: 0.7843\n",
      "Epoch 90/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4507 - acc: 0.8021 - val_loss: 0.4725 - val_acc: 0.7847\n",
      "Epoch 91/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4508 - acc: 0.8015 - val_loss: 0.4717 - val_acc: 0.7831\n",
      "Epoch 92/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4505 - acc: 0.8017 - val_loss: 0.4740 - val_acc: 0.7844\n",
      "Epoch 93/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4504 - acc: 0.8011 - val_loss: 0.4717 - val_acc: 0.7843\n",
      "Epoch 94/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4502 - acc: 0.8003 - val_loss: 0.4722 - val_acc: 0.7845\n",
      "Epoch 95/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4501 - acc: 0.8009 - val_loss: 0.4714 - val_acc: 0.7840\n",
      "Epoch 96/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4498 - acc: 0.8017 - val_loss: 0.4713 - val_acc: 0.7841\n",
      "Epoch 97/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4497 - acc: 0.8017 - val_loss: 0.4712 - val_acc: 0.7835\n",
      "Epoch 98/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4496 - acc: 0.8019 - val_loss: 0.4713 - val_acc: 0.7840\n",
      "Epoch 99/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4491 - acc: 0.8020 - val_loss: 0.4714 - val_acc: 0.7845\n",
      "Epoch 100/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4494 - acc: 0.8027 - val_loss: 0.4716 - val_acc: 0.7854\n",
      "Epoch 101/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4492 - acc: 0.8020 - val_loss: 0.4710 - val_acc: 0.7842\n",
      "Epoch 102/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4487 - acc: 0.8024 - val_loss: 0.4708 - val_acc: 0.7831\n",
      "Epoch 103/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4488 - acc: 0.8027 - val_loss: 0.4707 - val_acc: 0.7842\n",
      "Epoch 104/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4484 - acc: 0.8027 - val_loss: 0.4705 - val_acc: 0.7845\n",
      "Epoch 105/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4483 - acc: 0.8038 - val_loss: 0.4726 - val_acc: 0.7860\n",
      "Epoch 106/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4480 - acc: 0.8046 - val_loss: 0.4702 - val_acc: 0.7837\n",
      "Epoch 107/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4480 - acc: 0.8039 - val_loss: 0.4703 - val_acc: 0.7844\n",
      "Epoch 108/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4478 - acc: 0.8029 - val_loss: 0.4700 - val_acc: 0.7849\n",
      "Epoch 109/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4477 - acc: 0.8037 - val_loss: 0.4705 - val_acc: 0.7848\n",
      "Epoch 110/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4475 - acc: 0.8043 - val_loss: 0.4697 - val_acc: 0.7844\n",
      "Epoch 111/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4474 - acc: 0.8041 - val_loss: 0.4697 - val_acc: 0.7851\n",
      "Epoch 112/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4472 - acc: 0.8046 - val_loss: 0.4697 - val_acc: 0.7844\n",
      "Epoch 113/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4472 - acc: 0.8049 - val_loss: 0.4693 - val_acc: 0.7840\n",
      "Epoch 114/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4469 - acc: 0.8043 - val_loss: 0.4698 - val_acc: 0.7852\n",
      "Epoch 115/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4467 - acc: 0.8039 - val_loss: 0.4692 - val_acc: 0.7848\n",
      "Epoch 116/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4464 - acc: 0.8053 - val_loss: 0.4694 - val_acc: 0.7849\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4462 - acc: 0.8050 - val_loss: 0.4689 - val_acc: 0.7853\n",
      "Epoch 118/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4460 - acc: 0.8048 - val_loss: 0.4713 - val_acc: 0.7875\n",
      "Epoch 119/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4461 - acc: 0.8053 - val_loss: 0.4697 - val_acc: 0.7875\n",
      "Epoch 120/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4460 - acc: 0.8058 - val_loss: 0.4686 - val_acc: 0.7858\n",
      "Epoch 121/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4457 - acc: 0.8058 - val_loss: 0.4689 - val_acc: 0.7859\n",
      "Epoch 122/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4456 - acc: 0.8068 - val_loss: 0.4698 - val_acc: 0.7872\n",
      "Epoch 123/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4454 - acc: 0.8053 - val_loss: 0.4691 - val_acc: 0.7864\n",
      "Epoch 124/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4453 - acc: 0.8069 - val_loss: 0.4684 - val_acc: 0.7850\n",
      "Epoch 125/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4451 - acc: 0.8065 - val_loss: 0.4689 - val_acc: 0.7864\n",
      "Epoch 126/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4450 - acc: 0.8065 - val_loss: 0.4682 - val_acc: 0.7851\n",
      "Epoch 127/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4449 - acc: 0.8063 - val_loss: 0.4680 - val_acc: 0.7851\n",
      "Epoch 128/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4449 - acc: 0.8060 - val_loss: 0.4678 - val_acc: 0.7854\n",
      "Epoch 129/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4446 - acc: 0.8068 - val_loss: 0.4676 - val_acc: 0.7861\n",
      "Epoch 130/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4444 - acc: 0.8082 - val_loss: 0.4679 - val_acc: 0.7855\n",
      "Epoch 131/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4441 - acc: 0.8064 - val_loss: 0.4685 - val_acc: 0.7863\n",
      "Epoch 132/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4442 - acc: 0.8071 - val_loss: 0.4685 - val_acc: 0.7873\n",
      "Epoch 133/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4441 - acc: 0.8074 - val_loss: 0.4674 - val_acc: 0.7853\n",
      "Epoch 134/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4440 - acc: 0.8069 - val_loss: 0.4673 - val_acc: 0.7854\n",
      "Epoch 135/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4438 - acc: 0.8081 - val_loss: 0.4676 - val_acc: 0.7860\n",
      "Epoch 136/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4437 - acc: 0.8075 - val_loss: 0.4670 - val_acc: 0.7855\n",
      "Epoch 137/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4435 - acc: 0.8069 - val_loss: 0.4669 - val_acc: 0.7863\n",
      "Epoch 138/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4436 - acc: 0.8071 - val_loss: 0.4669 - val_acc: 0.7857\n",
      "Epoch 139/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4434 - acc: 0.8069 - val_loss: 0.4670 - val_acc: 0.7863\n",
      "Epoch 140/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4433 - acc: 0.8073 - val_loss: 0.4666 - val_acc: 0.7861\n",
      "Epoch 141/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4431 - acc: 0.8074 - val_loss: 0.4666 - val_acc: 0.7865\n",
      "Epoch 142/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4431 - acc: 0.8065 - val_loss: 0.4668 - val_acc: 0.7860\n",
      "Epoch 143/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4429 - acc: 0.8071 - val_loss: 0.4666 - val_acc: 0.7858\n",
      "Epoch 144/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4429 - acc: 0.8080 - val_loss: 0.4665 - val_acc: 0.7859\n",
      "Epoch 145/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4428 - acc: 0.8082 - val_loss: 0.4663 - val_acc: 0.7867\n",
      "Epoch 146/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4426 - acc: 0.8076 - val_loss: 0.4665 - val_acc: 0.7857\n",
      "Epoch 147/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4425 - acc: 0.8075 - val_loss: 0.4671 - val_acc: 0.7873\n",
      "Epoch 148/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4423 - acc: 0.8084 - val_loss: 0.4665 - val_acc: 0.7879\n",
      "Epoch 149/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4423 - acc: 0.8079 - val_loss: 0.4662 - val_acc: 0.7861\n",
      "Epoch 150/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4422 - acc: 0.8086 - val_loss: 0.4661 - val_acc: 0.7872\n",
      "Epoch 151/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4422 - acc: 0.8078 - val_loss: 0.4659 - val_acc: 0.7869\n",
      "Epoch 152/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4419 - acc: 0.8081 - val_loss: 0.4659 - val_acc: 0.7875\n",
      "Epoch 153/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4418 - acc: 0.8079 - val_loss: 0.4674 - val_acc: 0.7881\n",
      "Epoch 154/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4419 - acc: 0.8091 - val_loss: 0.4660 - val_acc: 0.7868\n",
      "Epoch 155/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4419 - acc: 0.8082 - val_loss: 0.4662 - val_acc: 0.7875\n",
      "Epoch 156/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4416 - acc: 0.8077 - val_loss: 0.4661 - val_acc: 0.7873\n",
      "Epoch 157/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4417 - acc: 0.8097 - val_loss: 0.4659 - val_acc: 0.7864\n",
      "Epoch 158/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4415 - acc: 0.8084 - val_loss: 0.4655 - val_acc: 0.7883\n",
      "Epoch 159/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4416 - acc: 0.8086 - val_loss: 0.4656 - val_acc: 0.7870\n",
      "Epoch 160/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4414 - acc: 0.8083 - val_loss: 0.4654 - val_acc: 0.7881\n",
      "Epoch 161/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4412 - acc: 0.8084 - val_loss: 0.4657 - val_acc: 0.7871\n",
      "Epoch 162/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4413 - acc: 0.8085 - val_loss: 0.4654 - val_acc: 0.7884\n",
      "Epoch 163/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4413 - acc: 0.8087 - val_loss: 0.4654 - val_acc: 0.7864\n",
      "Epoch 164/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4411 - acc: 0.8082 - val_loss: 0.4655 - val_acc: 0.7873\n",
      "Epoch 165/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4411 - acc: 0.8091 - val_loss: 0.4653 - val_acc: 0.7873\n",
      "Epoch 166/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4410 - acc: 0.8087 - val_loss: 0.4653 - val_acc: 0.7881\n",
      "Epoch 167/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4408 - acc: 0.8086 - val_loss: 0.4653 - val_acc: 0.7880\n",
      "Epoch 168/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4408 - acc: 0.8090 - val_loss: 0.4652 - val_acc: 0.7886\n",
      "Epoch 169/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4408 - acc: 0.8094 - val_loss: 0.4652 - val_acc: 0.7883\n",
      "Epoch 170/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4407 - acc: 0.8087 - val_loss: 0.4655 - val_acc: 0.7874\n",
      "Epoch 171/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4407 - acc: 0.8089 - val_loss: 0.4649 - val_acc: 0.7881\n",
      "Epoch 172/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4407 - acc: 0.8073 - val_loss: 0.4652 - val_acc: 0.7880\n",
      "Epoch 173/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4404 - acc: 0.8087 - val_loss: 0.4647 - val_acc: 0.7892\n",
      "Epoch 174/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4405 - acc: 0.8095 - val_loss: 0.4648 - val_acc: 0.7889\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4402 - acc: 0.8094 - val_loss: 0.4651 - val_acc: 0.7880\n",
      "Epoch 176/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4402 - acc: 0.8101 - val_loss: 0.4648 - val_acc: 0.7876\n",
      "Epoch 177/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4403 - acc: 0.8097 - val_loss: 0.4648 - val_acc: 0.7872\n",
      "Epoch 178/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4402 - acc: 0.8085 - val_loss: 0.4648 - val_acc: 0.7876\n",
      "Epoch 179/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4399 - acc: 0.8087 - val_loss: 0.4647 - val_acc: 0.7890\n",
      "Epoch 180/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4402 - acc: 0.8087 - val_loss: 0.4648 - val_acc: 0.7880\n",
      "Epoch 181/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4400 - acc: 0.8088 - val_loss: 0.4647 - val_acc: 0.7882\n",
      "Epoch 182/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4401 - acc: 0.8079 - val_loss: 0.4648 - val_acc: 0.7876\n",
      "Epoch 183/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4401 - acc: 0.8086 - val_loss: 0.4645 - val_acc: 0.7898\n",
      "Epoch 184/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4397 - acc: 0.8092 - val_loss: 0.4665 - val_acc: 0.7895\n",
      "Epoch 185/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4395 - acc: 0.8107 - val_loss: 0.4647 - val_acc: 0.7881\n",
      "Epoch 186/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4396 - acc: 0.8107 - val_loss: 0.4646 - val_acc: 0.7896\n",
      "Epoch 187/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4398 - acc: 0.8088 - val_loss: 0.4643 - val_acc: 0.7892\n",
      "Epoch 188/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4396 - acc: 0.8099 - val_loss: 0.4644 - val_acc: 0.7884\n",
      "Epoch 189/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4397 - acc: 0.8106 - val_loss: 0.4646 - val_acc: 0.7881\n",
      "Epoch 190/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4395 - acc: 0.8089 - val_loss: 0.4643 - val_acc: 0.7888\n",
      "Epoch 191/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4393 - acc: 0.8100 - val_loss: 0.4642 - val_acc: 0.7893\n",
      "Epoch 192/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4393 - acc: 0.8102 - val_loss: 0.4645 - val_acc: 0.7883\n",
      "Epoch 193/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4392 - acc: 0.8094 - val_loss: 0.4640 - val_acc: 0.7896\n",
      "Epoch 194/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4391 - acc: 0.8101 - val_loss: 0.4648 - val_acc: 0.7889\n",
      "Epoch 195/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4394 - acc: 0.8100 - val_loss: 0.4640 - val_acc: 0.7891\n",
      "Epoch 196/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4390 - acc: 0.8102 - val_loss: 0.4645 - val_acc: 0.7890\n",
      "Epoch 197/1000\n",
      "13764/13764 [==============================] - ETA: 0s - loss: 0.4410 - acc: 0.808 - 0s 26us/step - loss: 0.4391 - acc: 0.8098 - val_loss: 0.4640 - val_acc: 0.7883\n",
      "Epoch 198/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4389 - acc: 0.8089 - val_loss: 0.4645 - val_acc: 0.7893\n",
      "Epoch 199/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4389 - acc: 0.8101 - val_loss: 0.4639 - val_acc: 0.7883\n",
      "Epoch 200/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4388 - acc: 0.8091 - val_loss: 0.4638 - val_acc: 0.7901\n",
      "Epoch 201/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4388 - acc: 0.8095 - val_loss: 0.4639 - val_acc: 0.7892\n",
      "Epoch 202/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4388 - acc: 0.8094 - val_loss: 0.4641 - val_acc: 0.7890\n",
      "Epoch 203/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4387 - acc: 0.8086 - val_loss: 0.4637 - val_acc: 0.7896\n",
      "Epoch 204/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4386 - acc: 0.8104 - val_loss: 0.4637 - val_acc: 0.7898\n",
      "Epoch 205/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4386 - acc: 0.8089 - val_loss: 0.4636 - val_acc: 0.7894\n",
      "Epoch 206/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4384 - acc: 0.8088 - val_loss: 0.4636 - val_acc: 0.7889\n",
      "Epoch 207/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4384 - acc: 0.8091 - val_loss: 0.4639 - val_acc: 0.7884\n",
      "Epoch 208/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4383 - acc: 0.8096 - val_loss: 0.4643 - val_acc: 0.7896\n",
      "Epoch 209/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4384 - acc: 0.8083 - val_loss: 0.4636 - val_acc: 0.7878\n",
      "Epoch 210/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4382 - acc: 0.8100 - val_loss: 0.4635 - val_acc: 0.7890\n",
      "Epoch 211/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4382 - acc: 0.8096 - val_loss: 0.4634 - val_acc: 0.7900\n",
      "Epoch 212/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4380 - acc: 0.8093 - val_loss: 0.4633 - val_acc: 0.7899\n",
      "Epoch 213/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4380 - acc: 0.8097 - val_loss: 0.4637 - val_acc: 0.7890\n",
      "Epoch 214/1000\n",
      "13764/13764 [==============================] - 1s 56us/step - loss: 0.4379 - acc: 0.8102 - val_loss: 0.4634 - val_acc: 0.7888\n",
      "Epoch 215/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4379 - acc: 0.8078 - val_loss: 0.4633 - val_acc: 0.7904\n",
      "Epoch 216/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4377 - acc: 0.8098 - val_loss: 0.4632 - val_acc: 0.7901\n",
      "Epoch 217/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4379 - acc: 0.8085 - val_loss: 0.4630 - val_acc: 0.7903\n",
      "Epoch 218/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4376 - acc: 0.8095 - val_loss: 0.4630 - val_acc: 0.7901\n",
      "Epoch 219/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4375 - acc: 0.8093 - val_loss: 0.4632 - val_acc: 0.7892\n",
      "Epoch 220/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4374 - acc: 0.8090 - val_loss: 0.4631 - val_acc: 0.7893\n",
      "Epoch 221/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4373 - acc: 0.8106 - val_loss: 0.4641 - val_acc: 0.7906\n",
      "Epoch 222/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4372 - acc: 0.8099 - val_loss: 0.4632 - val_acc: 0.7874\n",
      "Epoch 223/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4373 - acc: 0.8099 - val_loss: 0.4630 - val_acc: 0.7911\n",
      "Epoch 224/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4373 - acc: 0.8102 - val_loss: 0.4631 - val_acc: 0.7891\n",
      "Epoch 225/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4371 - acc: 0.8084 - val_loss: 0.4632 - val_acc: 0.7894\n",
      "Epoch 226/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4371 - acc: 0.8088 - val_loss: 0.4633 - val_acc: 0.7898\n",
      "Epoch 227/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4370 - acc: 0.8096 - val_loss: 0.4638 - val_acc: 0.7908\n",
      "Epoch 228/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4370 - acc: 0.8089 - val_loss: 0.4631 - val_acc: 0.7886\n",
      "Epoch 229/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4370 - acc: 0.8096 - val_loss: 0.4635 - val_acc: 0.7906\n",
      "Epoch 230/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4368 - acc: 0.8100 - val_loss: 0.4629 - val_acc: 0.7890\n",
      "Epoch 231/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4368 - acc: 0.8104 - val_loss: 0.4630 - val_acc: 0.7889\n",
      "Epoch 232/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4368 - acc: 0.8096 - val_loss: 0.4631 - val_acc: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4364 - acc: 0.8079 - val_loss: 0.4648 - val_acc: 0.7920\n",
      "Epoch 234/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4364 - acc: 0.8088 - val_loss: 0.4628 - val_acc: 0.7892\n",
      "Epoch 235/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4364 - acc: 0.8096 - val_loss: 0.4628 - val_acc: 0.7882\n",
      "Epoch 236/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4363 - acc: 0.8090 - val_loss: 0.4632 - val_acc: 0.7893\n",
      "Epoch 237/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4364 - acc: 0.8091 - val_loss: 0.4638 - val_acc: 0.7908\n",
      "Epoch 238/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4363 - acc: 0.8098 - val_loss: 0.4628 - val_acc: 0.7892\n",
      "Epoch 239/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4360 - acc: 0.8086 - val_loss: 0.4629 - val_acc: 0.7889\n",
      "Epoch 240/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4361 - acc: 0.8088 - val_loss: 0.4633 - val_acc: 0.7900\n",
      "Epoch 241/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4361 - acc: 0.8082 - val_loss: 0.4630 - val_acc: 0.7894\n",
      "Epoch 242/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4359 - acc: 0.8086 - val_loss: 0.4628 - val_acc: 0.7895\n",
      "Epoch 243/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4359 - acc: 0.8097 - val_loss: 0.4630 - val_acc: 0.7884\n",
      "Epoch 244/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4357 - acc: 0.8090 - val_loss: 0.4627 - val_acc: 0.7891\n",
      "Epoch 245/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4357 - acc: 0.8093 - val_loss: 0.4625 - val_acc: 0.7876\n",
      "Epoch 246/1000\n",
      "13764/13764 [==============================] - 1s 46us/step - loss: 0.4356 - acc: 0.8093 - val_loss: 0.4631 - val_acc: 0.7904\n",
      "Epoch 247/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4355 - acc: 0.8102 - val_loss: 0.4627 - val_acc: 0.7902\n",
      "Epoch 248/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4355 - acc: 0.8099 - val_loss: 0.4626 - val_acc: 0.7892\n",
      "Epoch 249/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4355 - acc: 0.8104 - val_loss: 0.4625 - val_acc: 0.7880\n",
      "Epoch 250/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4354 - acc: 0.8102 - val_loss: 0.4627 - val_acc: 0.7882\n",
      "Epoch 251/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4353 - acc: 0.8094 - val_loss: 0.4631 - val_acc: 0.7909\n",
      "Epoch 252/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4351 - acc: 0.8109 - val_loss: 0.4625 - val_acc: 0.7892\n",
      "Epoch 253/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4351 - acc: 0.8104 - val_loss: 0.4624 - val_acc: 0.7885\n",
      "Epoch 254/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4351 - acc: 0.8108 - val_loss: 0.4632 - val_acc: 0.7910\n",
      "Epoch 255/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4351 - acc: 0.8095 - val_loss: 0.4624 - val_acc: 0.7885\n",
      "Epoch 256/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4351 - acc: 0.8098 - val_loss: 0.4631 - val_acc: 0.7895\n",
      "Epoch 257/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4348 - acc: 0.8100 - val_loss: 0.4624 - val_acc: 0.7876\n",
      "Epoch 258/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4348 - acc: 0.8102 - val_loss: 0.4625 - val_acc: 0.7885\n",
      "Epoch 259/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4348 - acc: 0.8101 - val_loss: 0.4622 - val_acc: 0.7883\n",
      "Epoch 260/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4348 - acc: 0.8099 - val_loss: 0.4628 - val_acc: 0.7900\n",
      "Epoch 261/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4345 - acc: 0.8106 - val_loss: 0.4623 - val_acc: 0.7879\n",
      "Epoch 262/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4346 - acc: 0.8103 - val_loss: 0.4629 - val_acc: 0.7903\n",
      "Epoch 263/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4348 - acc: 0.8102 - val_loss: 0.4623 - val_acc: 0.7879\n",
      "Epoch 264/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4345 - acc: 0.8103 - val_loss: 0.4623 - val_acc: 0.7880\n",
      "Epoch 265/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4344 - acc: 0.8105 - val_loss: 0.4625 - val_acc: 0.7880\n",
      "Epoch 266/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4343 - acc: 0.8102 - val_loss: 0.4635 - val_acc: 0.7923\n",
      "Epoch 267/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4342 - acc: 0.8100 - val_loss: 0.4624 - val_acc: 0.7896\n",
      "Epoch 268/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4343 - acc: 0.8108 - val_loss: 0.4622 - val_acc: 0.7879\n",
      "Epoch 269/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4339 - acc: 0.8106 - val_loss: 0.4630 - val_acc: 0.7900\n",
      "Epoch 270/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4342 - acc: 0.8099 - val_loss: 0.4627 - val_acc: 0.7891\n",
      "Epoch 271/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4340 - acc: 0.8110 - val_loss: 0.4623 - val_acc: 0.7885\n",
      "Epoch 272/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4340 - acc: 0.8111 - val_loss: 0.4628 - val_acc: 0.7891\n",
      "Epoch 273/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4340 - acc: 0.8104 - val_loss: 0.4631 - val_acc: 0.7905\n",
      "Epoch 274/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4340 - acc: 0.8106 - val_loss: 0.4635 - val_acc: 0.7914\n",
      "Epoch 275/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4340 - acc: 0.8103 - val_loss: 0.4624 - val_acc: 0.7883\n",
      "Epoch 276/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4337 - acc: 0.8107 - val_loss: 0.4623 - val_acc: 0.7889\n",
      "Epoch 277/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4338 - acc: 0.8100 - val_loss: 0.4620 - val_acc: 0.7893\n",
      "Epoch 278/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4337 - acc: 0.8105 - val_loss: 0.4621 - val_acc: 0.7892\n",
      "Epoch 279/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4335 - acc: 0.8101 - val_loss: 0.4639 - val_acc: 0.7914\n",
      "Epoch 280/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4338 - acc: 0.8108 - val_loss: 0.4625 - val_acc: 0.7893\n",
      "Epoch 281/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4337 - acc: 0.8099 - val_loss: 0.4624 - val_acc: 0.7909\n",
      "Epoch 282/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4334 - acc: 0.8114 - val_loss: 0.4627 - val_acc: 0.7893\n",
      "Epoch 283/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4335 - acc: 0.8103 - val_loss: 0.4621 - val_acc: 0.7894\n",
      "Epoch 284/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4335 - acc: 0.8114 - val_loss: 0.4621 - val_acc: 0.7895\n",
      "Epoch 285/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4335 - acc: 0.8109 - val_loss: 0.4623 - val_acc: 0.7896\n",
      "Epoch 286/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4331 - acc: 0.8108 - val_loss: 0.4630 - val_acc: 0.7871\n",
      "Epoch 287/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4333 - acc: 0.8108 - val_loss: 0.4620 - val_acc: 0.7903\n",
      "Epoch 288/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4333 - acc: 0.8106 - val_loss: 0.4622 - val_acc: 0.7898\n",
      "Epoch 289/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4332 - acc: 0.8107 - val_loss: 0.4629 - val_acc: 0.7896\n",
      "Epoch 290/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4332 - acc: 0.8113 - val_loss: 0.4620 - val_acc: 0.7906\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4332 - acc: 0.8098 - val_loss: 0.4626 - val_acc: 0.7900\n",
      "Epoch 292/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4333 - acc: 0.8107 - val_loss: 0.4624 - val_acc: 0.7901\n",
      "Epoch 293/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4331 - acc: 0.8104 - val_loss: 0.4623 - val_acc: 0.7895\n",
      "Epoch 294/1000\n",
      "13764/13764 [==============================] - 1s 46us/step - loss: 0.4331 - acc: 0.8105 - val_loss: 0.4622 - val_acc: 0.7908\n",
      "Epoch 295/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4330 - acc: 0.8099 - val_loss: 0.4624 - val_acc: 0.7895\n",
      "Epoch 296/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4330 - acc: 0.8103 - val_loss: 0.4621 - val_acc: 0.7900\n",
      "Epoch 297/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4329 - acc: 0.8116 - val_loss: 0.4623 - val_acc: 0.7898\n",
      "Epoch 298/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4328 - acc: 0.8110 - val_loss: 0.4629 - val_acc: 0.7904\n",
      "Epoch 299/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4330 - acc: 0.8107 - val_loss: 0.4620 - val_acc: 0.7903\n",
      "Epoch 300/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4328 - acc: 0.8111 - val_loss: 0.4621 - val_acc: 0.7904\n",
      "Epoch 301/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4330 - acc: 0.8107 - val_loss: 0.4622 - val_acc: 0.7901\n",
      "Epoch 302/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4327 - acc: 0.8107 - val_loss: 0.4622 - val_acc: 0.7910\n",
      "Epoch 303/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4328 - acc: 0.8107 - val_loss: 0.4624 - val_acc: 0.7898\n",
      "Epoch 304/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4328 - acc: 0.8108 - val_loss: 0.4621 - val_acc: 0.7916\n",
      "Epoch 305/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4328 - acc: 0.8108 - val_loss: 0.4625 - val_acc: 0.7909\n",
      "Epoch 306/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4327 - acc: 0.8120 - val_loss: 0.4621 - val_acc: 0.7904\n",
      "Epoch 307/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4326 - acc: 0.8114 - val_loss: 0.4623 - val_acc: 0.7899\n",
      "Epoch 308/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4326 - acc: 0.8116 - val_loss: 0.4622 - val_acc: 0.7906\n",
      "Epoch 309/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4326 - acc: 0.8113 - val_loss: 0.4621 - val_acc: 0.7900\n",
      "Epoch 310/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4325 - acc: 0.8103 - val_loss: 0.4624 - val_acc: 0.7895\n",
      "Epoch 311/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4322 - acc: 0.8103 - val_loss: 0.4637 - val_acc: 0.7916\n",
      "Epoch 312/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4324 - acc: 0.8101 - val_loss: 0.4641 - val_acc: 0.7920\n",
      "Epoch 313/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4326 - acc: 0.8103 - val_loss: 0.4624 - val_acc: 0.7906\n",
      "Epoch 314/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4323 - acc: 0.8111 - val_loss: 0.4630 - val_acc: 0.7909\n",
      "Epoch 315/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4324 - acc: 0.8103 - val_loss: 0.4632 - val_acc: 0.7912\n",
      "Epoch 316/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4324 - acc: 0.8108 - val_loss: 0.4621 - val_acc: 0.7908\n",
      "Epoch 317/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4323 - acc: 0.8110 - val_loss: 0.4621 - val_acc: 0.7898\n",
      "Epoch 318/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4325 - acc: 0.8110 - val_loss: 0.4619 - val_acc: 0.7905\n",
      "Epoch 319/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4322 - acc: 0.8119 - val_loss: 0.4629 - val_acc: 0.7894\n",
      "Epoch 320/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4324 - acc: 0.8108 - val_loss: 0.4621 - val_acc: 0.7924\n",
      "Epoch 321/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4323 - acc: 0.8112 - val_loss: 0.4623 - val_acc: 0.7898\n",
      "Epoch 322/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4323 - acc: 0.8104 - val_loss: 0.4622 - val_acc: 0.7900\n",
      "Epoch 323/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4323 - acc: 0.8110 - val_loss: 0.4623 - val_acc: 0.7901\n",
      "Epoch 324/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4323 - acc: 0.8100 - val_loss: 0.4621 - val_acc: 0.7919\n",
      "Epoch 325/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4321 - acc: 0.8102 - val_loss: 0.4622 - val_acc: 0.7900\n",
      "Epoch 326/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4321 - acc: 0.8115 - val_loss: 0.4619 - val_acc: 0.7911\n",
      "Epoch 327/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4321 - acc: 0.8108 - val_loss: 0.4624 - val_acc: 0.7904\n",
      "Epoch 328/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4321 - acc: 0.8107 - val_loss: 0.4626 - val_acc: 0.7904\n",
      "Epoch 329/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4319 - acc: 0.8110 - val_loss: 0.4622 - val_acc: 0.7915\n",
      "Epoch 330/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4322 - acc: 0.8118 - val_loss: 0.4621 - val_acc: 0.7916\n",
      "Epoch 331/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4320 - acc: 0.8117 - val_loss: 0.4652 - val_acc: 0.7925\n",
      "Epoch 332/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4322 - acc: 0.8100 - val_loss: 0.4620 - val_acc: 0.7920\n",
      "Epoch 333/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4317 - acc: 0.8123 - val_loss: 0.4630 - val_acc: 0.7909\n",
      "Epoch 334/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4321 - acc: 0.8104 - val_loss: 0.4620 - val_acc: 0.7930\n",
      "Epoch 335/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4318 - acc: 0.8110 - val_loss: 0.4626 - val_acc: 0.7905\n",
      "Epoch 336/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4319 - acc: 0.8104 - val_loss: 0.4625 - val_acc: 0.7901\n",
      "Epoch 337/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4319 - acc: 0.8111 - val_loss: 0.4621 - val_acc: 0.7905\n",
      "Epoch 338/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4317 - acc: 0.8117 - val_loss: 0.4625 - val_acc: 0.7911\n",
      "Epoch 339/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4315 - acc: 0.8109 - val_loss: 0.4620 - val_acc: 0.7915\n",
      "Epoch 340/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4319 - acc: 0.8107 - val_loss: 0.4621 - val_acc: 0.7919\n",
      "Epoch 341/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4317 - acc: 0.8122 - val_loss: 0.4620 - val_acc: 0.7916\n",
      "Epoch 342/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4318 - acc: 0.8109 - val_loss: 0.4624 - val_acc: 0.7905\n",
      "Epoch 343/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4319 - acc: 0.8108 - val_loss: 0.4621 - val_acc: 0.7922\n",
      "Epoch 344/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4318 - acc: 0.8120 - val_loss: 0.4621 - val_acc: 0.7918\n",
      "Epoch 345/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4317 - acc: 0.8121 - val_loss: 0.4621 - val_acc: 0.7924\n",
      "Epoch 346/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4317 - acc: 0.8109 - val_loss: 0.4628 - val_acc: 0.7901\n",
      "Epoch 347/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4317 - acc: 0.8118 - val_loss: 0.4620 - val_acc: 0.7914\n",
      "Epoch 348/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4317 - acc: 0.8116 - val_loss: 0.4620 - val_acc: 0.7920\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4315 - acc: 0.8107 - val_loss: 0.4626 - val_acc: 0.7903\n",
      "Epoch 350/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4317 - acc: 0.8104 - val_loss: 0.4622 - val_acc: 0.7913\n",
      "Epoch 351/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4316 - acc: 0.8115 - val_loss: 0.4620 - val_acc: 0.7914\n",
      "Epoch 352/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4317 - acc: 0.8107 - val_loss: 0.4620 - val_acc: 0.7918\n",
      "Epoch 353/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4317 - acc: 0.8107 - val_loss: 0.4627 - val_acc: 0.7900\n",
      "Epoch 354/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4316 - acc: 0.8102 - val_loss: 0.4623 - val_acc: 0.7928\n",
      "Epoch 355/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4315 - acc: 0.8118 - val_loss: 0.4627 - val_acc: 0.7916\n",
      "Epoch 356/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4315 - acc: 0.8118 - val_loss: 0.4623 - val_acc: 0.7918\n",
      "Epoch 357/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4316 - acc: 0.8121 - val_loss: 0.4622 - val_acc: 0.7913\n",
      "Epoch 358/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4315 - acc: 0.8104 - val_loss: 0.4627 - val_acc: 0.7916\n",
      "Epoch 359/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4315 - acc: 0.8114 - val_loss: 0.4628 - val_acc: 0.7912\n",
      "Epoch 360/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4314 - acc: 0.8117 - val_loss: 0.4621 - val_acc: 0.7926\n",
      "Epoch 361/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4315 - acc: 0.8105 - val_loss: 0.4620 - val_acc: 0.7934\n",
      "Epoch 362/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4314 - acc: 0.8120 - val_loss: 0.4620 - val_acc: 0.7934\n",
      "Epoch 363/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4315 - acc: 0.8122 - val_loss: 0.4621 - val_acc: 0.7918\n",
      "Epoch 364/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4314 - acc: 0.8111 - val_loss: 0.4621 - val_acc: 0.7924\n",
      "Epoch 365/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4314 - acc: 0.8106 - val_loss: 0.4629 - val_acc: 0.7912\n",
      "Epoch 366/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4313 - acc: 0.8115 - val_loss: 0.4620 - val_acc: 0.7932\n",
      "Epoch 367/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4314 - acc: 0.8108 - val_loss: 0.4619 - val_acc: 0.7922\n",
      "Epoch 368/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4315 - acc: 0.8107 - val_loss: 0.4620 - val_acc: 0.7914\n",
      "Epoch 369/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4313 - acc: 0.8103 - val_loss: 0.4627 - val_acc: 0.7913\n",
      "Epoch 370/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4313 - acc: 0.8108 - val_loss: 0.4623 - val_acc: 0.7919\n",
      "Epoch 371/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4313 - acc: 0.8107 - val_loss: 0.4625 - val_acc: 0.7915\n",
      "Epoch 372/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4312 - acc: 0.8112 - val_loss: 0.4621 - val_acc: 0.7928\n",
      "Epoch 373/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4313 - acc: 0.8109 - val_loss: 0.4623 - val_acc: 0.7921\n",
      "Epoch 374/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4313 - acc: 0.8122 - val_loss: 0.4620 - val_acc: 0.7929\n",
      "Epoch 375/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4312 - acc: 0.8112 - val_loss: 0.4623 - val_acc: 0.7918\n",
      "Epoch 376/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4312 - acc: 0.8118 - val_loss: 0.4623 - val_acc: 0.7912\n",
      "Epoch 377/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4312 - acc: 0.8118 - val_loss: 0.4622 - val_acc: 0.7905\n",
      "Epoch 378/1000\n",
      "13764/13764 [==============================] - 1s 36us/step - loss: 0.4313 - acc: 0.8114 - val_loss: 0.4619 - val_acc: 0.7921\n",
      "Epoch 379/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4313 - acc: 0.8105 - val_loss: 0.4620 - val_acc: 0.7921\n",
      "Epoch 380/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4311 - acc: 0.8104 - val_loss: 0.4620 - val_acc: 0.7931\n",
      "Epoch 381/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4312 - acc: 0.8115 - val_loss: 0.4630 - val_acc: 0.7914\n",
      "Epoch 382/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4312 - acc: 0.8119 - val_loss: 0.4621 - val_acc: 0.7936\n",
      "Epoch 383/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4311 - acc: 0.8112 - val_loss: 0.4619 - val_acc: 0.7925\n",
      "Epoch 384/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4311 - acc: 0.8115 - val_loss: 0.4621 - val_acc: 0.7916\n",
      "Epoch 385/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4310 - acc: 0.8120 - val_loss: 0.4620 - val_acc: 0.7929\n",
      "Epoch 386/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4310 - acc: 0.8114 - val_loss: 0.4624 - val_acc: 0.7921\n",
      "Epoch 387/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4311 - acc: 0.8123 - val_loss: 0.4621 - val_acc: 0.7939\n",
      "Epoch 388/1000\n",
      "13764/13764 [==============================] - 1s 42us/step - loss: 0.4309 - acc: 0.8111 - val_loss: 0.4634 - val_acc: 0.7919\n",
      "Epoch 389/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4313 - acc: 0.8104 - val_loss: 0.4631 - val_acc: 0.7921\n",
      "Epoch 390/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4310 - acc: 0.8115 - val_loss: 0.4624 - val_acc: 0.7925\n",
      "Epoch 391/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4309 - acc: 0.8103 - val_loss: 0.4621 - val_acc: 0.7916\n",
      "Epoch 392/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4310 - acc: 0.8115 - val_loss: 0.4619 - val_acc: 0.7934\n",
      "Epoch 393/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4309 - acc: 0.8106 - val_loss: 0.4624 - val_acc: 0.7916\n",
      "Epoch 394/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4310 - acc: 0.8108 - val_loss: 0.4623 - val_acc: 0.7912\n",
      "Epoch 395/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4309 - acc: 0.8100 - val_loss: 0.4619 - val_acc: 0.7923\n",
      "Epoch 396/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4309 - acc: 0.8115 - val_loss: 0.4625 - val_acc: 0.7920\n",
      "Epoch 397/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4310 - acc: 0.8118 - val_loss: 0.4619 - val_acc: 0.7938\n",
      "Epoch 398/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4308 - acc: 0.8119 - val_loss: 0.4622 - val_acc: 0.7915\n",
      "Epoch 399/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4310 - acc: 0.8116 - val_loss: 0.4619 - val_acc: 0.7933\n",
      "Epoch 400/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4308 - acc: 0.8117 - val_loss: 0.4623 - val_acc: 0.7915\n",
      "Epoch 401/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4308 - acc: 0.8118 - val_loss: 0.4624 - val_acc: 0.7910\n",
      "Epoch 402/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4308 - acc: 0.8106 - val_loss: 0.4625 - val_acc: 0.7919\n",
      "Epoch 403/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4308 - acc: 0.8116 - val_loss: 0.4622 - val_acc: 0.7914\n",
      "Epoch 404/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4309 - acc: 0.8112 - val_loss: 0.4621 - val_acc: 0.7932\n",
      "Epoch 405/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4308 - acc: 0.8106 - val_loss: 0.4619 - val_acc: 0.7923\n",
      "Epoch 406/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4307 - acc: 0.8112 - val_loss: 0.4622 - val_acc: 0.7923\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4309 - acc: 0.8112 - val_loss: 0.4619 - val_acc: 0.7926\n",
      "Epoch 408/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4307 - acc: 0.8119 - val_loss: 0.4621 - val_acc: 0.7936\n",
      "Epoch 409/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4307 - acc: 0.8125 - val_loss: 0.4621 - val_acc: 0.7924\n",
      "Epoch 410/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4306 - acc: 0.8119 - val_loss: 0.4643 - val_acc: 0.7928\n",
      "Epoch 411/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4306 - acc: 0.8120 - val_loss: 0.4626 - val_acc: 0.7920\n",
      "Epoch 412/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4307 - acc: 0.8112 - val_loss: 0.4620 - val_acc: 0.7932\n",
      "Epoch 413/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4306 - acc: 0.8115 - val_loss: 0.4620 - val_acc: 0.7950\n",
      "Epoch 414/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4307 - acc: 0.8114 - val_loss: 0.4626 - val_acc: 0.7928\n",
      "Epoch 415/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4306 - acc: 0.8104 - val_loss: 0.4621 - val_acc: 0.7943\n",
      "Epoch 416/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4307 - acc: 0.8116 - val_loss: 0.4620 - val_acc: 0.7912\n",
      "Epoch 417/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4308 - acc: 0.8121 - val_loss: 0.4619 - val_acc: 0.7930\n",
      "Epoch 418/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4309 - acc: 0.8117 - val_loss: 0.4621 - val_acc: 0.7920\n",
      "Epoch 419/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4306 - acc: 0.8123 - val_loss: 0.4633 - val_acc: 0.7916\n",
      "Epoch 420/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4306 - acc: 0.8123 - val_loss: 0.4617 - val_acc: 0.7928\n",
      "Epoch 421/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4306 - acc: 0.8113 - val_loss: 0.4619 - val_acc: 0.7914\n",
      "Epoch 422/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4307 - acc: 0.8120 - val_loss: 0.4618 - val_acc: 0.7928\n",
      "Epoch 423/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4306 - acc: 0.8107 - val_loss: 0.4623 - val_acc: 0.7918\n",
      "Epoch 424/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4306 - acc: 0.8121 - val_loss: 0.4619 - val_acc: 0.7928\n",
      "Epoch 425/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4307 - acc: 0.8107 - val_loss: 0.4619 - val_acc: 0.7928\n",
      "Epoch 426/1000\n",
      "13764/13764 [==============================] - 1s 41us/step - loss: 0.4305 - acc: 0.8110 - val_loss: 0.4624 - val_acc: 0.7919\n",
      "Epoch 427/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4306 - acc: 0.8113 - val_loss: 0.4630 - val_acc: 0.7921\n",
      "Epoch 428/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4307 - acc: 0.8116 - val_loss: 0.4620 - val_acc: 0.7919\n",
      "Epoch 429/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4306 - acc: 0.8121 - val_loss: 0.4618 - val_acc: 0.7929\n",
      "Epoch 430/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4304 - acc: 0.8121 - val_loss: 0.4617 - val_acc: 0.7921\n",
      "Epoch 431/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4306 - acc: 0.8119 - val_loss: 0.4620 - val_acc: 0.7925\n",
      "Epoch 432/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4306 - acc: 0.8108 - val_loss: 0.4621 - val_acc: 0.7923\n",
      "Epoch 433/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4304 - acc: 0.8118 - val_loss: 0.4617 - val_acc: 0.7934\n",
      "Epoch 434/1000\n",
      "13764/13764 [==============================] - 0s 19us/step - loss: 0.4306 - acc: 0.8112 - val_loss: 0.4617 - val_acc: 0.7932\n",
      "Epoch 435/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4305 - acc: 0.8116 - val_loss: 0.4618 - val_acc: 0.7921\n",
      "Epoch 436/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4304 - acc: 0.8118 - val_loss: 0.4621 - val_acc: 0.7921\n",
      "Epoch 437/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4303 - acc: 0.8118 - val_loss: 0.4618 - val_acc: 0.7936\n",
      "Epoch 438/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4304 - acc: 0.8117 - val_loss: 0.4620 - val_acc: 0.7928\n",
      "Epoch 439/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4307 - acc: 0.8104 - val_loss: 0.4619 - val_acc: 0.7923\n",
      "Epoch 440/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4304 - acc: 0.8107 - val_loss: 0.4618 - val_acc: 0.7928\n",
      "Epoch 441/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4304 - acc: 0.8113 - val_loss: 0.4618 - val_acc: 0.7932\n",
      "Epoch 442/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4304 - acc: 0.8114 - val_loss: 0.4618 - val_acc: 0.7924\n",
      "Epoch 443/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4304 - acc: 0.8107 - val_loss: 0.4618 - val_acc: 0.7939\n",
      "Epoch 444/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4306 - acc: 0.8113 - val_loss: 0.4617 - val_acc: 0.7932\n",
      "Epoch 445/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4304 - acc: 0.8116 - val_loss: 0.4617 - val_acc: 0.7930\n",
      "Epoch 446/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4304 - acc: 0.8114 - val_loss: 0.4620 - val_acc: 0.7921\n",
      "Epoch 447/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4303 - acc: 0.8113 - val_loss: 0.4617 - val_acc: 0.7930\n",
      "Epoch 448/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4303 - acc: 0.8117 - val_loss: 0.4618 - val_acc: 0.7913\n",
      "Epoch 449/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4304 - acc: 0.8117 - val_loss: 0.4625 - val_acc: 0.7921\n",
      "Epoch 450/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4301 - acc: 0.8112 - val_loss: 0.4617 - val_acc: 0.7930\n",
      "Epoch 451/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4304 - acc: 0.8115 - val_loss: 0.4619 - val_acc: 0.7929\n",
      "Epoch 452/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4303 - acc: 0.8113 - val_loss: 0.4620 - val_acc: 0.7923\n",
      "Epoch 453/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4303 - acc: 0.8125 - val_loss: 0.4622 - val_acc: 0.7914\n",
      "Epoch 454/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4303 - acc: 0.8110 - val_loss: 0.4617 - val_acc: 0.7923\n",
      "Epoch 455/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8115 - val_loss: 0.4619 - val_acc: 0.7930\n",
      "Epoch 456/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4303 - acc: 0.8115 - val_loss: 0.4617 - val_acc: 0.7940\n",
      "Epoch 457/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4305 - acc: 0.8115 - val_loss: 0.4616 - val_acc: 0.7928\n",
      "Epoch 458/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4302 - acc: 0.8115 - val_loss: 0.4621 - val_acc: 0.7916\n",
      "Epoch 459/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4304 - acc: 0.8119 - val_loss: 0.4617 - val_acc: 0.7930\n",
      "Epoch 460/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4302 - acc: 0.8116 - val_loss: 0.4617 - val_acc: 0.7932\n",
      "Epoch 461/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4302 - acc: 0.8107 - val_loss: 0.4618 - val_acc: 0.7928\n",
      "Epoch 462/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4304 - acc: 0.8106 - val_loss: 0.4621 - val_acc: 0.7919\n",
      "Epoch 463/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4301 - acc: 0.8119 - val_loss: 0.4616 - val_acc: 0.7923\n",
      "Epoch 464/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4301 - acc: 0.8108 - val_loss: 0.4628 - val_acc: 0.7923\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4303 - acc: 0.8108 - val_loss: 0.4636 - val_acc: 0.7914\n",
      "Epoch 466/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4302 - acc: 0.8110 - val_loss: 0.4616 - val_acc: 0.7925\n",
      "Epoch 467/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4303 - acc: 0.8112 - val_loss: 0.4616 - val_acc: 0.7925\n",
      "Epoch 468/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4302 - acc: 0.8115 - val_loss: 0.4618 - val_acc: 0.7928\n",
      "Epoch 469/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8098 - val_loss: 0.4634 - val_acc: 0.7923\n",
      "Epoch 470/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4303 - acc: 0.8118 - val_loss: 0.4616 - val_acc: 0.7928\n",
      "Epoch 471/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4301 - acc: 0.8109 - val_loss: 0.4626 - val_acc: 0.7919\n",
      "Epoch 472/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8117 - val_loss: 0.4618 - val_acc: 0.7910\n",
      "Epoch 473/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4300 - acc: 0.8114 - val_loss: 0.4634 - val_acc: 0.7923\n",
      "Epoch 474/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4303 - acc: 0.8111 - val_loss: 0.4616 - val_acc: 0.7930\n",
      "Epoch 475/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4301 - acc: 0.8107 - val_loss: 0.4620 - val_acc: 0.7908\n",
      "Epoch 476/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8120 - val_loss: 0.4627 - val_acc: 0.7928\n",
      "Epoch 477/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4300 - acc: 0.8118 - val_loss: 0.4616 - val_acc: 0.7925\n",
      "Epoch 478/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8110 - val_loss: 0.4620 - val_acc: 0.7921\n",
      "Epoch 479/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4299 - acc: 0.8122 - val_loss: 0.4626 - val_acc: 0.7930\n",
      "Epoch 480/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4302 - acc: 0.8118 - val_loss: 0.4615 - val_acc: 0.7934\n",
      "Epoch 481/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8120 - val_loss: 0.4618 - val_acc: 0.7914\n",
      "Epoch 482/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8120 - val_loss: 0.4620 - val_acc: 0.7925\n",
      "Epoch 483/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4301 - acc: 0.8115 - val_loss: 0.4618 - val_acc: 0.7934\n",
      "Epoch 484/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8129 - val_loss: 0.4614 - val_acc: 0.7925\n",
      "Epoch 485/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4299 - acc: 0.8109 - val_loss: 0.4615 - val_acc: 0.7932\n",
      "Epoch 486/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4301 - acc: 0.8114 - val_loss: 0.4616 - val_acc: 0.7934\n",
      "Epoch 487/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4301 - acc: 0.8110 - val_loss: 0.4619 - val_acc: 0.7929\n",
      "Epoch 488/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4298 - acc: 0.8110 - val_loss: 0.4618 - val_acc: 0.7923\n",
      "Epoch 489/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4301 - acc: 0.8112 - val_loss: 0.4618 - val_acc: 0.7930\n",
      "Epoch 490/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8114 - val_loss: 0.4621 - val_acc: 0.7928\n",
      "Epoch 491/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4299 - acc: 0.8120 - val_loss: 0.4632 - val_acc: 0.7925\n",
      "Epoch 492/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4301 - acc: 0.8118 - val_loss: 0.4621 - val_acc: 0.7923\n",
      "Epoch 493/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4299 - acc: 0.8120 - val_loss: 0.4627 - val_acc: 0.7932\n",
      "Epoch 494/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4300 - acc: 0.8107 - val_loss: 0.4618 - val_acc: 0.7928\n",
      "Epoch 495/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4299 - acc: 0.8113 - val_loss: 0.4615 - val_acc: 0.7941\n",
      "Epoch 496/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8116 - val_loss: 0.4615 - val_acc: 0.7939\n",
      "Epoch 497/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4298 - acc: 0.8114 - val_loss: 0.4626 - val_acc: 0.7934\n",
      "Epoch 498/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4300 - acc: 0.8113 - val_loss: 0.4615 - val_acc: 0.7934\n",
      "Epoch 499/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4298 - acc: 0.8115 - val_loss: 0.4615 - val_acc: 0.7936\n",
      "Epoch 500/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4298 - acc: 0.8120 - val_loss: 0.4616 - val_acc: 0.7936\n",
      "Epoch 501/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4299 - acc: 0.8117 - val_loss: 0.4615 - val_acc: 0.7930\n",
      "Epoch 502/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4299 - acc: 0.8119 - val_loss: 0.4615 - val_acc: 0.7943\n",
      "Epoch 503/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4299 - acc: 0.8117 - val_loss: 0.4615 - val_acc: 0.7943\n",
      "Epoch 504/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4298 - acc: 0.8123 - val_loss: 0.4617 - val_acc: 0.7925\n",
      "Epoch 505/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4298 - acc: 0.8121 - val_loss: 0.4615 - val_acc: 0.7934\n",
      "Epoch 506/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8107 - val_loss: 0.4615 - val_acc: 0.7930\n",
      "Epoch 507/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4298 - acc: 0.8119 - val_loss: 0.4614 - val_acc: 0.7931\n",
      "Epoch 508/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4299 - acc: 0.8118 - val_loss: 0.4616 - val_acc: 0.7923\n",
      "Epoch 509/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4298 - acc: 0.8110 - val_loss: 0.4614 - val_acc: 0.7928\n",
      "Epoch 510/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4298 - acc: 0.8114 - val_loss: 0.4613 - val_acc: 0.7930\n",
      "Epoch 511/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4296 - acc: 0.8108 - val_loss: 0.4630 - val_acc: 0.7925\n",
      "Epoch 512/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4298 - acc: 0.8115 - val_loss: 0.4615 - val_acc: 0.7941\n",
      "Epoch 513/1000\n",
      "13764/13764 [==============================] - 0s 19us/step - loss: 0.4297 - acc: 0.8113 - val_loss: 0.4614 - val_acc: 0.7925\n",
      "Epoch 514/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4297 - acc: 0.8122 - val_loss: 0.4624 - val_acc: 0.7923\n",
      "Epoch 515/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4298 - acc: 0.8113 - val_loss: 0.4614 - val_acc: 0.7930\n",
      "Epoch 516/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8115 - val_loss: 0.4614 - val_acc: 0.7932\n",
      "Epoch 517/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4297 - acc: 0.8116 - val_loss: 0.4618 - val_acc: 0.7923\n",
      "Epoch 518/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4299 - acc: 0.8115 - val_loss: 0.4614 - val_acc: 0.7930\n",
      "Epoch 519/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4298 - acc: 0.8111 - val_loss: 0.4616 - val_acc: 0.7930\n",
      "Epoch 520/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4297 - acc: 0.8109 - val_loss: 0.4613 - val_acc: 0.7925\n",
      "Epoch 521/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4297 - acc: 0.8115 - val_loss: 0.4614 - val_acc: 0.7934\n",
      "Epoch 522/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4295 - acc: 0.8120 - val_loss: 0.4619 - val_acc: 0.7928\n",
      "Epoch 523/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8118 - val_loss: 0.4614 - val_acc: 0.7941\n",
      "Epoch 524/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4297 - acc: 0.8119 - val_loss: 0.4615 - val_acc: 0.7930\n",
      "Epoch 525/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4297 - acc: 0.8115 - val_loss: 0.4616 - val_acc: 0.7939\n",
      "Epoch 526/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4613 - val_acc: 0.7932\n",
      "Epoch 527/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4299 - acc: 0.8110 - val_loss: 0.4615 - val_acc: 0.7938\n",
      "Epoch 528/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4297 - acc: 0.8124 - val_loss: 0.4616 - val_acc: 0.7932\n",
      "Epoch 529/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4296 - acc: 0.8119 - val_loss: 0.4613 - val_acc: 0.7928\n",
      "Epoch 530/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4297 - acc: 0.8106 - val_loss: 0.4613 - val_acc: 0.7932\n",
      "Epoch 531/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4299 - acc: 0.8107 - val_loss: 0.4618 - val_acc: 0.7941\n",
      "Epoch 532/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4296 - acc: 0.8127 - val_loss: 0.4619 - val_acc: 0.7921\n",
      "Epoch 533/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4615 - val_acc: 0.7928\n",
      "Epoch 534/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4295 - acc: 0.8118 - val_loss: 0.4621 - val_acc: 0.7921\n",
      "Epoch 535/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4297 - acc: 0.8120 - val_loss: 0.4613 - val_acc: 0.7939\n",
      "Epoch 536/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4297 - acc: 0.8124 - val_loss: 0.4614 - val_acc: 0.7923\n",
      "Epoch 537/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8112 - val_loss: 0.4622 - val_acc: 0.7921\n",
      "Epoch 538/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4296 - acc: 0.8117 - val_loss: 0.4614 - val_acc: 0.7934\n",
      "Epoch 539/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4296 - acc: 0.8123 - val_loss: 0.4612 - val_acc: 0.7934\n",
      "Epoch 540/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4296 - acc: 0.8121 - val_loss: 0.4612 - val_acc: 0.7936\n",
      "Epoch 541/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4296 - acc: 0.8120 - val_loss: 0.4613 - val_acc: 0.7945\n",
      "Epoch 542/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4296 - acc: 0.8122 - val_loss: 0.4613 - val_acc: 0.7941\n",
      "Epoch 543/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4296 - acc: 0.8116 - val_loss: 0.4612 - val_acc: 0.7930\n",
      "Epoch 544/1000\n",
      "13764/13764 [==============================] - 0s 19us/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4613 - val_acc: 0.7934\n",
      "Epoch 545/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8110 - val_loss: 0.4612 - val_acc: 0.7928\n",
      "Epoch 546/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4295 - acc: 0.8111 - val_loss: 0.4612 - val_acc: 0.7925\n",
      "Epoch 547/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8119 - val_loss: 0.4618 - val_acc: 0.7936\n",
      "Epoch 548/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4296 - acc: 0.8111 - val_loss: 0.4612 - val_acc: 0.7934\n",
      "Epoch 549/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4295 - acc: 0.8123 - val_loss: 0.4613 - val_acc: 0.7939\n",
      "Epoch 550/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4294 - acc: 0.8112 - val_loss: 0.4615 - val_acc: 0.7919\n",
      "Epoch 551/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4294 - acc: 0.8119 - val_loss: 0.4615 - val_acc: 0.7919\n",
      "Epoch 552/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8114 - val_loss: 0.4614 - val_acc: 0.7925\n",
      "Epoch 553/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8116 - val_loss: 0.4614 - val_acc: 0.7921\n",
      "Epoch 554/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4296 - acc: 0.8112 - val_loss: 0.4628 - val_acc: 0.7932\n",
      "Epoch 555/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4294 - acc: 0.8107 - val_loss: 0.4611 - val_acc: 0.7936\n",
      "Epoch 556/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4294 - acc: 0.8120 - val_loss: 0.4632 - val_acc: 0.7934\n",
      "Epoch 557/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4294 - acc: 0.8113 - val_loss: 0.4614 - val_acc: 0.7932\n",
      "Epoch 558/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4295 - acc: 0.8118 - val_loss: 0.4613 - val_acc: 0.7934\n",
      "Epoch 559/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8130 - val_loss: 0.4615 - val_acc: 0.7930\n",
      "Epoch 560/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4293 - acc: 0.8120 - val_loss: 0.4624 - val_acc: 0.7901\n",
      "Epoch 561/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4296 - acc: 0.8114 - val_loss: 0.4617 - val_acc: 0.7930\n",
      "Epoch 562/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4294 - acc: 0.8121 - val_loss: 0.4613 - val_acc: 0.7934\n",
      "Epoch 563/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4294 - acc: 0.8115 - val_loss: 0.4611 - val_acc: 0.7932\n",
      "Epoch 564/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4294 - acc: 0.8114 - val_loss: 0.4611 - val_acc: 0.7943\n",
      "Epoch 565/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4295 - acc: 0.8114 - val_loss: 0.4612 - val_acc: 0.7939\n",
      "Epoch 566/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4295 - acc: 0.8122 - val_loss: 0.4614 - val_acc: 0.7932\n",
      "Epoch 567/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8110 - val_loss: 0.4615 - val_acc: 0.7932\n",
      "Epoch 568/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4294 - acc: 0.8117 - val_loss: 0.4615 - val_acc: 0.7932\n",
      "Epoch 569/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4292 - acc: 0.8118 - val_loss: 0.4615 - val_acc: 0.7923\n",
      "Epoch 570/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4293 - acc: 0.8115 - val_loss: 0.4615 - val_acc: 0.7934\n",
      "Epoch 571/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4294 - acc: 0.8118 - val_loss: 0.4611 - val_acc: 0.7930\n",
      "Epoch 572/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4294 - acc: 0.8113 - val_loss: 0.4615 - val_acc: 0.7932\n",
      "Epoch 573/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4293 - acc: 0.8123 - val_loss: 0.4610 - val_acc: 0.7936\n",
      "Epoch 574/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8112 - val_loss: 0.4612 - val_acc: 0.7939\n",
      "Epoch 575/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4293 - acc: 0.8117 - val_loss: 0.4610 - val_acc: 0.7934\n",
      "Epoch 576/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8115 - val_loss: 0.4613 - val_acc: 0.7925\n",
      "Epoch 577/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4292 - acc: 0.8113 - val_loss: 0.4620 - val_acc: 0.7921\n",
      "Epoch 578/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4293 - acc: 0.8120 - val_loss: 0.4611 - val_acc: 0.7934\n",
      "Epoch 579/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4293 - acc: 0.8115 - val_loss: 0.4610 - val_acc: 0.7941\n",
      "Epoch 580/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4294 - acc: 0.8121 - val_loss: 0.4610 - val_acc: 0.7932\n",
      "Epoch 581/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8111 - val_loss: 0.4611 - val_acc: 0.7939\n",
      "Epoch 582/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8120 - val_loss: 0.4611 - val_acc: 0.7945\n",
      "Epoch 583/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8120 - val_loss: 0.4610 - val_acc: 0.7943\n",
      "Epoch 584/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4292 - acc: 0.8125 - val_loss: 0.4615 - val_acc: 0.7939\n",
      "Epoch 585/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4293 - acc: 0.8121 - val_loss: 0.4611 - val_acc: 0.7939\n",
      "Epoch 586/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4292 - acc: 0.8110 - val_loss: 0.4610 - val_acc: 0.7945\n",
      "Epoch 587/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4293 - acc: 0.8111 - val_loss: 0.4609 - val_acc: 0.7939\n",
      "Epoch 588/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4292 - acc: 0.8115 - val_loss: 0.4620 - val_acc: 0.7925\n",
      "Epoch 589/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8120 - val_loss: 0.4611 - val_acc: 0.7943\n",
      "Epoch 590/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8119 - val_loss: 0.4609 - val_acc: 0.7934\n",
      "Epoch 591/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8109 - val_loss: 0.4610 - val_acc: 0.7934\n",
      "Epoch 592/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8120 - val_loss: 0.4610 - val_acc: 0.7934\n",
      "Epoch 593/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8120 - val_loss: 0.4611 - val_acc: 0.7925\n",
      "Epoch 594/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8118 - val_loss: 0.4612 - val_acc: 0.7939\n",
      "Epoch 595/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4292 - acc: 0.8119 - val_loss: 0.4609 - val_acc: 0.7948\n",
      "Epoch 596/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.4611 - val_acc: 0.7943\n",
      "Epoch 597/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.4609 - val_acc: 0.7939\n",
      "Epoch 598/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4293 - acc: 0.8117 - val_loss: 0.4615 - val_acc: 0.7930\n",
      "Epoch 599/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4608 - val_acc: 0.7932\n",
      "Epoch 600/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4290 - acc: 0.8114 - val_loss: 0.4628 - val_acc: 0.7934\n",
      "Epoch 601/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4291 - acc: 0.8117 - val_loss: 0.4612 - val_acc: 0.7934\n",
      "Epoch 602/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4291 - acc: 0.8123 - val_loss: 0.4610 - val_acc: 0.7943\n",
      "Epoch 603/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8112 - val_loss: 0.4617 - val_acc: 0.7923\n",
      "Epoch 604/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.8120 - val_loss: 0.4613 - val_acc: 0.7928\n",
      "Epoch 605/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4291 - acc: 0.8122 - val_loss: 0.4615 - val_acc: 0.7928\n",
      "Epoch 606/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4293 - acc: 0.8122 - val_loss: 0.4609 - val_acc: 0.7948\n",
      "Epoch 607/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4292 - acc: 0.8125 - val_loss: 0.4610 - val_acc: 0.7932\n",
      "Epoch 608/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4290 - acc: 0.8122 - val_loss: 0.4610 - val_acc: 0.7952\n",
      "Epoch 609/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.8123 - val_loss: 0.4611 - val_acc: 0.7925\n",
      "Epoch 610/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8121 - val_loss: 0.4612 - val_acc: 0.7941\n",
      "Epoch 611/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4291 - acc: 0.8120 - val_loss: 0.4611 - val_acc: 0.7914\n",
      "Epoch 612/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4291 - acc: 0.8124 - val_loss: 0.4609 - val_acc: 0.7945\n",
      "Epoch 613/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4291 - acc: 0.8113 - val_loss: 0.4608 - val_acc: 0.7939\n",
      "Epoch 614/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 615/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8122 - val_loss: 0.4614 - val_acc: 0.7934\n",
      "Epoch 616/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4615 - val_acc: 0.7934\n",
      "Epoch 617/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4290 - acc: 0.8121 - val_loss: 0.4609 - val_acc: 0.7948\n",
      "Epoch 618/1000\n",
      "13764/13764 [==============================] - 0s 19us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4611 - val_acc: 0.7939\n",
      "Epoch 619/1000\n",
      "13764/13764 [==============================] - 0s 19us/step - loss: 0.4291 - acc: 0.8124 - val_loss: 0.4610 - val_acc: 0.7936\n",
      "Epoch 620/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4291 - acc: 0.8118 - val_loss: 0.4613 - val_acc: 0.7921\n",
      "Epoch 621/1000\n",
      "13764/13764 [==============================] - 0s 19us/step - loss: 0.4290 - acc: 0.8120 - val_loss: 0.4607 - val_acc: 0.7941\n",
      "Epoch 622/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4292 - acc: 0.8112 - val_loss: 0.4611 - val_acc: 0.7939\n",
      "Epoch 623/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8127 - val_loss: 0.4608 - val_acc: 0.7932\n",
      "Epoch 624/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8116 - val_loss: 0.4608 - val_acc: 0.7950\n",
      "Epoch 625/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4618 - val_acc: 0.7921\n",
      "Epoch 626/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8124 - val_loss: 0.4611 - val_acc: 0.7934\n",
      "Epoch 627/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4291 - acc: 0.8109 - val_loss: 0.4613 - val_acc: 0.7934\n",
      "Epoch 628/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8113 - val_loss: 0.4611 - val_acc: 0.7941\n",
      "Epoch 629/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.8115 - val_loss: 0.4608 - val_acc: 0.7930\n",
      "Epoch 630/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8130 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 631/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4290 - acc: 0.8121 - val_loss: 0.4609 - val_acc: 0.7945\n",
      "Epoch 632/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4291 - acc: 0.8117 - val_loss: 0.4610 - val_acc: 0.7941\n",
      "Epoch 633/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4290 - acc: 0.8112 - val_loss: 0.4608 - val_acc: 0.7939\n",
      "Epoch 634/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8132 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 635/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8115 - val_loss: 0.4609 - val_acc: 0.7930\n",
      "Epoch 636/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4611 - val_acc: 0.7934\n",
      "Epoch 637/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8121 - val_loss: 0.4610 - val_acc: 0.7936\n",
      "Epoch 638/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8114 - val_loss: 0.4607 - val_acc: 0.7948\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4287 - acc: 0.8108 - val_loss: 0.4608 - val_acc: 0.7948\n",
      "Epoch 640/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4290 - acc: 0.8120 - val_loss: 0.4607 - val_acc: 0.7948\n",
      "Epoch 641/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4289 - acc: 0.8113 - val_loss: 0.4608 - val_acc: 0.7943\n",
      "Epoch 642/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4290 - acc: 0.8115 - val_loss: 0.4611 - val_acc: 0.7939\n",
      "Epoch 643/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4288 - acc: 0.8122 - val_loss: 0.4612 - val_acc: 0.7925\n",
      "Epoch 644/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8122 - val_loss: 0.4611 - val_acc: 0.7939\n",
      "Epoch 645/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4288 - acc: 0.8123 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 646/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4288 - acc: 0.8112 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 647/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4607 - val_acc: 0.7943\n",
      "Epoch 648/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8121 - val_loss: 0.4610 - val_acc: 0.7939\n",
      "Epoch 649/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4290 - acc: 0.8122 - val_loss: 0.4610 - val_acc: 0.7936\n",
      "Epoch 650/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4289 - acc: 0.8126 - val_loss: 0.4607 - val_acc: 0.7948\n",
      "Epoch 651/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8115 - val_loss: 0.4607 - val_acc: 0.7932\n",
      "Epoch 652/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8115 - val_loss: 0.4607 - val_acc: 0.7948\n",
      "Epoch 653/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4289 - acc: 0.8120 - val_loss: 0.4609 - val_acc: 0.7945\n",
      "Epoch 654/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4288 - acc: 0.8118 - val_loss: 0.4609 - val_acc: 0.7948\n",
      "Epoch 655/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8109 - val_loss: 0.4609 - val_acc: 0.7941\n",
      "Epoch 656/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8130 - val_loss: 0.4607 - val_acc: 0.7941\n",
      "Epoch 657/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4289 - acc: 0.8118 - val_loss: 0.4613 - val_acc: 0.7939\n",
      "Epoch 658/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.8116 - val_loss: 0.4619 - val_acc: 0.7934\n",
      "Epoch 659/1000\n",
      "13764/13764 [==============================] - 1s 37us/step - loss: 0.4287 - acc: 0.8120 - val_loss: 0.4618 - val_acc: 0.7932\n",
      "Epoch 660/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4287 - acc: 0.8114 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 661/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4286 - acc: 0.8117 - val_loss: 0.4609 - val_acc: 0.7925\n",
      "Epoch 662/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4289 - acc: 0.8114 - val_loss: 0.4606 - val_acc: 0.7950\n",
      "Epoch 663/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4289 - acc: 0.8121 - val_loss: 0.4607 - val_acc: 0.7943\n",
      "Epoch 664/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8116 - val_loss: 0.4606 - val_acc: 0.7943\n",
      "Epoch 665/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4289 - acc: 0.8121 - val_loss: 0.4606 - val_acc: 0.7941\n",
      "Epoch 666/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4287 - acc: 0.8119 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 667/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8123 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 668/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4285 - acc: 0.8128 - val_loss: 0.4617 - val_acc: 0.7932\n",
      "Epoch 669/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4287 - acc: 0.8117 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 670/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8111 - val_loss: 0.4608 - val_acc: 0.7934\n",
      "Epoch 671/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4288 - acc: 0.8123 - val_loss: 0.4611 - val_acc: 0.7928\n",
      "Epoch 672/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4288 - acc: 0.8118 - val_loss: 0.4606 - val_acc: 0.7943\n",
      "Epoch 673/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4288 - acc: 0.8118 - val_loss: 0.4606 - val_acc: 0.7943\n",
      "Epoch 674/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4287 - acc: 0.8116 - val_loss: 0.4616 - val_acc: 0.7921\n",
      "Epoch 675/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4287 - acc: 0.8128 - val_loss: 0.4605 - val_acc: 0.7948\n",
      "Epoch 676/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4287 - acc: 0.8125 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 677/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4288 - acc: 0.8125 - val_loss: 0.4606 - val_acc: 0.7941\n",
      "Epoch 678/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4288 - acc: 0.8127 - val_loss: 0.4609 - val_acc: 0.7939\n",
      "Epoch 679/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4286 - acc: 0.8120 - val_loss: 0.4607 - val_acc: 0.7919\n",
      "Epoch 680/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4287 - acc: 0.8123 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 681/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4287 - acc: 0.8125 - val_loss: 0.4607 - val_acc: 0.7928\n",
      "Epoch 682/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4287 - acc: 0.8124 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 683/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4286 - acc: 0.8120 - val_loss: 0.4606 - val_acc: 0.7943\n",
      "Epoch 684/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4286 - acc: 0.8117 - val_loss: 0.4613 - val_acc: 0.7921\n",
      "Epoch 685/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4287 - acc: 0.8115 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 686/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4288 - acc: 0.8121 - val_loss: 0.4607 - val_acc: 0.7943\n",
      "Epoch 687/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4287 - acc: 0.8121 - val_loss: 0.4605 - val_acc: 0.7939\n",
      "Epoch 688/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4287 - acc: 0.8120 - val_loss: 0.4611 - val_acc: 0.7932\n",
      "Epoch 689/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4286 - acc: 0.8121 - val_loss: 0.4615 - val_acc: 0.7934\n",
      "Epoch 690/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4286 - acc: 0.8125 - val_loss: 0.4613 - val_acc: 0.7939\n",
      "Epoch 691/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4286 - acc: 0.8133 - val_loss: 0.4612 - val_acc: 0.7932\n",
      "Epoch 692/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4286 - acc: 0.8119 - val_loss: 0.4605 - val_acc: 0.7948\n",
      "Epoch 693/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4288 - acc: 0.8115 - val_loss: 0.4605 - val_acc: 0.7943\n",
      "Epoch 694/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4287 - acc: 0.8121 - val_loss: 0.4609 - val_acc: 0.7934\n",
      "Epoch 695/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4285 - acc: 0.8121 - val_loss: 0.4606 - val_acc: 0.7934\n",
      "Epoch 696/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4287 - acc: 0.8121 - val_loss: 0.4606 - val_acc: 0.7928\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4287 - acc: 0.8112 - val_loss: 0.4605 - val_acc: 0.7945\n",
      "Epoch 698/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4286 - acc: 0.8123 - val_loss: 0.4606 - val_acc: 0.7943\n",
      "Epoch 699/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4288 - acc: 0.8112 - val_loss: 0.4605 - val_acc: 0.7945\n",
      "Epoch 700/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4285 - acc: 0.8120 - val_loss: 0.4605 - val_acc: 0.7948\n",
      "Epoch 701/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4286 - acc: 0.8120 - val_loss: 0.4605 - val_acc: 0.7950\n",
      "Epoch 702/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4286 - acc: 0.8126 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 703/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4287 - acc: 0.8123 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 704/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4285 - acc: 0.8126 - val_loss: 0.4622 - val_acc: 0.7936\n",
      "Epoch 705/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4286 - acc: 0.8128 - val_loss: 0.4606 - val_acc: 0.7941\n",
      "Epoch 706/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4287 - acc: 0.8117 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 707/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4285 - acc: 0.8122 - val_loss: 0.4610 - val_acc: 0.7930\n",
      "Epoch 708/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4287 - acc: 0.8126 - val_loss: 0.4609 - val_acc: 0.7934\n",
      "Epoch 709/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4285 - acc: 0.8121 - val_loss: 0.4624 - val_acc: 0.7945\n",
      "Epoch 710/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4286 - acc: 0.8122 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 711/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4285 - acc: 0.8116 - val_loss: 0.4606 - val_acc: 0.7948\n",
      "Epoch 712/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4285 - acc: 0.8123 - val_loss: 0.4611 - val_acc: 0.7936\n",
      "Epoch 713/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4287 - acc: 0.8120 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 714/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4286 - acc: 0.8114 - val_loss: 0.4608 - val_acc: 0.7932\n",
      "Epoch 715/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4286 - acc: 0.8123 - val_loss: 0.4612 - val_acc: 0.7930\n",
      "Epoch 716/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4285 - acc: 0.8123 - val_loss: 0.4610 - val_acc: 0.7923\n",
      "Epoch 717/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4286 - acc: 0.8122 - val_loss: 0.4609 - val_acc: 0.7932\n",
      "Epoch 718/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4284 - acc: 0.8120 - val_loss: 0.4611 - val_acc: 0.7925\n",
      "Epoch 719/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4287 - acc: 0.8112 - val_loss: 0.4606 - val_acc: 0.7941\n",
      "Epoch 720/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4283 - acc: 0.8120 - val_loss: 0.4607 - val_acc: 0.7919\n",
      "Epoch 721/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4285 - acc: 0.8132 - val_loss: 0.4604 - val_acc: 0.7948\n",
      "Epoch 722/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4284 - acc: 0.8122 - val_loss: 0.4614 - val_acc: 0.7919\n",
      "Epoch 723/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4286 - acc: 0.8115 - val_loss: 0.4605 - val_acc: 0.7948\n",
      "Epoch 724/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4285 - acc: 0.8116 - val_loss: 0.4605 - val_acc: 0.7925\n",
      "Epoch 725/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4285 - acc: 0.8127 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 726/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4285 - acc: 0.8117 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 727/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4286 - acc: 0.8126 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 728/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4285 - acc: 0.8118 - val_loss: 0.4613 - val_acc: 0.7928\n",
      "Epoch 729/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4285 - acc: 0.8116 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 730/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4283 - acc: 0.8123 - val_loss: 0.4606 - val_acc: 0.7948\n",
      "Epoch 731/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4285 - acc: 0.8117 - val_loss: 0.4604 - val_acc: 0.7948\n",
      "Epoch 732/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4284 - acc: 0.8131 - val_loss: 0.4604 - val_acc: 0.7939\n",
      "Epoch 733/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4285 - acc: 0.8128 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 734/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4283 - acc: 0.8123 - val_loss: 0.4607 - val_acc: 0.7936\n",
      "Epoch 735/1000\n",
      "13764/13764 [==============================] - 0s 34us/step - loss: 0.4285 - acc: 0.8130 - val_loss: 0.4604 - val_acc: 0.7956\n",
      "Epoch 736/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4283 - acc: 0.8123 - val_loss: 0.4605 - val_acc: 0.7936\n",
      "Epoch 737/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4284 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7950\n",
      "Epoch 738/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4285 - acc: 0.8126 - val_loss: 0.4608 - val_acc: 0.7936\n",
      "Epoch 739/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4285 - acc: 0.8121 - val_loss: 0.4605 - val_acc: 0.7948\n",
      "Epoch 740/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4283 - acc: 0.8128 - val_loss: 0.4604 - val_acc: 0.7939\n",
      "Epoch 741/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4284 - acc: 0.8120 - val_loss: 0.4615 - val_acc: 0.7923\n",
      "Epoch 742/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4285 - acc: 0.8112 - val_loss: 0.4605 - val_acc: 0.7954\n",
      "Epoch 743/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4283 - acc: 0.8131 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 744/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4284 - acc: 0.8111 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 745/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4284 - acc: 0.8123 - val_loss: 0.4609 - val_acc: 0.7921\n",
      "Epoch 746/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4285 - acc: 0.8120 - val_loss: 0.4605 - val_acc: 0.7945\n",
      "Epoch 747/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4286 - acc: 0.8120 - val_loss: 0.4603 - val_acc: 0.7945\n",
      "Epoch 748/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4283 - acc: 0.8116 - val_loss: 0.4606 - val_acc: 0.7939\n",
      "Epoch 749/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4285 - acc: 0.8120 - val_loss: 0.4603 - val_acc: 0.7952\n",
      "Epoch 750/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4284 - acc: 0.8123 - val_loss: 0.4607 - val_acc: 0.7939\n",
      "Epoch 751/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4284 - acc: 0.8131 - val_loss: 0.4604 - val_acc: 0.7945\n",
      "Epoch 752/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4285 - acc: 0.8131 - val_loss: 0.4605 - val_acc: 0.7945\n",
      "Epoch 753/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4283 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 754/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4282 - acc: 0.8115 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4283 - acc: 0.8112 - val_loss: 0.4609 - val_acc: 0.7934\n",
      "Epoch 756/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4284 - acc: 0.8131 - val_loss: 0.4603 - val_acc: 0.7952\n",
      "Epoch 757/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4283 - acc: 0.8119 - val_loss: 0.4616 - val_acc: 0.7945\n",
      "Epoch 758/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4285 - acc: 0.8125 - val_loss: 0.4609 - val_acc: 0.7941\n",
      "Epoch 759/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4283 - acc: 0.8121 - val_loss: 0.4613 - val_acc: 0.7939\n",
      "Epoch 760/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4283 - acc: 0.8116 - val_loss: 0.4603 - val_acc: 0.7956\n",
      "Epoch 761/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4284 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7952\n",
      "Epoch 762/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4284 - acc: 0.8125 - val_loss: 0.4605 - val_acc: 0.7945\n",
      "Epoch 763/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4284 - acc: 0.8118 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 764/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4284 - acc: 0.8118 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 765/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4284 - acc: 0.8124 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 766/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4284 - acc: 0.8122 - val_loss: 0.4605 - val_acc: 0.7948\n",
      "Epoch 767/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4284 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7921\n",
      "Epoch 768/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4285 - acc: 0.8124 - val_loss: 0.4602 - val_acc: 0.7941\n",
      "Epoch 769/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4282 - acc: 0.8120 - val_loss: 0.4604 - val_acc: 0.7952\n",
      "Epoch 770/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4282 - acc: 0.8118 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 771/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4284 - acc: 0.8118 - val_loss: 0.4604 - val_acc: 0.7952\n",
      "Epoch 772/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4284 - acc: 0.8124 - val_loss: 0.4606 - val_acc: 0.7934\n",
      "Epoch 773/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4284 - acc: 0.8108 - val_loss: 0.4602 - val_acc: 0.7952\n",
      "Epoch 774/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4282 - acc: 0.8118 - val_loss: 0.4602 - val_acc: 0.7941\n",
      "Epoch 775/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4283 - acc: 0.8118 - val_loss: 0.4603 - val_acc: 0.7936\n",
      "Epoch 776/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4283 - acc: 0.8122 - val_loss: 0.4612 - val_acc: 0.7936\n",
      "Epoch 777/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4284 - acc: 0.8116 - val_loss: 0.4604 - val_acc: 0.7914\n",
      "Epoch 778/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8119 - val_loss: 0.4605 - val_acc: 0.7941\n",
      "Epoch 779/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4283 - acc: 0.8115 - val_loss: 0.4604 - val_acc: 0.7930\n",
      "Epoch 780/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4283 - acc: 0.8115 - val_loss: 0.4602 - val_acc: 0.7939\n",
      "Epoch 781/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4283 - acc: 0.8121 - val_loss: 0.4602 - val_acc: 0.7954\n",
      "Epoch 782/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4284 - acc: 0.8120 - val_loss: 0.4603 - val_acc: 0.7954\n",
      "Epoch 783/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4283 - acc: 0.8116 - val_loss: 0.4604 - val_acc: 0.7945\n",
      "Epoch 784/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4282 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7952\n",
      "Epoch 785/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4282 - acc: 0.8123 - val_loss: 0.4608 - val_acc: 0.7945\n",
      "Epoch 786/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4284 - acc: 0.8127 - val_loss: 0.4601 - val_acc: 0.7945\n",
      "Epoch 787/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4282 - acc: 0.8120 - val_loss: 0.4602 - val_acc: 0.7948\n",
      "Epoch 788/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4283 - acc: 0.8120 - val_loss: 0.4604 - val_acc: 0.7948\n",
      "Epoch 789/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.4603 - val_acc: 0.7954\n",
      "Epoch 790/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4283 - acc: 0.8125 - val_loss: 0.4608 - val_acc: 0.7943\n",
      "Epoch 791/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4282 - acc: 0.8115 - val_loss: 0.4604 - val_acc: 0.7950\n",
      "Epoch 792/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4283 - acc: 0.8121 - val_loss: 0.4606 - val_acc: 0.7945\n",
      "Epoch 793/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4283 - acc: 0.8111 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 794/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4279 - acc: 0.8118 - val_loss: 0.4613 - val_acc: 0.7936\n",
      "Epoch 795/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4282 - acc: 0.8122 - val_loss: 0.4602 - val_acc: 0.7952\n",
      "Epoch 796/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4281 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7939\n",
      "Epoch 797/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4281 - acc: 0.8127 - val_loss: 0.4603 - val_acc: 0.7919\n",
      "Epoch 798/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4282 - acc: 0.8122 - val_loss: 0.4604 - val_acc: 0.7916\n",
      "Epoch 799/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4283 - acc: 0.8110 - val_loss: 0.4602 - val_acc: 0.7959\n",
      "Epoch 800/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8125 - val_loss: 0.4607 - val_acc: 0.7945\n",
      "Epoch 801/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4282 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7941\n",
      "Epoch 802/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.4602 - val_acc: 0.7945\n",
      "Epoch 803/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4283 - acc: 0.8129 - val_loss: 0.4601 - val_acc: 0.7950\n",
      "Epoch 804/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4284 - acc: 0.8123 - val_loss: 0.4601 - val_acc: 0.7954\n",
      "Epoch 805/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4280 - acc: 0.8120 - val_loss: 0.4610 - val_acc: 0.7928\n",
      "Epoch 806/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4282 - acc: 0.8125 - val_loss: 0.4603 - val_acc: 0.7945\n",
      "Epoch 807/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4281 - acc: 0.8125 - val_loss: 0.4601 - val_acc: 0.7934\n",
      "Epoch 808/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4283 - acc: 0.8132 - val_loss: 0.4602 - val_acc: 0.7952\n",
      "Epoch 809/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4283 - acc: 0.8118 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 810/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.4609 - val_acc: 0.7950\n",
      "Epoch 811/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4282 - acc: 0.8120 - val_loss: 0.4610 - val_acc: 0.7948\n",
      "Epoch 812/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4280 - acc: 0.8118 - val_loss: 0.4601 - val_acc: 0.7936\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4283 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7939\n",
      "Epoch 814/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4282 - acc: 0.8125 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 815/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4281 - acc: 0.8128 - val_loss: 0.4604 - val_acc: 0.7939\n",
      "Epoch 816/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4283 - acc: 0.8115 - val_loss: 0.4601 - val_acc: 0.7959\n",
      "Epoch 817/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.4602 - val_acc: 0.7943\n",
      "Epoch 818/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4281 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7943\n",
      "Epoch 819/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4282 - acc: 0.8120 - val_loss: 0.4602 - val_acc: 0.7932\n",
      "Epoch 820/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4280 - acc: 0.8111 - val_loss: 0.4618 - val_acc: 0.7948\n",
      "Epoch 821/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4283 - acc: 0.8127 - val_loss: 0.4601 - val_acc: 0.7954\n",
      "Epoch 822/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4282 - acc: 0.8126 - val_loss: 0.4605 - val_acc: 0.7939\n",
      "Epoch 823/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8120 - val_loss: 0.4601 - val_acc: 0.7954\n",
      "Epoch 824/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4280 - acc: 0.8126 - val_loss: 0.4617 - val_acc: 0.7948\n",
      "Epoch 825/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8118 - val_loss: 0.4600 - val_acc: 0.7950\n",
      "Epoch 826/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4283 - acc: 0.8119 - val_loss: 0.4601 - val_acc: 0.7961\n",
      "Epoch 827/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4281 - acc: 0.8124 - val_loss: 0.4600 - val_acc: 0.7948\n",
      "Epoch 828/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4280 - acc: 0.8115 - val_loss: 0.4602 - val_acc: 0.7956\n",
      "Epoch 829/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4281 - acc: 0.8119 - val_loss: 0.4601 - val_acc: 0.7948\n",
      "Epoch 830/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.4601 - val_acc: 0.7952\n",
      "Epoch 831/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4282 - acc: 0.8121 - val_loss: 0.4603 - val_acc: 0.7950\n",
      "Epoch 832/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4282 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7948\n",
      "Epoch 833/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4280 - acc: 0.8120 - val_loss: 0.4601 - val_acc: 0.7950\n",
      "Epoch 834/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4281 - acc: 0.8127 - val_loss: 0.4603 - val_acc: 0.7941\n",
      "Epoch 835/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4281 - acc: 0.8129 - val_loss: 0.4602 - val_acc: 0.7941\n",
      "Epoch 836/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4283 - acc: 0.8126 - val_loss: 0.4603 - val_acc: 0.7943\n",
      "Epoch 837/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4281 - acc: 0.8128 - val_loss: 0.4600 - val_acc: 0.7952\n",
      "Epoch 838/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7956\n",
      "Epoch 839/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4280 - acc: 0.8126 - val_loss: 0.4602 - val_acc: 0.7945\n",
      "Epoch 840/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8114 - val_loss: 0.4599 - val_acc: 0.7952\n",
      "Epoch 841/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4281 - acc: 0.8120 - val_loss: 0.4601 - val_acc: 0.7948\n",
      "Epoch 842/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4281 - acc: 0.8118 - val_loss: 0.4606 - val_acc: 0.7952\n",
      "Epoch 843/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4280 - acc: 0.8115 - val_loss: 0.4605 - val_acc: 0.7952\n",
      "Epoch 844/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4281 - acc: 0.8121 - val_loss: 0.4600 - val_acc: 0.7959\n",
      "Epoch 845/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4281 - acc: 0.8122 - val_loss: 0.4600 - val_acc: 0.7943\n",
      "Epoch 846/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4279 - acc: 0.8117 - val_loss: 0.4602 - val_acc: 0.7943\n",
      "Epoch 847/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8121 - val_loss: 0.4603 - val_acc: 0.7952\n",
      "Epoch 848/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4281 - acc: 0.8115 - val_loss: 0.4601 - val_acc: 0.7948\n",
      "Epoch 849/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4281 - acc: 0.8122 - val_loss: 0.4611 - val_acc: 0.7941\n",
      "Epoch 850/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4281 - acc: 0.8110 - val_loss: 0.4603 - val_acc: 0.7950\n",
      "Epoch 851/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4280 - acc: 0.8114 - val_loss: 0.4603 - val_acc: 0.7943\n",
      "Epoch 852/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4280 - acc: 0.8121 - val_loss: 0.4599 - val_acc: 0.7943\n",
      "Epoch 853/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4282 - acc: 0.8119 - val_loss: 0.4599 - val_acc: 0.7954\n",
      "Epoch 854/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4278 - acc: 0.8116 - val_loss: 0.4603 - val_acc: 0.7945\n",
      "Epoch 855/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4279 - acc: 0.8121 - val_loss: 0.4600 - val_acc: 0.7959\n",
      "Epoch 856/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4280 - acc: 0.8116 - val_loss: 0.4606 - val_acc: 0.7948\n",
      "Epoch 857/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8116 - val_loss: 0.4601 - val_acc: 0.7950\n",
      "Epoch 858/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4283 - acc: 0.8119 - val_loss: 0.4603 - val_acc: 0.7952\n",
      "Epoch 859/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4280 - acc: 0.8117 - val_loss: 0.4600 - val_acc: 0.7945\n",
      "Epoch 860/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4280 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7961\n",
      "Epoch 861/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4282 - acc: 0.8110 - val_loss: 0.4600 - val_acc: 0.7930\n",
      "Epoch 862/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4280 - acc: 0.8112 - val_loss: 0.4600 - val_acc: 0.7934\n",
      "Epoch 863/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4279 - acc: 0.8120 - val_loss: 0.4602 - val_acc: 0.7950\n",
      "Epoch 864/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4280 - acc: 0.8123 - val_loss: 0.4608 - val_acc: 0.7939\n",
      "Epoch 865/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4282 - acc: 0.8120 - val_loss: 0.4599 - val_acc: 0.7956\n",
      "Epoch 866/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4280 - acc: 0.8125 - val_loss: 0.4599 - val_acc: 0.7941\n",
      "Epoch 867/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8121 - val_loss: 0.4617 - val_acc: 0.7945\n",
      "Epoch 868/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4280 - acc: 0.8114 - val_loss: 0.4606 - val_acc: 0.7954\n",
      "Epoch 869/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.8116 - val_loss: 0.4604 - val_acc: 0.7950\n",
      "Epoch 870/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4281 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7956\n",
      "Epoch 871/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4280 - acc: 0.8118 - val_loss: 0.4600 - val_acc: 0.7939\n",
      "Epoch 872/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4279 - acc: 0.8118 - val_loss: 0.4601 - val_acc: 0.7943\n",
      "Epoch 873/1000\n",
      "13764/13764 [==============================] - 0s 33us/step - loss: 0.4279 - acc: 0.8123 - val_loss: 0.4598 - val_acc: 0.7954\n",
      "Epoch 874/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4279 - acc: 0.8118 - val_loss: 0.4601 - val_acc: 0.7954\n",
      "Epoch 875/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4281 - acc: 0.8120 - val_loss: 0.4601 - val_acc: 0.7952\n",
      "Epoch 876/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4279 - acc: 0.8122 - val_loss: 0.4599 - val_acc: 0.7963\n",
      "Epoch 877/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4279 - acc: 0.8121 - val_loss: 0.4599 - val_acc: 0.7952\n",
      "Epoch 878/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4280 - acc: 0.8119 - val_loss: 0.4599 - val_acc: 0.7921\n",
      "Epoch 879/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4279 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7930\n",
      "Epoch 880/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4280 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7916\n",
      "Epoch 881/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4280 - acc: 0.8118 - val_loss: 0.4598 - val_acc: 0.7950\n",
      "Epoch 882/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4279 - acc: 0.8129 - val_loss: 0.4598 - val_acc: 0.7954\n",
      "Epoch 883/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4279 - acc: 0.8127 - val_loss: 0.4600 - val_acc: 0.7923\n",
      "Epoch 884/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4280 - acc: 0.8120 - val_loss: 0.4599 - val_acc: 0.7948\n",
      "Epoch 885/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4279 - acc: 0.8127 - val_loss: 0.4607 - val_acc: 0.7945\n",
      "Epoch 886/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4279 - acc: 0.8133 - val_loss: 0.4599 - val_acc: 0.7954\n",
      "Epoch 887/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4278 - acc: 0.8123 - val_loss: 0.4598 - val_acc: 0.7959\n",
      "Epoch 888/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8121 - val_loss: 0.4599 - val_acc: 0.7963\n",
      "Epoch 889/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4280 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7948\n",
      "Epoch 890/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4279 - acc: 0.8125 - val_loss: 0.4599 - val_acc: 0.7943\n",
      "Epoch 891/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4278 - acc: 0.8122 - val_loss: 0.4599 - val_acc: 0.7945\n",
      "Epoch 892/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8120 - val_loss: 0.4600 - val_acc: 0.7954\n",
      "Epoch 893/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8120 - val_loss: 0.4605 - val_acc: 0.7954\n",
      "Epoch 894/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4278 - acc: 0.8131 - val_loss: 0.4606 - val_acc: 0.7950\n",
      "Epoch 895/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4279 - acc: 0.8119 - val_loss: 0.4601 - val_acc: 0.7950\n",
      "Epoch 896/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8123 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 897/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4278 - acc: 0.8118 - val_loss: 0.4608 - val_acc: 0.7948\n",
      "Epoch 898/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4279 - acc: 0.8118 - val_loss: 0.4608 - val_acc: 0.7941\n",
      "Epoch 899/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4280 - acc: 0.8121 - val_loss: 0.4600 - val_acc: 0.7963\n",
      "Epoch 900/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4280 - acc: 0.8113 - val_loss: 0.4601 - val_acc: 0.7954\n",
      "Epoch 901/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4280 - acc: 0.8124 - val_loss: 0.4600 - val_acc: 0.7952\n",
      "Epoch 902/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4280 - acc: 0.8114 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 903/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4278 - acc: 0.8114 - val_loss: 0.4598 - val_acc: 0.7965\n",
      "Epoch 904/1000\n",
      "13764/13764 [==============================] - 1s 40us/step - loss: 0.4278 - acc: 0.8117 - val_loss: 0.4599 - val_acc: 0.7950\n",
      "Epoch 905/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4279 - acc: 0.8123 - val_loss: 0.4600 - val_acc: 0.7952\n",
      "Epoch 906/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4279 - acc: 0.8118 - val_loss: 0.4599 - val_acc: 0.7950\n",
      "Epoch 907/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4277 - acc: 0.8118 - val_loss: 0.4607 - val_acc: 0.7963\n",
      "Epoch 908/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4278 - acc: 0.8117 - val_loss: 0.4597 - val_acc: 0.7945\n",
      "Epoch 909/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4277 - acc: 0.8121 - val_loss: 0.4597 - val_acc: 0.7939\n",
      "Epoch 910/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4280 - acc: 0.8120 - val_loss: 0.4596 - val_acc: 0.7959\n",
      "Epoch 911/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4278 - acc: 0.8107 - val_loss: 0.4596 - val_acc: 0.7950\n",
      "Epoch 912/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4279 - acc: 0.8115 - val_loss: 0.4598 - val_acc: 0.7954\n",
      "Epoch 913/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4278 - acc: 0.8120 - val_loss: 0.4601 - val_acc: 0.7956\n",
      "Epoch 914/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4278 - acc: 0.8118 - val_loss: 0.4598 - val_acc: 0.7954\n",
      "Epoch 915/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4277 - acc: 0.8121 - val_loss: 0.4596 - val_acc: 0.7952\n",
      "Epoch 916/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4278 - acc: 0.8115 - val_loss: 0.4599 - val_acc: 0.7959\n",
      "Epoch 917/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4279 - acc: 0.8123 - val_loss: 0.4601 - val_acc: 0.7952\n",
      "Epoch 918/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4278 - acc: 0.8125 - val_loss: 0.4597 - val_acc: 0.7954\n",
      "Epoch 919/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4278 - acc: 0.8125 - val_loss: 0.4598 - val_acc: 0.7954\n",
      "Epoch 920/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4278 - acc: 0.8118 - val_loss: 0.4597 - val_acc: 0.7936\n",
      "Epoch 921/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4278 - acc: 0.8120 - val_loss: 0.4599 - val_acc: 0.7954\n",
      "Epoch 922/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4277 - acc: 0.8118 - val_loss: 0.4597 - val_acc: 0.7954\n",
      "Epoch 923/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4278 - acc: 0.8132 - val_loss: 0.4597 - val_acc: 0.7961\n",
      "Epoch 924/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4280 - acc: 0.8113 - val_loss: 0.4597 - val_acc: 0.7945\n",
      "Epoch 925/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4278 - acc: 0.8123 - val_loss: 0.4597 - val_acc: 0.7941\n",
      "Epoch 926/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4278 - acc: 0.8123 - val_loss: 0.4604 - val_acc: 0.7948\n",
      "Epoch 927/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4277 - acc: 0.8123 - val_loss: 0.4603 - val_acc: 0.7936\n",
      "Epoch 928/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4279 - acc: 0.8119 - val_loss: 0.4597 - val_acc: 0.7952\n",
      "Epoch 929/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4276 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7959\n",
      "Epoch 930/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4277 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7948\n",
      "Epoch 931/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4277 - acc: 0.8125 - val_loss: 0.4597 - val_acc: 0.7948\n",
      "Epoch 932/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7954\n",
      "Epoch 933/1000\n",
      "13764/13764 [==============================] - 1s 38us/step - loss: 0.4276 - acc: 0.8113 - val_loss: 0.4613 - val_acc: 0.7954\n",
      "Epoch 934/1000\n",
      "13764/13764 [==============================] - 0s 35us/step - loss: 0.4279 - acc: 0.8121 - val_loss: 0.4598 - val_acc: 0.7956\n",
      "Epoch 935/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4278 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7956\n",
      "Epoch 936/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8120 - val_loss: 0.4598 - val_acc: 0.7952\n",
      "Epoch 937/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4277 - acc: 0.8120 - val_loss: 0.4596 - val_acc: 0.7952\n",
      "Epoch 938/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4278 - acc: 0.8115 - val_loss: 0.4596 - val_acc: 0.7959\n",
      "Epoch 939/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4279 - acc: 0.8116 - val_loss: 0.4597 - val_acc: 0.7948\n",
      "Epoch 940/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4278 - acc: 0.8121 - val_loss: 0.4597 - val_acc: 0.7959\n",
      "Epoch 941/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4277 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7936\n",
      "Epoch 942/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4277 - acc: 0.8126 - val_loss: 0.4601 - val_acc: 0.7948\n",
      "Epoch 943/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4278 - acc: 0.8120 - val_loss: 0.4597 - val_acc: 0.7956\n",
      "Epoch 944/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4277 - acc: 0.8129 - val_loss: 0.4596 - val_acc: 0.7941\n",
      "Epoch 945/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4277 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7959\n",
      "Epoch 946/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4277 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7928\n",
      "Epoch 947/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4277 - acc: 0.8120 - val_loss: 0.4602 - val_acc: 0.7959\n",
      "Epoch 948/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4277 - acc: 0.8128 - val_loss: 0.4597 - val_acc: 0.7956\n",
      "Epoch 949/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4278 - acc: 0.8125 - val_loss: 0.4598 - val_acc: 0.7956\n",
      "Epoch 950/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4277 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7963\n",
      "Epoch 951/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4279 - acc: 0.8117 - val_loss: 0.4598 - val_acc: 0.7954\n",
      "Epoch 952/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4277 - acc: 0.8121 - val_loss: 0.4596 - val_acc: 0.7936\n",
      "Epoch 953/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4276 - acc: 0.8126 - val_loss: 0.4595 - val_acc: 0.7950\n",
      "Epoch 954/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4276 - acc: 0.8129 - val_loss: 0.4599 - val_acc: 0.7963\n",
      "Epoch 955/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4279 - acc: 0.8119 - val_loss: 0.4598 - val_acc: 0.7952\n",
      "Epoch 956/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4276 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7925\n",
      "Epoch 957/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4278 - acc: 0.8116 - val_loss: 0.4603 - val_acc: 0.7948\n",
      "Epoch 958/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4275 - acc: 0.8121 - val_loss: 0.4598 - val_acc: 0.7930\n",
      "Epoch 959/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4277 - acc: 0.8115 - val_loss: 0.4598 - val_acc: 0.7961\n",
      "Epoch 960/1000\n",
      "13764/13764 [==============================] - 0s 30us/step - loss: 0.4278 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7943\n",
      "Epoch 961/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4276 - acc: 0.8123 - val_loss: 0.4599 - val_acc: 0.7952\n",
      "Epoch 962/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4276 - acc: 0.8120 - val_loss: 0.4594 - val_acc: 0.7945\n",
      "Epoch 963/1000\n",
      "13764/13764 [==============================] - 0s 32us/step - loss: 0.4277 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7956\n",
      "Epoch 964/1000\n",
      "13764/13764 [==============================] - 0s 36us/step - loss: 0.4277 - acc: 0.8130 - val_loss: 0.4595 - val_acc: 0.7941\n",
      "Epoch 965/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4275 - acc: 0.8112 - val_loss: 0.4597 - val_acc: 0.7956\n",
      "Epoch 966/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4276 - acc: 0.8110 - val_loss: 0.4601 - val_acc: 0.7954\n",
      "Epoch 967/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4276 - acc: 0.8128 - val_loss: 0.4600 - val_acc: 0.7921\n",
      "Epoch 968/1000\n",
      "13764/13764 [==============================] - 0s 29us/step - loss: 0.4274 - acc: 0.8123 - val_loss: 0.4598 - val_acc: 0.7959\n",
      "Epoch 969/1000\n",
      "13764/13764 [==============================] - 0s 31us/step - loss: 0.4278 - acc: 0.8122 - val_loss: 0.4597 - val_acc: 0.7963\n",
      "Epoch 970/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4277 - acc: 0.8121 - val_loss: 0.4598 - val_acc: 0.7961\n",
      "Epoch 971/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4277 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7954\n",
      "Epoch 972/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4275 - acc: 0.8118 - val_loss: 0.4595 - val_acc: 0.7956\n",
      "Epoch 973/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4277 - acc: 0.8121 - val_loss: 0.4596 - val_acc: 0.7943\n",
      "Epoch 974/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4278 - acc: 0.8119 - val_loss: 0.4597 - val_acc: 0.7950\n",
      "Epoch 975/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4277 - acc: 0.8123 - val_loss: 0.4595 - val_acc: 0.7954\n",
      "Epoch 976/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4275 - acc: 0.8118 - val_loss: 0.4599 - val_acc: 0.7959\n",
      "Epoch 977/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4276 - acc: 0.8128 - val_loss: 0.4598 - val_acc: 0.7928\n",
      "Epoch 978/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8122 - val_loss: 0.4596 - val_acc: 0.7943\n",
      "Epoch 979/1000\n",
      "13764/13764 [==============================] - 1s 39us/step - loss: 0.4276 - acc: 0.8120 - val_loss: 0.4602 - val_acc: 0.7934\n",
      "Epoch 980/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4276 - acc: 0.8126 - val_loss: 0.4597 - val_acc: 0.7954\n",
      "Epoch 981/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4277 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7952\n",
      "Epoch 982/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4275 - acc: 0.8123 - val_loss: 0.4596 - val_acc: 0.7959\n",
      "Epoch 983/1000\n",
      "13764/13764 [==============================] - 0s 23us/step - loss: 0.4275 - acc: 0.8128 - val_loss: 0.4596 - val_acc: 0.7928\n",
      "Epoch 984/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4277 - acc: 0.8114 - val_loss: 0.4601 - val_acc: 0.7952\n",
      "Epoch 985/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4277 - acc: 0.8126 - val_loss: 0.4600 - val_acc: 0.7961\n",
      "Epoch 986/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4274 - acc: 0.8126 - val_loss: 0.4616 - val_acc: 0.7943\n",
      "Epoch 987/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4277 - acc: 0.8128 - val_loss: 0.4595 - val_acc: 0.7939\n",
      "Epoch 988/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4276 - acc: 0.8120 - val_loss: 0.4595 - val_acc: 0.7932\n",
      "Epoch 989/1000\n",
      "13764/13764 [==============================] - 0s 25us/step - loss: 0.4276 - acc: 0.8128 - val_loss: 0.4599 - val_acc: 0.7954\n",
      "Epoch 990/1000\n",
      "13764/13764 [==============================] - 0s 24us/step - loss: 0.4276 - acc: 0.8119 - val_loss: 0.4596 - val_acc: 0.7970\n",
      "Epoch 991/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4276 - acc: 0.8126 - val_loss: 0.4599 - val_acc: 0.7952\n",
      "Epoch 992/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4276 - acc: 0.8118 - val_loss: 0.4598 - val_acc: 0.7956\n",
      "Epoch 993/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4275 - acc: 0.8114 - val_loss: 0.4595 - val_acc: 0.7956\n",
      "Epoch 994/1000\n",
      "13764/13764 [==============================] - 0s 20us/step - loss: 0.4277 - acc: 0.8120 - val_loss: 0.4596 - val_acc: 0.7954\n",
      "Epoch 995/1000\n",
      "13764/13764 [==============================] - 0s 21us/step - loss: 0.4275 - acc: 0.8116 - val_loss: 0.4600 - val_acc: 0.7961\n",
      "Epoch 996/1000\n",
      "13764/13764 [==============================] - 0s 28us/step - loss: 0.4275 - acc: 0.8126 - val_loss: 0.4594 - val_acc: 0.7948\n",
      "Epoch 997/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4275 - acc: 0.8117 - val_loss: 0.4594 - val_acc: 0.7963\n",
      "Epoch 998/1000\n",
      "13764/13764 [==============================] - 0s 22us/step - loss: 0.4275 - acc: 0.8128 - val_loss: 0.4593 - val_acc: 0.7961\n",
      "Epoch 999/1000\n",
      "13764/13764 [==============================] - 0s 26us/step - loss: 0.4276 - acc: 0.8120 - val_loss: 0.4594 - val_acc: 0.7939\n",
      "Epoch 1000/1000\n",
      "13764/13764 [==============================] - 0s 27us/step - loss: 0.4274 - acc: 0.8118 - val_loss: 0.4600 - val_acc: 0.7934\n"
     ]
    }
   ],
   "source": [
    "h = model_TP.fit(train_X_TP[:], train_Y_TP[:], \n",
    "          epochs=1000, \n",
    "          #batch_size=32, \n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          #validation_split=0.1)\n",
    "          validation_data=([test_X_TP, test_Y_TP]))\n",
    "\n",
    "history_TP = {k : history_TP[k] + h.history[k] for k in hist_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f381ee5e4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NC = nodes correct\n",
    "model_TP.save('URZ_model_15-6-2_norm_NC_TP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4502/4502 [==============================] - 0s 17us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45997298121293456, 0.7934251444861922]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TP.evaluate(test_X_TP, test_Y_TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1723  531]\n",
      " [ 399 1849]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = numpy.reshape(numpy.argmax(model_TP.predict(test_X_TP), axis=1), (test_X_TP.shape[0],1))\n",
    "\n",
    "# calculate confusion matrix\n",
    "conf_mat = confusion_matrix(test_Y_TP_, Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cascade of all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NTPS = load_model('URZ_model_15-6-2_norm_NC_NTPS.h5')\n",
    "model_STP = load_model('URZ_model_15-6-2_norm_NC_TPS.h5')\n",
    "model_PT = load_model('URZ_model_15-6-2_norm_NC_TP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iwt(X, stage=0):\n",
    "    \"\"\"\n",
    "    predicts initial wave type for given featrue vectors\n",
    "    Class encoding generated by sklearn Label Encoder\n",
    "    0 - noise\n",
    "    2 - regS \n",
    "    1 - regP\n",
    "    3 - T\n",
    "    \"\"\"\n",
    "    Y = numpy.arange(X.shape[0])\n",
    "    \n",
    "    N_indices = None\n",
    "    S_indices = None \n",
    "    \n",
    "    if stage >= 1:\n",
    "        N_indices = [False] * X.shape[0]\n",
    "        X_PTS = X\n",
    "        Y_NPTS = numpy.zeros(X.shape[0])\n",
    "    if stage >= 2:\n",
    "        S_indices =  [False] * X.shape[0]  # numpy.array([])\n",
    "        X_PT = X\n",
    "        Y_PTS = numpy.zeros(X.shape[0])\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    N vs regS, regP, T\n",
    "    \"\"\"\n",
    "    if N_indices is None:\n",
    "        #predict N vs T,regP,regS\n",
    "        Y_NPTS = numpy.argmax(model_NTPS.predict(X), axis=1)\n",
    "        #set which are noise\n",
    "        N_indices = Y[Y_NPTS == 1]  #  = 0\n",
    "        #get candidates for TPS\n",
    "        X_PTS = X[Y_NPTS < 1]\n",
    "    else:\n",
    "        print('Skipping N, classifying TPS only')\n",
    "\n",
    "    \"\"\"\n",
    "    regS vs regP, T\n",
    "    \"\"\"\n",
    "    if S_indices is None:    \n",
    "        #predict regS vs T,regP\n",
    "        Y_PTS = numpy.argmax(model_STP.predict(X_PTS), axis=1)\n",
    "        #set which are regS\n",
    "        S_indices = Y[Y_NPTS < 1][Y_PTS == 1]  # = 2\n",
    "        #get candidates for regP,T\n",
    "        X_PT = X_PTS[Y_PTS < 1]\n",
    "    else:\n",
    "        print('Skipping N, regS, classifying TP only')\n",
    "\n",
    "    \"\"\"\n",
    "    regP vs T\n",
    "    \"\"\"\n",
    "    #predict regP vs T\n",
    "    Y_PT = numpy.argmax(model_PT.predict(X_PT), axis=1)\n",
    "    #set which are regP\n",
    "    P_indices = Y[Y_NPTS < 1][Y_PTS < 1][Y_PT == 1]  # = 1    \n",
    "    #set which are T\n",
    "    T_indices = Y[Y_NPTS < 1][Y_PTS < 1][Y_PT < 1]   # = 3\n",
    "    #get those which are T\n",
    "    X_T = X_PT[Y_PT < 1]\n",
    "    \n",
    "    Y[N_indices] = 0 # N\n",
    "    Y[S_indices] = 2 # regS\n",
    "    Y[P_indices] = 1 # regP \n",
    "    Y[T_indices] = 3 # tele\n",
    "    \n",
    "    return Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = predict_iwt(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall confusion matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13700, 15), (13700,), (13700,))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, Y.shape, test_Y_GT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5687  238  609  292]\n",
      " [ 346 1687   14  473]\n",
      " [ 653    5 1651  374]\n",
      " [ 164  318   74 1115]]\n"
     ]
    }
   ],
   "source": [
    "C = confusion_matrix(Y, test_Y_GT)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.01%\n"
     ]
    }
   ],
   "source": [
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's produce result data frame from metadata and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13700\n"
     ]
    }
   ],
   "source": [
    "class_dict = {0:'N', 1:'regP', 2:'regS', 3:'tele'}\n",
    "Y_pred_cat = [class_dict[yi] for yi in Y]\n",
    "print(len(Y_pred_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata = test[metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add new column to the pandas dataframe\n",
    "#test_metadata['CLASS_IPHASE_NEW'] = Y_pred_cat  # deprecated - gives warning\n",
    "test_metadata.assign(CLASS_IPHASE_NEW = pd.Series(Y_pred_cat, index=test_metadata.index)).to_csv('test_set_prediction.txt', na_rep='null',columns=metadata+[\"CLASS_IPHASE_NEW\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some experiment section.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_Y = predict_iwt(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41098, 15)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iphase_Y = le.transform(test['CLASS_IPHASE'])\n",
    "test_truth_Y = le.transform(test['CLASS_PHASE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_truth_Y.min(), test_truth_Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iphase_Y.min(), test_iphase_Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.sum( ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5687  238  609  292]\n",
      " [ 346 1687   14  473]\n",
      " [ 653    5 1651  374]\n",
      " [ 164  318   74 1115]]\n",
      "Accuracy: 74.01%\n"
     ]
    }
   ],
   "source": [
    "#print('IPHASE:')\n",
    "C = confusion_matrix(test_pred_Y, test_Y_GT)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data just regS, regP, T - not to be confused with overall statistics on all arrivals in our DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_TPS = test[test['CLASS_PHASE'] != 'N']\n",
    "test_TPS_X = test_TPS[x_indices]\n",
    "test_TPS_pred_Y = predict_iwt(test_TPS_X)\n",
    "test_TPS_truth_Y = le.transform(test_TPS['CLASS_PHASE'])\n",
    "test_TPS_iphase_Y = le.transform(test_TPS['CLASS_IPHASE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_TPS_iphase_Y.min(), test_TPS_iphase_Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPHASE:\n",
      "[[   0  337  829  380]\n",
      " [   0 1655  170  540]\n",
      " [   0   45 1312  351]\n",
      " [   0  211   37  983]]\n",
      "Accuracy: 57.66%\n"
     ]
    }
   ],
   "source": [
    "print('IPHASE:')\n",
    "C = confusion_matrix(test_TPS_iphase_Y, test_TPS_truth_Y)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW PREDICTION\n",
      "[[   0  238  609  292]\n",
      " [   0 1687   14  473]\n",
      " [   0    5 1651  374]\n",
      " [   0  318   74 1115]]\n",
      "Accuracy: 65.01%\n"
     ]
    }
   ],
   "source": [
    "print('NEW PREDICTION')\n",
    "C = confusion_matrix(test_TPS_pred_Y, test_TPS_truth_Y)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in the test data, the current iwt characterized 1501 as NOISE (first row of confusion matrix)\n",
    "* our new iwt would characterize 1089 as NOISE\n",
    "* after re-training, accuracy on the test set rose from 58.68% to 65.82% for regS, regP and T phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ..on train data just to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17144   681  1621   893]\n",
      " [ 1009  5296    24  1317]\n",
      " [ 1886    20  4931  1186]\n",
      " [  510   888   209  3483]]\n",
      "Accuracy: 75.07%\n"
     ]
    }
   ],
   "source": [
    "Y = predict_iwt(train_X)\n",
    "C = confusion_matrix(Y, train_Y_GT)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall confusion matrix for all manual associations (no Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping N, classifying TPS only\n",
      "[[1308   68  394]\n",
      " [ 229 1819  736]\n",
      " [ 465  115  872]]\n"
     ]
    }
   ],
   "source": [
    "C = confusion_matrix(predict_iwt(manual_X, stage=1), manual_Y_GT)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.58%\n"
     ]
    }
   ],
   "source": [
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try all noise samples we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get all noise phases not used for training\n",
    "N_data_diff = pd.concat([df_N_all, N_data]).loc[\n",
    "    df_N_all.index.symmetric_difference(N_data.index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273972, 25) (301371, 25) (27399, 25)\n",
      "273972 should equal 273972\n"
     ]
    }
   ],
   "source": [
    "print(N_data_diff.shape, df_N_all.shape, N_data.shape)\n",
    "print(N_data_diff.shape[0], 'should equal', df_N_all.shape[0]-N_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_data_norm = N_data_diff[x_indices].copy(deep=True)\n",
    "N_data_norm['INANG1'] /= 90.\n",
    "N_data_norm['INANG3'] /= 90.\n",
    "N_data_norm['HMXMN'] = numpy.log10(df_N_all['HMXMN'])\n",
    "N_data_norm['HVRATP'] = numpy.log10(df_N_all['HVRATP'])\n",
    "N_data_norm['HVRAT'] = numpy.log10(df_N_all['HVRAT'])\n",
    "N_data_norm['HTOV1'] = numpy.log10(df_N_all['HTOV1'])\n",
    "N_data_norm['HTOV2'] = numpy.log10(df_N_all['HTOV2'])\n",
    "N_data_norm['HTOV3'] = numpy.log10(df_N_all['HTOV3'])\n",
    "N_data_norm['HTOV4'] = numpy.log10(df_N_all['HTOV4'])\n",
    "N_data_norm['HTOV5'] = numpy.log10(df_N_all['HTOV5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273972, 15)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_data_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273972, 15) (273972,)\n"
     ]
    }
   ],
   "source": [
    "N_X = N_data_norm[x_indices].values.astype(float)\n",
    "N_Y = numpy.zeros(N_X.shape[0])\n",
    "\n",
    "print(N_X.shape, N_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226601      0      0      0]\n",
      " [ 14624      0      0      0]\n",
      " [ 25745      0      0      0]\n",
      " [  7002      0      0      0]]\n",
      "Accuracy: 82.71%\n"
     ]
    }
   ],
   "source": [
    "Y = predict_iwt(N_X)\n",
    "C = confusion_matrix(Y, N_Y)\n",
    "print(C)\n",
    "diagsum = numpy.diag(C).sum()\n",
    "accuracy = diagsum/C.sum()\n",
    "print('Accuracy: %3.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From all avaibale noise phases which were not used for training we are able to correctly identify 83.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's analyze weights and generate a new weight file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NTPS = load_model('URZ_model_15-6-2_norm_NC_NTPS.h5')\n",
    "model_STP = load_model('URZ_model_15-6-2_norm_NC_TPS.h5')\n",
    "model_PT = load_model('URZ_model_15-6-2_norm_NC_TP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_NTPS = model_NTPS.get_weights()\n",
    "weights_STP = model_STP.get_weights()\n",
    "weights_PT = model_PT.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_name': 'Dense',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'activity_regularizer': None,\n",
       "   'batch_input_shape': (None, 15),\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'dtype': 'float32',\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_3',\n",
       "   'trainable': True,\n",
       "   'units': 6,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_4',\n",
       "   'trainable': True,\n",
       "   'units': 2,\n",
       "   'use_bias': True}}]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NTPS.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 6)\n",
      "(6,)\n",
      "(6, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "for w in weights_NTPS:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 6)\n",
      "[[ 3.0968304e+00 -6.2974734e+00 -6.0831946e-01  3.3535042e-01\n",
      "   7.3604131e-01 -5.1173096e+00]\n",
      " [ 1.5893496e+00  5.9721166e-01  3.0018559e-01 -3.7043059e-01\n",
      "   1.4576182e+00  1.8395895e+00]\n",
      " [ 8.1926721e-01  1.9752675e-01  2.5312366e+00  5.3096199e-01\n",
      "  -3.5982567e-01  3.2447159e+00]\n",
      " [-3.6883567e-02 -9.6345270e-01 -2.4427168e+00 -1.5438526e+00\n",
      "  -2.7777711e-01 -4.3399863e+00]\n",
      " [-3.1460006e+00  8.1218439e-01  1.3475110e-01  1.3429914e+00\n",
      "   8.1348144e-02  3.8094811e+00]\n",
      " [ 7.9141688e-01 -9.4347733e-01 -1.2419647e+00  8.0076642e-02\n",
      "  -4.3333632e-01 -7.6197380e-01]\n",
      " [ 8.4127265e-01  6.0071838e-01  5.4882258e-01  2.1394522e+00\n",
      "   3.4600765e-01  2.6178579e+00]\n",
      " [-7.1985970e+00  3.0666075e+00 -2.3497241e+00 -1.5398337e+00\n",
      "   5.2326065e-01 -1.7296457e+00]\n",
      " [-9.0053434e+00 -2.4356239e+00 -1.4334965e+00  1.5572799e+00\n",
      "   1.1707560e+00 -2.6465592e-01]\n",
      " [ 1.1615269e+00  7.3884091e+00  9.1120631e-01 -2.4208775e+00\n",
      "   1.7806054e+00  1.4888449e+00]\n",
      " [-1.0096141e+00  6.5410084e-01  3.0025646e-01 -2.8725207e-01\n",
      "   7.4853809e-03 -2.0281962e-01]\n",
      " [ 1.5286468e+00 -5.1044309e-01 -2.0080798e+00  2.3981875e-01\n",
      "  -3.0815380e-02 -8.4579490e-02]\n",
      " [-1.0686145e+00 -1.1549722e+00 -2.7866745e+00  5.0445023e+00\n",
      "   4.6477166e-01  9.0137029e-01]\n",
      " [-5.1438608e+00 -9.4258585e+00 -4.0915313e+00  3.2901285e+00\n",
      "   1.3854930e+00  1.2833536e+00]\n",
      " [-2.5418541e+00 -6.9373670e+00  3.7866557e+00  7.8121314e+00\n",
      "  -6.5355837e-02 -2.2082040e+00]]\n",
      "(6,)\n",
      "[-3.070555  -4.162659  -4.292239   4.470312  -1.8595684 -5.771726 ]\n",
      "(6, 2)\n",
      "[[ 1.3897128 -1.3886656]\n",
      " [ 2.6143394 -2.6118538]\n",
      " [ 3.7250488 -3.72518  ]\n",
      " [-3.0344965  3.0374565]\n",
      " [ 5.2593293 -5.2589803]\n",
      " [ 5.6010756 -5.60136  ]]\n",
      "(2,)\n",
      "[-1.0317107  1.0285506]\n"
     ]
    }
   ],
   "source": [
    "for w in weights_STP:\n",
    "    print(w.shape)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 6)\n",
      "[[-4.3343186e+00 -1.3297454e+00  1.3434204e+01 -2.1856134e+00\n",
      "   2.3028407e-01  1.6430157e+00]\n",
      " [ 6.0653180e-01 -4.5222077e-01 -1.5330170e+00  3.5276824e-01\n",
      "  -1.9227149e+00 -2.4453971e+00]\n",
      " [-1.3291728e+00 -2.9051635e+00  3.3752039e-01 -1.0292636e+00\n",
      "   1.7666313e-01  9.6791750e-01]\n",
      " [-9.8054534e-01  8.9465481e-01 -1.3053843e+00 -9.5680672e-01\n",
      "  -7.8966961e+00 -1.5717601e+00]\n",
      " [ 5.5053496e-01  6.5853506e-01 -1.9370100e+00  1.1448896e+00\n",
      "  -1.9921190e-01  4.0188032e-01]\n",
      " [-2.1095469e+00  9.5903254e-01  7.0435876e-01  2.2798316e+00\n",
      "  -3.0110271e+00 -9.4721055e-01]\n",
      " [ 5.8626592e-01 -9.2940539e-01 -1.4385526e-01 -5.8406287e-01\n",
      "  -2.2411005e-01  1.1940055e+00]\n",
      " [ 1.5824299e-02  2.0358088e+00  9.3259200e-02  7.5488722e-01\n",
      "  -6.4224154e-01 -6.3099217e-01]\n",
      " [ 3.5099022e+00  3.5581011e-01  7.5784688e+00  2.9768522e+00\n",
      "   2.9937739e+00 -8.1714973e+00]\n",
      " [ 1.3947524e+01 -9.7597486e-01  2.3631625e+00 -1.6984946e+00\n",
      "  -1.8858331e+00 -6.2410712e-01]\n",
      " [ 5.0232202e-01 -1.4293006e-01  2.6400593e-01  1.3442415e-01\n",
      "   1.3542766e-02  3.0668220e-01]\n",
      " [ 5.3106236e-01  2.4274411e-02  1.0489791e-01  4.0561199e-01\n",
      "  -1.1653050e+00  1.5353509e+00]\n",
      " [ 1.0916122e+00  4.3897822e-01  9.9435723e-01  4.4432253e-01\n",
      "  -4.6250095e+00  1.3003099e+00]\n",
      " [ 6.9998026e-01  1.4751567e+00  3.9339533e-01  8.2311016e-01\n",
      "  -5.1674247e-01 -2.9345887e+00]\n",
      " [-4.3734592e-01  2.9043052e+00 -8.2231098e-01  6.1786675e-01\n",
      "   5.1958102e-01 -1.1963488e+00]]\n",
      "(6,)\n",
      "[ 0.24735661  1.3996652  -0.18590939 -1.7632049  -1.783642   -0.49723282]\n",
      "(6, 2)\n",
      "[[-2.5156434  2.5156431]\n",
      " [ 6.9600024 -6.9600024]\n",
      " [ 2.8115933 -2.8115933]\n",
      " [-6.380467   6.380467 ]\n",
      " [ 2.6962144 -2.6962144]\n",
      " [ 2.2161126 -2.2161129]]\n",
      "(2,)\n",
      "[-2.3000581  2.3000584]\n"
     ]
    }
   ],
   "source": [
    "for w in weights_PT:\n",
    "    print(w.shape)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's export it to a weigth file using a template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_name = 'URZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open template\n",
    "with open(os.path.join('templates','%s.template' % station_name)) as f:\n",
    "    template = f.read()\n",
    "    #print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#put name to template\n",
    "s = template.replace('{{station_name}}', station_name)\n",
    "s = s.replace('{{date}}', datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_idents = ('TPSN', 'TPS', 'TP')\n",
    "weights = (weights_NTPS, weights_STP, weights_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare weights\n",
    "for ident, w in zip(weight_idents, weights):\n",
    "    weights_layer_1 = \"\"\n",
    "    for i in range(w[0].shape[0]):\n",
    "        weights_layer_1 += ' '.join(list(map(lambda x: \"%7.6f\" % x, w[0][i,:])))\n",
    "        if i < w[0].shape[0] - 1:\n",
    "            weights_layer_1  += '\\n'        \n",
    "        \n",
    "    bias_layer_1 = ' '.join(list(map(lambda x: \"%7.6f\" % x, w[1])))\n",
    "    \n",
    "    weights_layer_2 = \"\"\n",
    "    for i in range(w[2].shape[0]):\n",
    "        weights_layer_2 += ' '.join(list(map(lambda x: \"%7.6f\" % x, w[2][i,:])))\n",
    "        if i < w[2].shape[0] - 1:\n",
    "            weights_layer_2  += '\\n'        \n",
    "        \n",
    "    bias_layer_2 = ' '.join(list(map(lambda x: \"%7.6f\" % x, w[3])))\n",
    "    \n",
    "    #put coefficients into template\n",
    "    s = s.replace('{{%s_bias_layer_1}}' % ident, bias_layer_1) \n",
    "    s = s.replace('{{%s_weights_layer_1}}' % ident, weights_layer_1)\n",
    "    s = s.replace('{{%s_bias_layer_2}}' % ident, bias_layer_2)\n",
    "    s = s.replace('{{%s_weights_layer_2}}' % ident, weights_layer_2)\n",
    "    \n",
    "#print(bias_layer_1)\n",
    "#print(weights_layer_1)\n",
    "#print(bias_layer_2)\n",
    "#print(weights_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*-- URZ.nn\n",
      "*-- Comments ...\n",
      "*-- Auxiliary 3C station URZ uses the new weights from retraining by RH\n",
      "*-- Generated 2018/03/01 14:03:33\n",
      "*-- {{training_summary}}\n",
      "URZ\n",
      "3\n",
      "60.000000\n",
      "  0.25   0.50   1.00   2.00   4.00\n",
      "URZ TPS-N\n",
      "628 3 16 7 2\n",
      "-2.376368 -0.927079 0.028574 2.904464 -2.757271 -1.935304\n",
      "-0.191346 4.051755 1.413869 0.624177 0.207417 1.420089\n",
      "-0.180812 -2.573530 0.729465 -0.681179 0.570158 0.958584\n",
      "-1.278597 -0.526079 -2.246546 0.778902 -1.163404 -1.092274\n",
      "-1.898848 -2.574765 -0.639069 0.227027 -2.255667 0.494329\n",
      "-0.975701 1.124619 0.238758 0.842130 -0.524736 -0.131241\n",
      "-0.404206 -0.089438 0.093127 1.365064 2.619826 -0.238154\n",
      "-1.269986 0.988002 0.199616 0.266871 -0.923373 0.576104\n",
      "0.614613 -0.511615 0.422346 1.290045 -0.445179 -0.448549\n",
      "3.853374 -0.962503 2.460750 -5.204453 4.308048 10.046413\n",
      "0.130815 0.663980 -4.139403 -0.264204 -0.831771 1.013826\n",
      "-0.172663 0.247883 0.169815 0.090731 0.724148 -0.175524\n",
      "-1.957182 -1.185950 0.356313 -0.064343 0.720119 -0.315432\n",
      "-3.777756 -0.243621 1.908931 0.087674 -0.673471 -1.378646\n",
      "-2.523925 1.450644 2.859220 0.440211 -2.374313 -1.127731\n",
      "1.183857 0.657498 2.285724 4.120945 -0.672444 -0.579711\n",
      "-0.746031 0.742951\n",
      "4.952499 -4.954469\n",
      "-5.983372 5.982491\n",
      "5.791893 -5.792640\n",
      "-2.737370 2.740902\n",
      "2.932872 -2.929439\n",
      "2.819576 -2.818889\n",
      " 25.428436\n",
      "0.500000\n",
      " 0.505 0.515 0.525 0.535 0.545 0.555 0.565 0.575 0.585 0.595 0.605 0.615 0.625 0.635 0.645 0.655 0.665 0.675 0.685 0.695\n",
      " 0.5976969 0.4867996 0.3770624 0.3490854 0.3521079 0.4214766 0.4963668 0.5708284 0.6283861 0.6960425 0.7512097 0.8151730 0.8560692 0.8823753 0.9086716 0.9388899 0.9706593 0.9899895 0.9946248 0.9987423\n",
      " 0.4853042 0.5188849 0.5555364 0.6276949 0.7230064 0.7812760 0.8288509 0.8678071 0.9065589 0.9311690 0.9531009 0.9430134 0.9230905 0.9006768 0.8623297 0.8341841 0.8088839 0.7978647 0.7903540 0.7834764\n",
      "URZ TP-S\n",
      "628 3 16 7 2\n",
      "-3.070555 -4.162659 -4.292239 4.470312 -1.859568 -5.771726\n",
      "3.096830 -6.297473 -0.608319 0.335350 0.736041 -5.117310\n",
      "1.589350 0.597212 0.300186 -0.370431 1.457618 1.839589\n",
      "0.819267 0.197527 2.531237 0.530962 -0.359826 3.244716\n",
      "-0.036884 -0.963453 -2.442717 -1.543853 -0.277777 -4.339986\n",
      "-3.146001 0.812184 0.134751 1.342991 0.081348 3.809481\n",
      "0.791417 -0.943477 -1.241965 0.080077 -0.433336 -0.761974\n",
      "0.841273 0.600718 0.548823 2.139452 0.346008 2.617858\n",
      "-7.198597 3.066607 -2.349724 -1.539834 0.523261 -1.729646\n",
      "-9.005343 -2.435624 -1.433496 1.557280 1.170756 -0.264656\n",
      "1.161527 7.388409 0.911206 -2.420877 1.780605 1.488845\n",
      "-1.009614 0.654101 0.300256 -0.287252 0.007485 -0.202820\n",
      "1.528647 -0.510443 -2.008080 0.239819 -0.030815 -0.084579\n",
      "-1.068614 -1.154972 -2.786674 5.044502 0.464772 0.901370\n",
      "-5.143861 -9.425858 -4.091531 3.290128 1.385493 1.283354\n",
      "-2.541854 -6.937367 3.786656 7.812131 -0.065356 -2.208204\n",
      "-1.031711 1.028551\n",
      "1.389713 -1.388666\n",
      "2.614339 -2.611854\n",
      "3.725049 -3.725180\n",
      "-3.034497 3.037457\n",
      "5.259329 -5.258980\n",
      "5.601076 -5.601360\n",
      " 1.092174\n",
      "0.500000\n",
      " 0.505 0.515 0.525 0.535 0.545 0.555 0.565 0.575 0.585 0.595 0.605 0.615 0.625 0.635 0.645 0.655 0.665 0.675 0.685 0.695\n",
      " 0.5976969 0.4867996 0.3770624 0.3490854 0.3521079 0.4214766 0.4963668 0.5708284 0.6283861 0.6960425 0.7512097 0.8151730 0.8560692 0.8823753 0.9086716 0.9388899 0.9706593 0.9899895 0.9946248 0.9987423\n",
      " 0.4853042 0.5188849 0.5555364 0.6276949 0.7230064 0.7812760 0.8288509 0.8678071 0.9065589 0.9311690 0.9531009 0.9430134 0.9230905 0.9006768 0.8623297 0.8341841 0.8088839 0.7978647 0.7903540 0.7834764\n",
      "URZ T-P\n",
      "628 3 16 7 2\n",
      "0.247357 1.399665 -0.185909 -1.763205 -1.783642 -0.497233\n",
      "-4.334319 -1.329745 13.434204 -2.185613 0.230284 1.643016\n",
      "0.606532 -0.452221 -1.533017 0.352768 -1.922715 -2.445397\n",
      "-1.329173 -2.905164 0.337520 -1.029264 0.176663 0.967918\n",
      "-0.980545 0.894655 -1.305384 -0.956807 -7.896696 -1.571760\n",
      "0.550535 0.658535 -1.937010 1.144890 -0.199212 0.401880\n",
      "-2.109547 0.959033 0.704359 2.279832 -3.011027 -0.947211\n",
      "0.586266 -0.929405 -0.143855 -0.584063 -0.224110 1.194005\n",
      "0.015824 2.035809 0.093259 0.754887 -0.642242 -0.630992\n",
      "3.509902 0.355810 7.578469 2.976852 2.993774 -8.171497\n",
      "13.947524 -0.975975 2.363163 -1.698495 -1.885833 -0.624107\n",
      "0.502322 -0.142930 0.264006 0.134424 0.013543 0.306682\n",
      "0.531062 0.024274 0.104898 0.405612 -1.165305 1.535351\n",
      "1.091612 0.438978 0.994357 0.444323 -4.625010 1.300310\n",
      "0.699980 1.475157 0.393395 0.823110 -0.516742 -2.934589\n",
      "-0.437346 2.904305 -0.822311 0.617867 0.519581 -1.196349\n",
      "-2.300058 2.300058\n",
      "-2.515643 2.515643\n",
      "6.960002 -6.960002\n",
      "2.811593 -2.811593\n",
      "-6.380467 6.380467\n",
      "2.696214 -2.696214\n",
      "2.216113 -2.216113\n",
      " 0.000001\n",
      "0.500000\n",
      " 0.505 0.515 0.525 0.535 0.545 0.555 0.565 0.575 0.585 0.595 0.605 0.615 0.625 0.635 0.645 0.655 0.665 0.675 0.685 0.695\n",
      " 0.5976969 0.4867996 0.3770624 0.3490854 0.3521079 0.4214766 0.4963668 0.5708284 0.6283861 0.6960425 0.7512097 0.8151730 0.8560692 0.8823753 0.9086716 0.9388899 0.9706593 0.9899895 0.9946248 0.9987423\n",
      " 0.4853042 0.5188849 0.5555364 0.6276949 0.7230064 0.7812760 0.8288509 0.8678071 0.9065589 0.9311690 0.9531009 0.9430134 0.9230905 0.9006768 0.8623297 0.8341841 0.8088839 0.7978647 0.7903540 0.7834764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it\n",
    "with open(os.path.join('nn_weights','%s.nn' % station_name), 'w+') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
